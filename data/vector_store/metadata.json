[
  {
    "doc_id": "doc_20260206_105438_be6f5d37",
    "page": 1,
    "chunk_index": 0,
    "text": "Arabic OCR Leaderboard (2025–2026)\nRank\nOCR Tool / Model\nKey Notes on Arabic\nAccuracy & Usability\nSize / OS\nOutput\n1\nDeepSeekOCR\n✓Excellent\ncharacter/word\nrecognition, preserves\nsemantic flow, minimal\nerrors, readable structure.\nGreat for educational\ntexts, exam passages, and\nstructured content.\n6.22 GB, Heavy\nOutputs Markdown\n2\nQARI OCR\n✓Strong for Arabic\nscript words sometimes\nmisread. Needs\npost-processing for clean\ntext.\n4.12 GB,\nVision-Language\n(Qwen2-VL-2B-\nInstruct),\nfine-tuned on\nArabic,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105438_be6f5d37",
    "page": 1,
    "chunk_index": 1,
    "text": "an\ntext.\n4.12 GB,\nVision-Language\n(Qwen2-VL-2B-\nInstruct),\nfine-tuned on\nArabic,\nLightweight\nOutput sometimes\nfragmented with HTML\nnoise\n3\nPaddleOCR VL\n✓Very good for Arabic,\nespecially on structured\ndocuments and tables.\nSlightly behind\nDeepSeek/QARI in\ncharacter-level accuracy,\nbut excellent for layout\npreservation.\n1.79 GB,\nLightweight,\nRequires\nWSL/Linux\nOutputs Structured Text\n4\nPaddleOCR v5\n☞Good Arabic\nrecognition on printed\ntext; tested on Windows\nbut does not show top\nperformance.\nLight",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105438_be6f5d37",
    "page": 1,
    "chunk_index": 2,
    "text": "tion on printed\ntext; tested on Windows\nbut does not show top\nperformance.\nLightweight,\nWindows\ncompatible\n5\nTesseract OCR\n✗Lowest accuracy\n( 74–80% on Arabic).\nStruggles with cursive,\ndiacritics, and word\nligatures. Only suitable\nfor very simple printed\ntext.\nLightweight,\nCross-platform\nOutputs Plain Text\n1",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 1,
    "chunk_index": 0,
    "text": "اللجنة التقنية للكورونا بالحمل \n وزارة الصحة–\n لبنان \n  \n \n \n \n \n المبادىء التوجيهية للعيادات الخارجية لحاالت\nكوفيد-\n91\n \n \nنيسان ،\n0202\n \nتم إعداد هذا الكتيب من قبل اللجنة التقنية للكورونا والحمل - لبنان كجزء من \n سلسلة \n المبادئ التوجيهية للعيادات الخارجية لحاالت الكوفيد-\n91\n للسيدات الحوامل.\n \nبالتعاون مع",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 2,
    "chunk_index": 0,
    "text": "1 \n \nمقدمة \nتعمل اللجنة الوطنية التقنية للكورونا والحمل في وزارة الصحة العامة على إعداد بروتوكوالت وطنية موحدة \nتتعلق بمتابعة الحمل والطلق والوالدة وما بعد الوالدة للمصابات او المشتبه بإصابته ن \n،بالكورونا وذلك لتوحيد \nوتسهيل األعمال الطبية للزميالت والزمالء. كما وتعمل أيضًا على إعداد مواد تدريبية لتدريب الزمالء من خالل \nورش عمل عن طريق تقنيات التواصل \n،المعلوماتية اضافة الى مواد تثقيفية تحاكي تساؤالت الحامل وأسرتها \nفيما يتعلق بالحماية والمتابعة والممارسات الصحية اليومية. كما وتعمل اللجنة على ر",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 2,
    "chunk_index": 1,
    "text": "فيما يتعلق بالحماية والمتابعة والممارسات الصحية اليومية. كما وتعمل اللجنة على رصد ومتابعة حاالت الحمل \nالمصابة بالكورونا لضمها الى السجل الوطني لإلحصاء.\n \nالمعلومات حول كو\nرونا تتجدد بشكل دوري وسريع وتتغير معها بعض االرشادات. سوف تصلكم الموارد \nمن اللجنة تباعًا بحسب الدراسات واألدلة التي يتم تجديدها وتحديثها.\n \nتتطلع اللجنة الى تعاونكم واقتراحاتكم في هذا المجال.\n \nد فيصل القاق \nرئيس اللجنة الوطنية التقنية للكورونا والحمل في وزارة الصحة العامة \nاألعضاء: د. سعد الدين عيتاني رئيس الجمعية اللبنانية",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 2,
    "chunk_index": 2,
    "text": "الحمل في وزارة الصحة العامة \nاألعضاء: د. سعد الدين عيتاني رئيس الجمعية اللبنانية للتوليد واألمراض \n،النسائية د. جهاد الحسن رئيس دائرة \nالتوليد والجراحة النسائية في الجامعة \n،اللبنانية د. أنور نصار رئيس دائرة التوليد والجراحة النسائية في الجامعة \nاألميركية المركز \n،الطبي د. إيلي عطية رئيس دائرة التوليد والجراحة النسائية في الجامعة \n،اليسوعية د. رنا \nسكاف رئيس قسم التوليد في جامعة \n،البلمند د. طوني زريق رئيس دائرة التوليد والجراحة النسائية في الجامعة \nاللبنانية \n،األميركية د. ربيع شاهين رئيس دائرة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 2,
    "chunk_index": 3,
    "text": "ليد والجراحة النسائية في الجامعة \nاللبنانية \n،األميركية د. ربيع شاهين رئيس دائرة التوليد والجراحة النسائية في مستشفى رفيق الحريري الحكومي \n،الجامعي د. وديع غنمة رئيس دائرة التوليد والجراحة النسائية جامعة \n،الكسليك د. ربيع شماعي مدير البرنامج \nالوطني للصحة \n،النفسية السيدة دعد العاكوم رئيسة نقابة القابالت \n،القانونيات السيدة وفاء كنعان دائرة الرعاية \nاألولية- وزارة الصحة العامة \nتتعاون اللجنة مع السيدة اسمى قرداحي- صندوق األمم المتحدة \n،للسكان د. رشا حمرا - رئيسة دائرة التثقيف \nفي وزارة الصحة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 0,
    "text": "2 \n \nالفهرس \n \n1.\n الهدف \n................................\n................................\n................................\n................................\n..........\n 3\n \n2.\n مبادىء توجيهية عامة \n................................\n................................\n................................\n......................\n 4\n \n3.\n زيارات الحامل قبل الوالدة \n................................\n................................\n................................\n.................\n 5\n \n توقيت زيارات الحامل",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 1,
    "text": "..\n................................\n.................\n 5\n \n توقيت زيارات الحامل                                             \n 5\n \n4.\n \n،التقصي الفرز والتقييم للكوفيد - \n11\n \n................................\n................................\n................................\n....\n 8\n \n الفرز عبر الهاتف \n                                                                                               \n8\n \n  .الفرز في العيادة \n /المركز",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 2,
    "text": "ة \n /المركز \n                                                                                               \n8\n \n5.\n تقديم النصائح للحامل \n................................\n................................\n................................\n.....................\n \n11\n \n6.\n سياسة إستقبال المرافقين \n................................\n................................\n................................\n..................\n \n12\n \n السياسة العامة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 3,
    "text": "السياسة العامة \n                                                                                             \n12\n \n الظروف الخاصة \n                                                                                             \n12\n \n7.\n سياسة و إجراءات وحدة التصوير الصوتي \n................................\n................................\n..........................\n \n13\n \n إرشادات عامة \n                                                                                             \n13\n \n جدولة الصور",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 4,
    "text": "13\n \n جدولة الصور الصوتية للحامل \n                                                                                             \n14\n \n8.\n المبادئ العامة \n................................\n................................\n................................\n..............................\n \n16\n \n1.\n المالحق \n................................\n................................\n................................\n................................\n...",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 5,
    "text": "..........\n................................\n................................\n......\n \n17\n \n الملحق رقم 1\n :ب حسا حركة الجنين وعدد حرك\nات الجنين؟ \n17\n \n الملحق رقم 2\n :كيفية فحص ضغط الدم في المنزل؟ \n                                                                                             \n18\n \n الملحق رقم 3\n :تقييم وإدارة حاالت فيروس كورونا (\nCOVID-19\n )في العيادات الخارجية لدى الحوامل المشتبه بإصابتهم أو المصابات بالفيروس\n \n11\n \n11\n.\n المراجع والمصادر األخرى \n................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 3,
    "chunk_index": 6,
    "text": "بالفيروس\n \n11\n \n11\n.\n المراجع والمصادر األخرى \n................................\n................................\n................................\n.............\n \n21\n \n11\n.\n مساهمات \n................................\n................................\n................................\n................................\n....\n \n21",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 4,
    "chunk_index": 0,
    "text": "3 \n \n ا\nلهدف \n \nي\nعالج هذ\nا الكتيب \n جائحة كوفيد- \n91\n \n .الحالية \n تكمن\nأهداف\nه ف:ي \n \n9\n.\n \n الحد من\nخطر تعرض \n الحوامل لكوفيد- \n91\n \n أثناء الحصول\nعلى خدمات ا\nلرعاية الصحية، و\nإست دراك\n  أن \n النظم الصحية و مقدمي الرعاية الصحية\nقد يصبحون األ\nكثر\nعرضة نقل ل \n.العدوى \n0\n.\n \n الحد من\nإ نتقال كوفيد- \n91\n ل\nعامة السكان الذي يشكل عبئاً على الصحة العامة.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 5,
    "chunk_index": 0,
    "text": "4 \n \n \n مبادىء توجيهية عامة \n \n\n \n.يجب أن يكون منع االنتشار من أولى األولويات \n\n ينصح بال\nتباعد اإل جتماعي لـ0\n متر على األقل.\n \n\n \n ترك كافة األبواب مفتوحة\nفي المساحات العيادية ل\nلحد، قدراالمكان، من \n لمس المقابض\nم ن\nقبل المرضى ومقدمي الخدمات.\n \n\n \n تأجيل أي\nإجراء إ\nختياري أو غير عاجل. \n \n\n \n يجب\nإجرا ء إتصال هاتفي مع كل\nحامل بغية إ تخاذ\nال\nقرار\nت المتعلقة ب الحاجة للزيارة و \n / أو\nاإل ختبار.المقبلة \n\n \n تنفيذ خدمات\nالكشف ال\nي صح \n عن بعد.متى أمكن \n\n \n عدم السماح\nل\nلمرافقين بالقدوم \n مع",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 5,
    "chunk_index": 1,
    "text": "خدمات\nالكشف ال\nي صح \n عن بعد.متى أمكن \n\n \n عدم السماح\nل\nلمرافقين بالقدوم \n مع\nالسيدة الحامل ل لزيارات\nالعيادية\n، قد يتم تطبيق استث ناءات\nمعينة.\n \n\n تجرى عملية ال فرز\nل جميع\nالسيدات الحوامل \n عبر الهاتف من أجل تقييم حاجتهم إلى\nدخو ل\n المستشفى و إجراء\nأي إ\nختبار\nات ضرورية.\n \n\n \n يجب التعامل مع السيدة\nلح ا\nامل التي يوجد لديها أي عوارض على أنها حالة مشتبه بها ،\nو اقتراح العزل الذاتي لمدة91\n يوم ًا.\n \n\n \n يجب التعامل مع السيدة الحامل التي تحمل عوارض\nتو صل إلى\nالم ستشفى / العيادة كما لو\nأنها إيجا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 5,
    "chunk_index": 2,
    "text": "مع السيدة الحامل التي تحمل عوارض\nتو صل إلى\nالم ستشفى / العيادة كما لو\nأنها إيجابية اإلختبار ل\nكوفيد-\n91\n .\n \n\n \n .زيادة التعقيم ضع معقم اليدين\nعلى مكتب االستقبال، في منطقة االنتظار ؛ امسح ا لمقاعد في\nمنطقة االنتظار صباح ًا \n ًوظهرا وبعد ساعات \n.الذروة \n \n\n يكفي أن تشعر الحامل ب\nعوارض \n تشبه\nعوارض اإلنفل\nونزا للشرو ع باختبار األنفلونزا\nو إ ختبار\nال كوفيد-\n91\n.\n \n \n ال يوجد توجيه يمكن أن يغطي كل الحاالت. يرجى متابعة التحديثات بسبب التغييرات التي قد طرأ\nعلى اإلرشادات.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 6,
    "chunk_index": 0,
    "text": "5 \n \n \n زيارات \n الحامل\nقبل الوالدة \n \n توقيت زيار\nات ال\nحامل \n  اعت مدت رعاية ما قبل الوالدة منذ سنوات\nأل نها الدليل للحفاظ على سالمة األمهات\n واألطفال أثناء الحمل. ولذلك ينبغي\nإ عتبار معظم الرعاية التي تحصل ما قبل الوالدة\n والالحقة لها رعاية أساسية. ومع هذا الوباء، فإن المبدأ العام هو الحد والتقليل من\nال\nزيارات العياد ية \n للحامل\nو\nحضورها شخصي  ا.\n \n لقد وضعنا توصيات\nل توقيت الزيارات العيادية\nو تباعدها أكثر بأسبوع أو نين إث .ف مع\nإ نتشارالوباء، ينبغي القيام بزيارات أقل. من ناحية أخرى، إذا كانت",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 6,
    "chunk_index": 1,
    "text": "أو نين إث .ف مع\nإ نتشارالوباء، ينبغي القيام بزيارات أقل. من ناحية أخرى، إذا كانت هناك حاجة إلى\nزيارة طارئة (مثل وجود عوارض حمل \n .خارج الرحم)، فيجب أن يتم ذلك على الفور\nنوصي بأخذ المواعي\nد ومتابعة الحامل عن بعد (عبر \n الهاتف أو باستخدام وسائل\n التواصل الحديثة كاإلتصال عبر\nالفيديو حسب ا)لحاجة لمتابعة ال\nمشاكل الطبي\nة وغيره\nا \n التي قد تعاني منها السيدة الحامل والتي ال تستدعي إجراء معاينات أو إختبارات إضافية\nمباشرة في العيادة .يجب التأكد من توثيق متابعة الحم ل عند كل لقاء مع الحامل (أكان\nمباشرة في",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 6,
    "chunk_index": 2,
    "text": "العيادة .يجب التأكد من توثيق متابعة الحم ل عند كل لقاء مع الحامل (أكان\nمباشرة في العيادة أو عن بعد )\n خاصة للسيدات اللواتي يعانين من مشاكل في إرتفاع\n ضغط الدم\nوبحاجة للمراقبة عن قرب. \n تزو  د الحامل بمعلومات تشير إلى كيفية إستخدام\nجهاز \n مراقبة ضغط الدم في المنزل وجدول بأوقات المراقبة لتدوين النتائج الالزمة\n:للمتابعة. يكون التوقيت للزيارات كما يلي \n \no\n \n90\n أسبوع ًا \n : زيارة الحامل ل لعيادة- الحضور \n  شخصيا \n \n•\n لتحديد تاريخ \n الحمل و عمر الجنين /إجراء صورة صوتية ل سماكة\nعنق الجنين \nNT\n \n \n•",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 6,
    "chunk_index": 3,
    "text": "تحديد تاريخ \n الحمل و عمر الجنين /إجراء صورة صوتية ل سماكة\nعنق الجنين \nNT\n \n \n•\n \n طلب\nفحوصات الدم ال\nمخبري\nة \n \no\n \n20\n أسبوع ًا : زيارة الحامل للعيادة- الحضور \n  شخصيا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 7,
    "chunk_index": 0,
    "text": "6 \n \n•\n \n إجراء( صورة صوتية مفص لة للجنينMorphological scan\n)\n \n \no\n \n28\n أسبوع ًا :\n زيارة الحامل للعيادة- الحضور \n  شخصيا \n•\n \n طلب إختبار السكر1hr PC\n  \n / متابعة التلقيحTdap، و متابعة\nإعطاء \nRhogam\n عند الحاجة \n•\n \n إعطاء الحامل معلومات عن كيفية مراقبة\nحركة الجنين \n (الملحق \n رقم\n9\n)\n \n \no\n \n20\n أسبوع ًا :\n زيارة الحامل للعيادة- الحضور \n  شخصيا \n•\n \n تزويد الحامل بمعلومات لكيفية\nمراقبة \n ضغط الدم في المنزل\n(المل حق\n رقم0\n)\n \n\n ت\nزويد الحامل بجدول ل\nتدوين نتائج مراقبة \n ضغط الدم في\n المنزل",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 7,
    "chunk_index": 1,
    "text": "حق\n رقم0\n)\n \n\n ت\nزويد الحامل بجدول ل\nتدوين نتائج مراقبة \n ضغط الدم في\n المنزل \n•\n \n التأكد من متابعة الحامل لمراقبة حركة الجنين (الملحق  رقم9\n)\n \n•\n جدولة الوالدة القيصرية \n)(متى اقتضت الحاجة \n•\n \n ممكن إجراء زيارة23\n أسبوع للحامل عن بعد (\n بحال توفر التواصل\n والحصول على الخدمات الصحية عن ب عد\nمن قبل الحامل)\n \n \no\n \n23\n أسبوع ًا :\n زيارة الحامل للعيادة- الحضور \n  شخصيا \n•\n \n إجراء\nإ ختبارGBS\n \n•\n \n متابعة\nضغط دم ال\nحامل \n في المنزل  (إذا توف\nر جهاز \n مراقبة ضغط\n)الدم \n•\n \n التأكد من متابعة الحا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 7,
    "chunk_index": 2,
    "text": "حامل \n في المنزل  (إذا توف\nر جهاز \n مراقبة ضغط\n)الدم \n•\n \n التأكد من متابعة الحامل لمراقبة حركة الجنين (الملحق  رقم9\n)\n \n•\n \n جدولة / تحريض المخاض\nالوالدة القيصرية \n)(متى اقتضت الحاجة \n•\n \n مناقشة تحريض للمخاض في األسبوع ال21",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 8,
    "chunk_index": 0,
    "text": "7 \n \no\n \n38\n أسبوع ًا :زيارة ا لحامل للعيادة- الحضور \n  شخصيا ب حال عدم وجود\nجهاز \n لمراقبة ضغط الدم.في المنزل \n \no\n \n زيارات:بعد الوالدة \n•\n \n يفضل\nاإلتصال ب\nال\nسيدة بعد الوالدة ل.حجز موعد الزيارة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 9,
    "chunk_index": 0,
    "text": "8 \n \n ال\nتقصي\n، الفرز والتقييم للكوفيد - \n91\n \n \n الفرز عبر الهاتف \nيج ب اإلتصال بكل سيدة حامل لتأكيد\nموعد الزيارة \n عبر إعتماد خطوات الرسم البياني\nأدناه .\n تت بع الخطوات ذاتها بحال\nإتصلت \n الحامل\nلأ\nخذ موعد  للزيارة.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . الفرز في\nالعيادة / المركز \n يتم إتباع نفس النمط\nالمدرج والمفصل أعاله بحال قدوم السيدات ا\nل\nحوامل إلى الع يادة\nدون سابق إ تصال أوالمرور\nعبر ال فرز\nال\nهاتفي. ومع ذلك، ستواصل الممرضة / الق ابلة\n /الطبيب التقييم وفق ًا ل لرسم البياني رقم9\n .",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 9,
    "chunk_index": 1,
    "text": "ذلك، ستواصل الممرضة / الق ابلة\n /الطبيب التقييم وفق ًا ل لرسم البياني رقم9\n .\n \n*ال\nعوار:ض \n •خفيفة: سعال جديد \n •معتدلة: ح\nرارة مرتفعة جديدة ، سعال جديد \n •شديدة: ألم في الصدر وضيق في التنفس \n \n ** ابدأ بعقارTamiflu\n \n وفق ا إلرشاداتACOG\n:\n \n •حرارة أكثر من \n23\n \n :درجة مئوية وأي من الحاالت التالية\nعوارض \n ،عدوى الجهاز التنفسي العلوي ، ألم عضلي ، تعب\nآالم الرأس / الجسم \n •\n لا\nيوجد حرارة ولكن ظهور مفاجئ ل\nعوارض \n توحي\nباإلنفلون\nزا \n •\n عالجoseltamivir\n \n57\n \n مجم مرتين باليوم مدة7\n ايام \nأسئلة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 9,
    "chunk_index": 2,
    "text": "وحي\nباإلنفلون\nزا \n •\n عالجoseltamivir\n \n57\n \n مجم مرتين باليوم مدة7\n ايام \nأسئلة التقص ي \n ألي\nحامل تتصل \n \nألخذ أو تأكيد موعد الزيارة \nالفرز عبر الهاتف \nالسؤال \n9\n: \n هل لديك حمى و/أو\nعوارض \nتنفسية (سعال ، ضيق في التنفس)؟ \nو \n السؤال3\n: \n هل سافرت إلى الخارج خالل ال\n91\n \n اليوم السابق أو كان لديك اتصال وثيق\n مع أي شخص عائد من الخارج أو مع حالة\nمشبوهة أو مؤكدة من \n كوفيد-\n91؟ \n نعم للسؤال1\n \n ونعم أو ال للسؤال2\n \n نعم للسؤال0\n \n فقط\n)(خطر التعر ض توجيه ال\nحامل \n إلى طبيب الصحة لمزيد\n من التقيم",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 9,
    "chunk_index": 3,
    "text": "م للسؤال0\n \n فقط\n)(خطر التعر ض توجيه ال\nحامل \n إلى طبيب الصحة لمزيد\n من التقيم \n \n •إتباع \n تقييم المخاطر حسب \nACOG / SMFM\n \n (الملحق رقم3\n)\n \n عدم\nاستقبال الحامل للزيارة \n •تقديم المشورة ل لحجر الصحي\n المنزلي لمدة14\n \n ًيوما من التعرض \n •حاليا ال \n يتم إختبار حامل ت ال\nظهر \nعوارض \n الرسم البياني رقم1 :  فرز إتصاالت الحامل ألخذ / تأكيد مواعيد الزيارات",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 10,
    "chunk_index": 0,
    "text": "9 \n \n كما ستقوم الممرضة / القابلة\nبالتشاور مع الطبيب المعالج \n بحال وجدت حاجة\nجإل راء\nإ ختبار كوفيد- \n91\n \n للحامل\nأو \n إذا كانت حالة\nالحامل تستدعي ال دخول\nلل\nمستشفى أو إن \n وجب.إرسالها إلى المنزل",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 11,
    "chunk_index": 0,
    "text": "10 \n \n تقديم النصائح لل\nحامل \n يجب توعية الحوامل حول:ما يلي \n \no\n \n ال تشير البيانات المتوفرة عن وجود خطر متزايد على الحوامل بسبب\nكوفيد - \n91\n .\n ولكن بالرغم من\n ،ذلك تجدر\nاإلشارة بأ ن الحوامل هن أكثرعرضة\nلمراضة ووفيات مرتفعة نتيجة إصابتهن ب\nلت إ هابات\nالجهاز التنفسي األخرى مثل اإل .نفلونزا\nلذا يجب اعتبارهن معرض\nات لخطر ال كوفيد- \n91\n \n وعليهن\n القيام\nبنفس اإل\nجراءات \n المتخذة من قبل:عامة الناس لحماية أنفسهن من األمراض \n\n  \n تغطية\nالسعال (بإ ستخدام\nتقنية ال)مرفق \n\n \n تجنب\nاإلحتكاك ب\nاألشخاص ال",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 11,
    "chunk_index": 1,
    "text": "اض \n\n  \n تغطية\nالسعال (بإ ستخدام\nتقنية ال)مرفق \n\n \n تجنب\nاإلحتكاك ب\nاألشخاص المرضى \n\n تنظيف اليدين ب\nإ ستخدام\nالماء و\nالصابون أو معقم اليدين الكحولي.\n \n \no\n وبحال اإلصابة بال كوفيد- \n91\n ، ستكون العوارض خفيفة وإحتمال التعافي من المرض كبير.\n \n \no\n إذا ظهرت عل\nى الحامل إحدى العوارض \n الشديدة وإستمر ت ،لفترة طويلة فيطلب منها التواصل الفوري\n مع الطبيب المعالج منعاً لتفاقم المرض\nوظهور إ.لتهابات صدرية حادة \n \n \no\n إذا أصبح\nت الحامل بحالة جيدة، ولم يكن لديه ة أي ا \n مضاعفات من حمل سابق وكان من المفت",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 11,
    "chunk_index": 2,
    "text": "صبح\nت الحامل بحالة جيدة، ولم يكن لديه ة أي ا \n مضاعفات من حمل سابق وكان من المفترض أن\nتقوم ب\nمعاينة روتيني\nة قريب ًا\n، فمن األفضل االتصال لطلب النصيحة بشأن ما إذا كان يجب تث بيت الموعد\n.أو إعادة جدولته \n \no\n إذا كانت لديه ا\nأي مخاوف، فيجب عليه\nا اإل\nتصال ب\nال.طبيب كالمعتاد \n \no\n إذا كانت لديه\nا \n  مشكلة ملح\nة تتعلق بالحمل ولكنها ال تتعلق ب\nال كوفيد- \n91\n ،يجب اإل تصال الفوري\nلل عناية\nال.فورية",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 12,
    "chunk_index": 0,
    "text": "11 \n \no\n إذا كان\nت لديه\nا ع\nوارض \n كوفيد- \n91\n ، فيجب\nعليه\nا اإل تصال وإبالغ الطبيب أو\nال\nممرض\nة ل ترتيب\n المكان والوقت المناسبين\nل\nلزيار\nة.\n \n \no\n يجب عدم حضور الحامل \n بشكل مفاجئ للزيارة في\nالعيادة الخارجية إال بعد إجراء إتصال لإلبالغ.\n \n \no\n \n قد تكون هناك حاجة إلى مباعدة زيارات\nمتابعة الحمل وفق ًا \n لتوصيات مقدم الخدمة\nو\nحسب كل حالة. \n يجب أن ال تقوم الحامل بمباعدة زيارات المعاينة من تلقاء نفسها إال بعد توجيه من الطبيب و/ أو مقدم\n.الرعاية \n للحصول على التفاصيل، راجع القسم2.9\n.\n \n \no\n \n عند ا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 12,
    "chunk_index": 1,
    "text": "الطبيب و/ أو مقدم\n.الرعاية \n للحصول على التفاصيل، راجع القسم2.9\n.\n \n \no\n \n عند الحضور للزيارة ي طلب\nمن الحوامل عدم إصطحاب مرافقين (إال ضمن استثناءات)، كما ي\nفض\nل \nعدم اصطحاب األطفال إلى الع.يادة \n للحصول على التفاصيل، راجع القسم7\n.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 13,
    "chunk_index": 0,
    "text": "12 \n \n \n سياسة إستقبال\nال\nمرافقين \n \n \n ا\nلسياسة العامة \n\n \n يفضل إصطحاب\nشخص واحد من ال عائلة- صديق /ة- شريك للمعاين ة على األكثر وإذا أمكن\n.الحضور دون مرافقين \n\n ي\nطلب من الحوامل عدم إحضار األطفال.\n \n \n ال ظروف\nال\nخاصة \n\n وحدة التصوير الصوتي :ي سمح بإجراء\nتواصل هاتفي / عبر \n الفيديو أثناء\nالقيام بالت صوير\nالصوتي \n بدالً من\nإ\nصطحاب شخص داعم / مرافق  . قد ي\nسمح بحضور الشخص \n الداعم\nشخصي ًا \nبحال وجود حالة صحية طارئة للجنين والحامل تستدعي الشرح والمشورة.\n \n\n اإل\nحتياجات الخاصة: ي سمح ل\nل شخص\nا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 13,
    "chunk_index": 1,
    "text": "ة للجنين والحامل تستدعي الشرح والمشورة.\n \n\n اإل\nحتياجات الخاصة: ي سمح ل\nل شخص\nال داعم بالتواجد مع الحامل بحال كانت حالتها تستدعي\nعناية خاصة و.ذلك لتقديم المساعدة حسب تقدير مقدم الخدمة \n\n اإلجراءات العيادية : يمكن\nللحامل إحضار شخص داعم\n/ مرافق \n.أثناء اإلجراء \n\n األطفال: ألن األطفال يعتبرون مصدر إلنتقال العدوى، فال \n يسمح\nب\nوجود هم في/ العيادة \nالمركز الصحي .ويفضل ال\nطلب من الشخص \n/ الداعم \n المرافق (إن وجد) إصطحاب الطفل\nو\nمغادرة ال\nعيادة / المركز الصحي . إذا كان\nالطفل يعان\nي من ع\nراو ض، سي طل",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 13,
    "chunk_index": 2,
    "text": "الطفل\nو\nمغادرة ال\nعيادة / المركز الصحي . إذا كان\nالطفل يعان\nي من ع\nراو ض، سي طلب من الحامل\nإعادة جدولة الزيارة\n. يوصى بشدة بعدم إحضار \n األطفال إلى أي زيارة\nل.لعيادات الخارجية \n\n ال\nعوار ض الموجودة: قد ي طلب من الحوامل إعادة جدولة زيارتهن (غير العاجلة) لمتابعة\n الحمل إذا ظهرت عليهن أو على مرافقيهن / ذويهن\nأي\nة عوارض للمرض .",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 14,
    "chunk_index": 0,
    "text": "13 \n \n سياسة و \n إجراءات\nوحدة التصوير الصوتي \nت درج المبادئ العامة\nالتالية لمتابعة بالحامل بحيث \n يجب أن\nي\nكون توقيت الم\nعاين ة\nخالل الحمل مصم\nي م ل تناسب\n مع\nحاجات ال\nحامل  و\nمقدم الخدمة و\nوجود \n عوامل\nالخط\nةرو . وقد تبي ن أن هذه التغييرات\nل زيارة\nالحامل ل لعيادة\nفي هذا الوقت، ممكن أن ت\nشكل مخاطر صحية شخصية وعامة محتملة\n، وعليها \n سوف يتم إعادة تقييم\nفو ائد\n ومخاطر\nالم\nة عاين عن قرب وت\nتظيم توقيت الم\nعاينة.\n \n \n إرشادات عامة \no\n تص اإل\nا ل بالحامل\nعبر \n الهاتف\nقبل الموعد بيوم للتأكد من وجود ساب",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 14,
    "chunk_index": 1,
    "text": "ادات عامة \no\n تص اإل\nا ل بالحامل\nعبر \n الهاتف\nقبل الموعد بيوم للتأكد من وجود سابق ل\nلسفر و\n/أو بروز أية عوار\nض .\nسيتم توجيه الح\nامل ال تي\nت عاني من\nعوار ض، والتي تم\nتشخيصه\nا ب\nكوفيد - \n91\n خ الل األسبوعين\n الماضيين، أو\nال تي تعاني من حالة مشتبه بها\nبعدم الحضور إل جراء\nالصورة الصوتية.\n \n \no\n \n قبل الزيارة ، سيتم\nإعالم \n الحامل\nبعد إصطحاب \n مرافق معها إلى\nموعد \n الزيارة، ما لم يكن ذلك\nضروري ًا \n ومحدد( من الناحية الطبية راجع القسم3.0\n ). إذا\nإصطحبت \n الحامل مرافق، فعلينا فحصه\nأيض ًا للتقصي.\n \n \no",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 14,
    "chunk_index": 2,
    "text": "ة راجع القسم3.0\n ). إذا\nإصطحبت \n الحامل مرافق، فعلينا فحصه\nأيض ًا للتقصي.\n \n \no\n \n في\nال\nيوم المقر ر \n إلجراء الصورة\nالصوتية، يجب فحص الح\nامل والتقصي عن أية عالمات وعوارض \n.مرة أخرى \n \no\n إزالة جميع المعدات واألدوات \n غير الضرورية في وحدة التصوير الصوتي (على سبيل المثال\nو جود\n.)صناديق إضافية، وكراسي \n \no\n \n ينصح بإستخدام\nمسبار(probe)\n \n عدد0\n ل جهاز\nالتصوير الصوتي (واحد ذات \n تردد منخفض9\n \n إلى\n3\n ميجاهرتز و آخر ذات تردد عالي0\n \n إلى1\n \n)ميجاهرتز \n والتقليل من إبقاء المسابر األخرى متصلة\nبالجهاز",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 14,
    "chunk_index": 3,
    "text": "تردد عالي0\n \n إلى1\n \n)ميجاهرتز \n والتقليل من إبقاء المسابر األخرى متصلة\nبالجهاز .وضع المسبار المهبلي خارج غرفة ال\nتصوير .\n ًينصح أيضا بإزالة وتخزين جميع\nمس ابير\nالجهاز \n األخرى عندما ال\nيتم إستخدامها ، خاصة تلك التي تكون\nحساسة \n ويمكن أن\nتتعر ض لل\nتل ف",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 15,
    "chunk_index": 0,
    "text": "14 \n \n بواسطة محلول\nالتنظيف، مثل الم\nسابير \n اإللكترونية والميكانيكية\nال( ثالثية األبعاد3 D\n )\n المغطاة بغالف\nدقيق.\n \n \no\n \n تنظيف وحدة التصوير\nالصوتي جيد ًا \n كل صباح قبل\nالمعاينات وبعدها .ي\nتضمن التنظيف سح م \n لوحة\nمفاتيح الكمبيوتر والماوس، وأبواب ال\nوحدة ،األ  سر\nة / طاولة التصوير، كراسي ال زوار\nوالطبيب ،\nجهاز التصوير ،كافة األسطح والخزائن.\n \n \no\n \n قبل وبعد\nإجراء الصورة \n:الصوتية \n\n قم بغ سل\nاليدين بالماء الدافئ و الصابون أو بمنظف مضاد للميكروبات لمدة02\n \n ثانية على\n.األقل \n\n \n قم بتنظيف\nم",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 15,
    "chunk_index": 1,
    "text": "و الصابون أو بمنظف مضاد للميكروبات لمدة02\n \n ثانية على\n.األقل \n\n \n قم بتنظيف\nمسابير \n وأسالك.جهاز التصوير الصوتي \n\n قم بإ رتداء قفازات\nذات اإلستعمال الواحد \n (خالية من مادة الالتكس) أثناء إجراء التصوير\nالصوتي وتغيير\nها \n بعد كل\nمتابعة/تصوير.\n \n \no\n قد ال تنطبق إجراءات ال\nتصوير \n العامة في هذه الحالة بسبب\nالظروف اإل .ستثنائية لذا\nيقترح \n تقصير\n  مد ة\nالتصوير الصوتي \n بقدر اإلمكان. حفظ صور\nالجنين بدالً من سحبها لتسريع مد ة ال\nتصوي\nر . ضبط\n وقت التصوير تبعاً للمتابعة والحاجة.والتركيز على األولوي",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 15,
    "chunk_index": 2,
    "text": "يع مد ة ال\nتصوي\nر . ضبط\n وقت التصوير تبعاً للمتابعة والحاجة.والتركيز على األولويات \n \n \n \n جدولة\nالصور \n الصوتية للحامل \no\n تحديد تاريخ الحمل / عمر الجنين (\n90\n \n ًأسبوعا:)\n \n\n الجمع بين مواعيد \n/ التصوير \n إجراء صورة صوتية لسماكة عنق الجنينNT\n \n تبعاً لتاريخ\n آخر دورةLMP\n \n\n \n اذا كان تاريخ آخر دورة\nغير معروف أو \n محدد وكان عمر الحمل يتعدى ال91\n أسبوع ًا ،\n يمكن تحديد موعد.آخر للتصوير",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 16,
    "chunk_index": 0,
    "text": "15 \n \no\n الصورة الصوتية التفصيلية للجنين \n(Morphological scan)\n (\n02\n أسبوع ًا:)\n \n\n ألي تفصيل ثانوي تأجيل ال مراجعة\nمن \n1\n \n إلى3\n \n أسابيع بدالً من أسبوع إلى0\n.\n \n\n \n بحال كان\nمؤشر كتلة الجسم \n لدى الحامل أكثر من12\n كجم/م0\n :إجراء الصورة \n عند00\n \nأسبوع ًا.\n \n\n \n:فحص طول عنق الرحم \n\n \n أن يتم فحص طول عنق الرحم.تزاكناً مع إجراء الصورة التفصيلية للجنين \n\n بحال عدم تواجد \n تاريخ سابق\nلوالدة مبكرة : ال\nداعي ل\nمتابعة إضافية لفحص طول عنق الرحم.\n \n\n \n إذا بلغ طول عنق الرحم أكثر\nمن \n07\n \n مم :",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 16,
    "chunk_index": 1,
    "text": "عة إضافية لفحص طول عنق الرحم.\n \n\n \n إذا بلغ طول عنق الرحم أكثر\nمن \n07\n \n مم :\n ال ضرورة لمزيد من المراقبة عبر\n.التصوير الصوتي أو إعطاء عالج للحامل \n\n \n إذا بلغ طول عنق الرحم07\n \n : مم أو أقل \n إعطاء الحامل عالج\nب البروجسترون\nالمهبلي.\n \n\n بحال تواجد تاريخ سابق لوالدة مبكرة :\n \n\n  \n مراقبة طول عنق الرحم\nعبر التصوير من األسبوع \n93\n \n إلى01\n \n :بحال \n حدوث\nوالدة مبكرة سابقة \nمن \n األسبوع93\n \n إلى21\n \n / تاريخ سابق لقصر / عنق الرحم ربط سابق لعنق الرحم\n(cerclage)\n.\n \n\n \n مراقبة طول عنق الرحم أثنا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 16,
    "chunk_index": 2,
    "text": "صر / عنق الرحم ربط سابق لعنق الرحم\n(cerclage)\n.\n \n\n \n مراقبة طول عنق الرحم أثناء إجراء الصورة المفصلة لمرة واحدة : بحال تواجد\nتاريخ سابق لوالدة مبكرة \n بين األسبوع21\n و23\n.\n \n \no\n مراقبة نمو الجنين بالتصوير الصوتي :\n  على أن تتم\nجدول ة موعد إلجراء الصورة\nو\nالحضور ش  خصيا \nعندما يكون ذلك ممكن ًا \n\n \n ي أن تم التصوير\nلك\nافة \n الحوامل\n(ذات ال جنين\nال\nواحد) في الفصل الثالث من ال\nح\nمل أي ا ألسبوع\n20\n.\n \n\n م\nراقب ة( الخالص المتقدمplacenta preavia\n) في \n األسبوع21\n.\n \n\n \n مراقبة النمو التسلسلي إبتد",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 16,
    "chunk_index": 3,
    "text": "الص المتقدمplacenta preavia\n) في \n األسبوع21\n.\n \n\n \n مراقبة النمو التسلسلي إبتداء من03\n أسبوع ًا (بدل \n01\n أسبوع ًا)، يعدل بحال وجودإ\nستثناء.\n \n\n \n ًمتابعة  الحامل عموما \n كل3\n \n إلى3\n \n أسابيع بدالً من كل1\n أسابيع.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 17,
    "chunk_index": 0,
    "text": "16 \n \n المبادئ العامة \no\n ي  جب على األطباء التأك\nد من ت\nقديم \n كامل الرعاية للحوامل اللواتي يعانين من حاالت عالية الخطورة\n تستدعي المتابعة المستمرة وإجراء\nإختبارات \n معي نة.عند الحاجة \n \no\n يجب على األطباء أيض ًا \n وضع خطة للتعامل مع: االحتماالت التالية \n نقص القوى العاملة في مجال\n ،الرعاية الصحية، نقص معدات الحماية الشخصية عدم توف ر غرف\nعزل كافية ،\n لذا ينبغي زيادة\nإ\nستخدام الخدما ت الصحية عن بعد (عبر الهاتف أو\nالفيديو) ب قدر اإلمكان\nلمتابعة الحامل قبل ا.لوالدة \n \no\n \n يجب على األطباء التأك د",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 17,
    "chunk_index": 1,
    "text": "يديو) ب قدر اإلمكان\nلمتابعة الحامل قبل ا.لوالدة \n \no\n \n يجب على األطباء التأك د من أن\nتوثيق المعلومات و\nاإل حتفاظ بالسجالت\nأمر ًا \n.بالغ األهمية \n \no\n \n يفضل إبقاء\nالعاملين في مجال الرعاية الصحية \n الذين قد تعرضوا لحامل مصابة بكوفيد- \n91\n \n (دون\n .سابق معرفة أو حماية بوضع معدات الحماية الشخصية) في مبنى المستشفى هذا ألنه في معظم\nالحاالت\n، سيكون هذا تعرض ًا قصير األ\nمد ، على عكس البيئة المنزلية حيث يكون التعرض\nمس تمر\nوالتفشي أسرع\n. يجب على العاملين في مجال الرع:اية الصحية \n \n\n \n عدم الحضور الى ال",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 17,
    "chunk_index": 2,
    "text": "التفشي أسرع\n. يجب على العاملين في مجال الرع:اية الصحية \n \n\n \n عدم الحضور الى العمل إذا ظهرت عليهم\nعوارض \n أثناء تواجدهم في المنزل (خارج\nالخدمة)، و\nإ\nبالغ مديرهم المباشر على الفور.\n \n\n \n إذا ظهرت عليهم\nعوارض أثناء تواجدهم في ال\nعمل فيجب القيام بال عزل\nال ذاتي\nوإبالغ مدير هم\nالمباشر على الفور.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 18,
    "chunk_index": 0,
    "text": "17 \n \n المالحق \n \n \n \n الملحق رقم9: حساب حركة الجنين و\nعدد حرك\nات الجنين؟ \nعدد حركات الجنين هو عدد المرات التي تشعر \n الحامل\nفيها ب\nحركة الجنين خالل فترة زمنية معينة. قد ي سمى \n ًهذا أيضا \n عدد ركالت الجنين. يوصى بحساب حركة الجنين لكل\nسيدة حامل. قد ي طلب من\nك \n البدء\nحس ب اب\nحركات الجنين في وقت مبكر من الحمل أي من األسبوع الـ03\n \n من الحمل. انتبهي إلى وقت نشاط طفلك. قد\nتالحظين نوم طفلك واستيقاظه. و\nقد تالحظين أيض ًا أشياء تجعل طفلك يتحرك أكثر. يجب عليك القيام بحسب ان\nعدد حرك\nات \n:الجنين \n\n ع ًا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 18,
    "chunk_index": 1,
    "text": "أشياء تجعل طفلك يتحرك أكثر. يجب عليك القيام بحسب ان\nعدد حرك\nات \n:الجنين \n\n ع ًادة \n.ًعندما يكون طفلك أكثر نشاطا \n\n يفضل  أن تقومي بذلك في نفس الوقت من كل ي.وم \n \nالوقت المناس\nب لحساب الحركات هو أثناء الراحة ، بعد تناول\nالمأكوالت والمشروبات.\n \n \nكيف أحسب حركات الجنين؟ \n9.\n إ عثري على\nمكان \n .هادئ ومريح\nإ جلسي أو\nإ.ستلقي على جانبك \n2.\n إ كتبي التاريخ ووقت البدء ووقت التوقف وعدد الحركات التي شعرتي بها بين هاتين المرتين. خذي\n هذه المعلومات معك إلى زيارات\nمتابعة الحمل.\n \n3.\n \n  احسبي الركالت والرفر",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 18,
    "chunk_index": 2,
    "text": "ن. خذي\n هذه المعلومات معك إلى زيارات\nمتابعة الحمل.\n \n3.\n \n  احسبي الركالت والرفرفات، والحركات، واللف\nات، و\nالوخزات \n  لمد ة ساعتين. يجب أن تشعري بـ92\n \n.حركات على األقل خالل ساعتين \n4.\n \n قد تتوقفي\nعن العد بعد أن تشعرين \n بـ92\n \n.حركات \n5.\n \n إذا لم تشعري بـ92\n \n حركات خالل ساعتين، تناولي\nبعض الطعام والشراب . ثم، استريحي واستعدي\n لمدة ساعة واحدة. إذا شعرتي بـ1\n \n.حركات على األقل خالل تلك الساعة فقد تتوقفي عن العد \n \n:اتصلي بمقدم الرعاية الصحية إذا \n \n\n ش عرتي بأقل من92\n \n.حركات في ساعتين \n\n كا",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 18,
    "chunk_index": 3,
    "text": ":اتصلي بمقدم الرعاية الصحية إذا \n \n\n ش عرتي بأقل من92\n \n.حركات في ساعتين \n\n كا.ن طفلك ال يتحرك كما يفعل عادة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 19,
    "chunk_index": 0,
    "text": "18 \n \n \n \n الملحق رقم3: كيفية فحص ضغط الدم في المنزل؟ \nما نوع جهاز قياس ضغط الدم المطلوب شرا\n؟ ؤه \nهناك ثالثة أنواع من أجهزة مراقبة ضغط الدم: آلي، وشبه تلقائي و يدوي. نوصي بشراء جهاز قياس ضغط\n الدم التلقائي في\nالفصل \n.األول من الحمل \nكيف تستخدمين جهاز قياس ضغط الدم في المنزل؟ \n:إليك كيفية الحصول على قراءة دقيقة \n\n إ نتظري22\n دقيقة قبل قياس ضغط الدم إذا كنت قد دخنتي سيجارة\n، أو تناولتي مشرو\nب ًا يحتوي على الك ،افيين\n.أو مارستي الرياضة. هذه عادة ما ترفع ضغط الدم \n\n قومي بتثبيت الكفة \n.على الذراع",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 19,
    "chunk_index": 1,
    "text": ".أو مارستي الرياضة. هذه عادة ما ترفع ضغط الدم \n\n قومي بتثبيت الكفة \n.على الذراع التي ال تستخدميها للكتابة \n\n \n ًاجلسي بثبات، تأكدي من أن يكون ظهرك مستقيما ومدعوم ًا \n .وقدميك مسطحتان على األرض\nإ دعمي ذراعك\n بوضعه\nعلى ا\nلطاولة. يجب أن يكون ذراعك مستوي ًا \n.مع قلبك \n\n خذي قراءاتك في نفس الوقت من اليوم.\n \n\n \n احتفظي بسجل لضغط الدم- تحدثي مع طبيب\nك \n الخاص بشأن عدد المرات التي يجب عليك فحص ضغط الدم\n.فيها \nما هو ضغط الدم الطبيعي؟ \n\n \n يجب أن يكون الرقم االنقباضي (العلوي) أقل من912\n \n وأن يكون ال",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 19,
    "chunk_index": 2,
    "text": "لدم الطبيعي؟ \n\n \n يجب أن يكون الرقم االنقباضي (العلوي) أقل من912\n \n وأن يكون الرقم االنبساطي (السفلي) أقل من12\n.\n \n\n \n إذا كنت تعانين من\nإ رتفاع في.ضغط الدم، تحدثي مع طبيبك حول الحدود المناسبة \n\n \n إذا\nإ رتفع ضغط الدم، استريحي لمدة97\n دقيقة وكرري ذلك.\n \n\n \n إذا كان ضغط الدم لديك912\n/\n12\n ، فاتصلي.طبيبك إلعالمه \n\n \n إذا كان ضغط الدم لديك \n يتعدى932\n/\n922\n \n فقد تكون هذه حالة طارئة وعندها تحتاجين إلى التحدث مع\n.ًطبيبك فورا \n\n لمزيد من المعلومات مراجعة ال موقع التالي :\n-\nhttps://www.babycente",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 19,
    "chunk_index": 3,
    "text": "بيبك فورا \n\n لمزيد من المعلومات مراجعة ال موقع التالي :\n-\nhttps://www.babycenter.com/0_monitoring\nhome_10415175.bc\n-\nat\n-\nbloodpressure\n-\nyour\n \n \n \n ضغط الدم اإل                   نقباضي \n  ضغط الدم اإل                                                                           نبساطي",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 20,
    "chunk_index": 0,
    "text": "19 \n \n \n الملحق رقم2\n :\n( تقييم وإدارة حاالت فيروس كوروناCOVID-19\n ) في العيادات\nالخارجية \n لدى الحوامل المشتبه بإصابتهم أو\nالمصابات بالفيروس",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 21,
    "chunk_index": 0,
    "text": "20 \n \n المراجع والمصادر األخرى \n \nAmerican Institute of Ultrasound in Medicine (AIUM)  \n \nAIUM Official Statement: Guidelines for Cleaning and Preparing External- \nand Internal-Use Ultrasound Transducers Between Patients & Safe \nHandling and Use of Ultrasound Coupling Gel  \n \nCenters for Disease Control and Prevention (CDC) \n كوفيد- \n91\nResources  \n \nEPA Approved Disinfectants for COVID-19  \nHealthcare Professionals: Frequently Asked Questions and Answers  \nResources for Healthcare Facilities",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 21,
    "chunk_index": 1,
    "text": "Frequently Asked Questions and Answers  \nResources for Healthcare Facilities  \nInterim Guidance for Risk Assessment of Healthcare Personnel with \nPotential Exposure in Healthcare Setting \n  \n كوفيد- \n91\n| SMFM.org - The Society for Maternal-Fetal Medicine  \n \nSMFM Resources: Coronavirus (COVID-19) \n  \n كوفيد- \n91\n| ACOG.org - The American College of Obstetricians and \nGynecologists  \nACOG Resources: COVID-19  \n \nACOG/SMFM:  \nOutpatient Assessment and Management for Pregnant Women With \nSuspecte",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_105844_b5f457fa",
    "page": 21,
    "chunk_index": 2,
    "text": "G/SMFM:  \nOutpatient Assessment and Management for Pregnant Women With \nSuspected or  \nConfirmed Novel Coronavirus (COVID-19) \n \n \n مساهمات \n \nفيصل القاق  تطوير وتدقيق \nأنور نصار  مراجعة وتدقيق \nايلي عطية- \n ربيع شاهين- \n سعد عيتاني- \n رنا سكاف- \n طوني زريق \n مراجعة",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_111911_e55d92fe",
    "page": 1,
    "chunk_index": 0,
    "text": "Whisper LoRA Training Docker Usage Guide\n1\nOverview\nThis document describes how to load, start, and use the provided Docker image for Whisper\nLoRA training. The workflow includes dataset preparation, training, and evaluation.\n2\nLoading the Docker Image\nIf you received the prebuilt Docker image file whisper-lora-trainer.tar, load it using the\nfollowing command:\ndocker\nload -i whisper -lora -trainer.tar\nVerify that the image has been successfully loaded:\ndocker\nimages\n3\nStarting the Container\nAfte",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_111911_e55d92fe",
    "page": 1,
    "chunk_index": 1,
    "text": "image has been successfully loaded:\ndocker\nimages\n3\nStarting the Container\nAfter loading the image, start the container using its ID:\ndocker\nstart -ai 2ebbec699883\nNote: 2ebbec699883 refers to the container ID created from the image.\nOnce started, the container opens an interactive shell and the /workspace directory becomes\navailable.\n4\nTraining Workflow\nAfter entering the container and accessing the workspace, training can begin.\n4.1\nStep 1: Prepare Common Voice Dataset\npython\nprepare_cv_data",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_111911_e55d92fe",
    "page": 1,
    "chunk_index": 2,
    "text": "ning can begin.\n4.1\nStep 1: Prepare Common Voice Dataset\npython\nprepare_cv_data .py\nThis step prepares the Common Voice dataset by cleaning metadata and organizing audio\nfiles.\n4.2\nStep 2: Data Preprocessing\npython\npreparing_data .py\nThis step performs additional preprocessing required before training.\n1",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_111911_e55d92fe",
    "page": 2,
    "chunk_index": 0,
    "text": "4.3\nStep 3: Model Training\npython\ntrain.py\nThe training outputs, including checkpoints and logs, are saved in the outputs directory.\n5\nEvaluation\nAfter training is completed, the model can be evaluated using:\npython\neval.py\nThis script evaluates the trained model on the prepared evaluation dataset.\n6\nProject Directory Structure\nFigure 1: Project directory structure of the Whisper LoRA training container\n7\nSummary\nThe complete workflow is as follows:\n1. Load the Docker image\n2. Start the containe",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_111911_e55d92fe",
    "page": 2,
    "chunk_index": 1,
    "text": "complete workflow is as follows:\n1. Load the Docker image\n2. Start the container\n3. Prepare the dataset\n4. Preprocess the data\n2",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_111911_e55d92fe",
    "page": 3,
    "chunk_index": 0,
    "text": "5. Train the model\n6. Evaluate the model\nThis container is designed for offline Whisper LoRA training and evaluation.\n3",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 1,
    "chunk_index": 0,
    "text": "1 \n \nSuez Canal University \nFaculty of Engineering \nDepartment of Computers & Control \n  \nArabic Handwriting Recognition \n \nGraduation project report submitted to the \nFaculty of Engineering \nSuez Canal University \nIn partial fulfillment to the requirements for the \nDegree of Bachelor of Engineering in Computers & Control \n \nSupervised By \nProf. Khalid Abd El Salam \n \nPrepared By \nAyman Saber Gad \nAhmed Taha Ahmed Abd El Rahim \nAhmed Nagah Ahmed Mohamed \nMohammed Fathi Mohammed Mohammed Ali \nAba",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 1,
    "chunk_index": 1,
    "text": "d El Rahim \nAhmed Nagah Ahmed Mohamed \nMohammed Fathi Mohammed Mohammed Ali \nAbanoub Ayed Khalaf \nReem Mohammed Fouad \nRawan Gamal Mohammed \nNada Mahmoud Mohammed \nNada Hussein Asran \nKerollos Samir Fathi \nMohammed Abd El Fattah Fathi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 2,
    "chunk_index": 0,
    "text": "ACKNOWLEDGMENT\n \n2 \nACKNOWLEDGMENT \n \nWe would like to express our heartfelt gratitude and appreciation \nto all those who have contributed to the successful completion of this \nproject. Their unwavering support, guidance, and encouragement have \nbeen invaluable throughout this journey. \n \nFirst and foremost, we extend our deepest gratitude to the Almighty \nGod for granting us the strength, wisdom, and perseverance to \nundertake and complete this project. Without the divine blessings and \ngrace,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 2,
    "chunk_index": 1,
    "text": "undertake and complete this project. Without the divine blessings and \ngrace, none of this would have been possible. \n \nWe are immensely grateful to Dr. Khaled Abdelsalam, whose expertise \nand guidance have been instrumental in shaping the direction of this \nproject. Dr. Khaled's extensive knowledge, insightful feedback, and \nconstant motivation have been crucial in refining our ideas and \nmethodologies. Their mentorship has truly been a privilege, and we are \nindebted to their invaluable cont",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 2,
    "chunk_index": 2,
    "text": "orship has truly been a privilege, and we are \nindebted to their invaluable contributions. \n \nWe would like to extend our appreciation to our fellow teammates who \nhave collaborated on this project. Their commitment, hard work, and \ndedication have been instrumental in overcoming challenges and \nachieving our goals. We are grateful for the sense of unity we shared \nthroughout this endeavor. \n \nWe would also like to express our gratitude to our families and friends \nwho have provided unwavering e",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 2,
    "chunk_index": 3,
    "text": "xpress our gratitude to our families and friends \nwho have provided unwavering encouragement and understanding \nduring this project. Their love, patience, and belief in our abilities have \nbeen a constant source of motivation. \n \nLastly, we would like to acknowledge the contributions of all the \nindividuals who have provided assistance, feedback, or resources at \nvarious stages of this project. Your invaluable insights and support have \nenriched this work and made it possible to achieve our obje",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 2,
    "chunk_index": 4,
    "text": "ts and support have \nenriched this work and made it possible to achieve our objectives. \nThank you.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 3,
    "chunk_index": 0,
    "text": "PUBLICATIONS\n \n3 \nPUBLICATIONS \n \nThe Publications section of our graduation project book highlights the \nsignificant research contributions made by our team during our \nacademic journey. This section encompasses the various papers, \narticles, and conference presentations that showcase our dedication to \nadvancing knowledge in our field. Each publication reflects our \ncommitment to rigorous research methodologies, innovative problem-\nsolving, and the dissemination of findings to the broader acad",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 3,
    "chunk_index": 1,
    "text": "novative problem-\nsolving, and the dissemination of findings to the broader academic and \nprofessional communities. We take pride in the recognition and \nvalidation our work has received through these scholarly outputs, which \nunderscore our team's efforts and achievements. \n \nFirst Publication: We are proud to announce that our research, titled \"A \nComprehensive Approach to Arabic Handwriting Recognition: Deep \nConvolutional Networks and Bidirectional Recurrent Models for Arabic \nScripts\" was p",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 3,
    "chunk_index": 2,
    "text": "olutional Networks and Bidirectional Recurrent Models for Arabic \nScripts\" was presented at the 6th Annual Conference “Artificial \nIntelligence: \nFuture \nProspects \nfor \nAchieving \nthe \nSustainable \nDevelopment Goals” for Student Research and Creativity held on May 7, \n2024, at Suez Canal University. This study, conducted by Ayman Saber \nGad Mohammed, Ahmed Taha Ahmed Abd El Rahim, and Dr. Khaled Abd \nEl Salam from the Faculty of Engineering's Electrical Department, \nexplores advanced deep learn",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 3,
    "chunk_index": 3,
    "text": "he Faculty of Engineering's Electrical Department, \nexplores advanced deep learning techniques for Arabic handwriting \nrecognition. These outcomes demonstrate a significant advancement \nover traditional methods, highlighting our contribution to the field of \nArabic handwriting recognition.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 4,
    "chunk_index": 0,
    "text": "PUBLICATIONS\n \n4",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 5,
    "chunk_index": 0,
    "text": "PUBLICATIONS\n \n5 \nSecond Publication: We are also pleased to announce that our research, \ntitled \"A Comprehensive Approach to Arabic Handwriting Recognition: \nDeep Convolutional Networks and Bidirectional Recurrent Models for \nArabic Scripts,\" has been published in the International Journal of \nTelecommunications (IJT), Volume 15, on June 15, 2024. This study, \nauthored by Ayman Saber Gad Mohammed, Ahmed Taha Ahmed Abd El \nRahim, and Khaled Abd El Salam from the Faculty of Engineering's \nElectri",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 5,
    "chunk_index": 1,
    "text": "bd El \nRahim, and Khaled Abd El Salam from the Faculty of Engineering's \nElectrical Department, delves into cutting-edge deep learning \ntechniques for recognizing Arabic handwriting. By leveraging \nConvolutional Neural Networks (CNNs) and Bidirectional Long Short-\nTerm Memory (Bi-LSTM) networks, our method achieved outstanding \nresults, including a character error rate of roughly 2.96% and an \naccuracy rate of 97.04% on the KHATT dataset. These results \nsignificantly \nsurpass \ntraditional \nappro",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 5,
    "chunk_index": 2,
    "text": "on the KHATT dataset. These results \nsignificantly \nsurpass \ntraditional \napproaches, \nunderscoring \nour \nsubstantial contributions to the domain of Arabic handwriting \nrecognition.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 6,
    "chunk_index": 0,
    "text": "PUBLICATIONS\n \n6",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 7,
    "chunk_index": 0,
    "text": "ABSTRACT\n \n7 \nABSTRACT \n \n \nThis graduation project, titled \"Arabic Handwriting Recognition,\" \nwas undertaken by a team of 11 students at the Faculty of Engineering, \nSuez Canal University, under the supervision of Prof. Khalid Abd El Salam. \nThe project's primary objective was to develop a system capable of \nrecognizing and digitizing handwritten Arabic scripts. The system was \ndesigned to convert various types of documents, such as scanned paper \ndocuments, PDF files, or images captured by a d",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 7,
    "chunk_index": 1,
    "text": "ocuments, such as scanned paper \ndocuments, PDF files, or images captured by a digital camera, into \neditable and searchable data. \n \nThe project involved several stages, including data collection and \npreparation, system planning and design, system implementation, AI \ntraining, and results analysis. The team utilized various techniques and \ntechnologies, such as image preprocessing, character recognition \nmodels, Convolutional Neural Networks (CNNs), Recurrent Neural \nNetworks (RNNs), and deep",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 7,
    "chunk_index": 2,
    "text": "nvolutional Neural Networks (CNNs), Recurrent Neural \nNetworks (RNNs), and deep learning algorithms. \n \nDespite facing challenges such as the complexity of Arabic calligraphy, \nvariations in writing styles, and the lack of publicly available standardized \ndatasets for handwritten Arabic text, the team successfully developed a \nsystem that achieved high levels of accuracy and robustness in \nrecognizing handwritten Arabic text. The project not only contributes to \nthe field of Optical Character Re",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 7,
    "chunk_index": 3,
    "text": "bic text. The project not only contributes to \nthe field of Optical Character Recognition (OCR) but also opens up new \navenues for future research and development in the specialized subset \nof Arabic Handwriting Recognition (AHR).",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 0,
    "text": "LIST OF CONTENTS\n \n8 \nLIST OF CONTENTS \n \nACKNOWLEDGMENT .......................................................................................... 2 \nPUBLICATIONS ...................................................................................................... 3 \nABSTRACT ............................................................................................................... 7 \nLIST OF CONTENTS .........................................................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 1,
    "text": "..................................................................................... 8 \nLIST OF FIGURES ................................................................................................. 12 \nLIST OF ABBREVIATIONS ................................................................................... 14 \nLIST OF TABLES ................................................................................................... 15 \nCHAPTER 1 INTRODUCTION .........................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 2,
    "text": "........... 15 \nCHAPTER 1 INTRODUCTION ............................................................................. 17 \n1.1 \nOverview................................................................................................... 17 \n1.1.1 \nOCR Overview ................................................................................ 17 \n1.1.2 \nTypes Of OCR ................................................................................. 18 \n1.1.2.1 \nArabic Printed and Handwritten OCR .......",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 3,
    "text": "........................ 18 \n1.1.2.1 \nArabic Printed and Handwritten OCR ............................... 18 \n1.1.2.2 \nOnline Character Recognition System .............................. 19 \n1.1.2.3 \nOffline Character Recognition System ............................. 21 \n1.1.3 \nArabic handwriting Recognition Overview ............................. 21 \n1.2 Problem Definition.................................................................................. 22 \n1.2.1 \nImportance of OCR for Arabic langu",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 4,
    "text": ".................................. 22 \n1.2.1 \nImportance of OCR for Arabic language ................................. 23 \n1.2.2 \nArabic Handwriting Recognition Challenges .......................... 23 \n1.2.3 \nProblem Statement ...................................................................... 24 \n1.3 \nRelated Work ...................................................................................... 24 \n1.3.1 \nTraditional Approaches ...........................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 5,
    "text": "aditional Approaches ................................................................ 25 \n1.3.2 \nDeep Learning Advances ............................................................. 25 \n1.3.3 \nChallenges and Limitations ......................................................... 25 \n1.3.4 \nRecent Trends and Future Directions ....................................... 25 \n1.3.5 \nComparison with Existing Work ................................................. 26 \n1.3.5.1 \nGoogle Lens ...................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 6,
    "text": "................................... 26 \n1.3.5.1 \nGoogle Lens ............................................................................. 26 \n1.3.5.2 \nTesseract OCR ........................................................................ 27 \n1.3.5.3 \nSakhr .......................................................................................... 27 \nCHAPTER 2 SYSTEM PLANNING ..................................................................... 30 \n2.1 Requirement Gathering .....................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 8,
    "chunk_index": 7,
    "text": "............................ 30 \n2.1 Requirement Gathering ....................................................................... 30 \n2.2 \nProblems with Performing OCR ..................................................... 31",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 0,
    "text": "LIST OF CONTENTS\n \n9 \n2.2.1 \nGeometric Warping ...................................................................... 31 \n2.2.1.1 \nAngle of Taking the Picture .................................................. 31 \n2.2.1.2 \nCropping of the image .......................................................... 32 \n2.2.1.3 \nUnwarping of the Image ....................................................... 32 \n2.2.2 \nSegmentation Of Paragraphs ..................................................... 33 \nCHAPTER",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 1,
    "text": "Of Paragraphs ..................................................... 33 \nCHAPTER 3 SYSTEM DESIGN ............................................................................ 35 \n3.1 \nGeometric Unwrapping .................................................................... 35 \n3.1.1 \nThe Naive Approach...................................................................... 35 \n3.1.2 \nDocTr: Document Image Transformer ..................................... 37 \n3.2 \nParagraph Segmentation .............",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 2,
    "text": ".................................. 37 \n3.2 \nParagraph Segmentation ................................................................. 38 \n3.2.1 \nSegmentation Based on Histogram Projections .................... 39 \n3.2.2 \nIssues with Histogram Projection Segmentation ................... 41 \n3.3 \nCRAFT: Character Region Awareness for Text Detection ...... 42 \nCHAPTER 4 SYSTEM DEVELOPMENT ............................................................ 46 \n4.1 \nSegmentation System ......................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 3,
    "text": "............................ 46 \n4.1 \nSegmentation System ...................................................................... 46 \n4.2 \nRecognition System .......................................................................... 47 \n4.2.1 \nDataset ............................................................................................ 47 \n4.2.1.1 \nFiguring Out an Approach to Data Labeling ................... 48 \n4.2.1.2 \nImplementing The Labeling App: .....................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 4,
    "text": "8 \n4.2.1.2 \nImplementing The Labeling App: ....................................... 49 \n4.2.1.2.1 PHP ...................................................................................... 49 \n4.2.1.2.2 Image-Tag App Details ................................................... 51 \n4.2.1.2.3 Benefits Of Image-Tag App in Our Project: .............. 53 \n4.2.1.3 \nTagger  امّوس ................................................................................ 53 \n4.2.2 \nArtificial Neural Networks (ANNs)",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 5,
    "text": ".................................. 53 \n4.2.2 \nArtificial Neural Networks (ANNs) ............................................ 55 \n4.2.2.1 \nConvolutional Neural Network (CNN) .............................. 56 \n4.2.2.2 \nRecurrent Neural Network (RNN) ...................................... 56 \n4.2.2.3 Usage of CNN and RNN in OCR .......................................... 57 \n4.2.2.3.1 CNNs for OCR ................................................................... 57 \n4.2.2.3.2 RNNs for OCR ........",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 6,
    "text": "............................................ 57 \n4.2.2.3.2 RNNs for OCR ................................................................... 57 \n4.2.2.4 CTC Loss Layer ...................................................................... 58 \n4.2.2.5 Requirements For Implementing the System ................. 58 \n4.2.2.6 Nvidia’s RTX 3060 .................................................................. 59 \nCHAPTER 5 AI SYSTEM ......................................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 9,
    "chunk_index": 7,
    "text": "AI SYSTEM ....................................................................................... 62 \n5.1 \nData Collection & Preparation ......................................................... 62",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 0,
    "text": "LIST OF CONTENTS\n \n10 \n5.1.1 \nDataset Collection and Preparation .......................................... 63 \n5.1.2 \nData Augmentation Techniques ................................................ 63 \n5.2 \nDefining The Training Process ........................................................ 64 \n5.2.1 \nBatch Size ....................................................................................... 64 \n5.2.2 \nEpochs ...............................................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 1,
    "text": "............................................................................................. 65 \n5.2.3 \nOptimizer ........................................................................................ 65 \n5.2.4 Learning Rate ................................................................................. 65 \n5.3 \nDefining Model Evaluation .............................................................. 65 \n5.4 \nDealing with Characters’ Sequence: Bi-LSTM ............................ 66 \nCHAPTE",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 2,
    "text": "aling with Characters’ Sequence: Bi-LSTM ............................ 66 \nCHAPTER 6 APPLICATION ................................................................................ 71 \n6.1 \nOverview .............................................................................................. 71 \n6.2 \nFrontend and Backend Model ......................................................... 71 \n6.2.1 \nFrontend .......................................................................................... 72 \n6",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 3,
    "text": ".......................................................................... 72 \n6.2.1.1 \nNative Application .................................................................. 72 \n6.2.1.2 \nWeb Technology ..................................................................... 72 \n6.2.1.2.1 HTML / CSS / JS ................................................................ 73 \n6.2.1.2.2 React .................................................................................... 73 \n6.2.2 \nBackend .........",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 4,
    "text": "................................................... 73 \n6.2.2 \nBackend ........................................................................................... 76 \n6.2.2.1 \nDeveloping an API .................................................................. 76 \n6.2.2.2 \nFake API .................................................................................... 76 \n6.2.2.3 \nFastAPI ...................................................................................... 76 \n6.2.2.4 Flask ......",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 5,
    "text": "....................................................... 76 \n6.2.2.4 Flask ........................................................................................... 77 \n6.2.2.5 \nHardware Configuration ....................................................... 77 \nCHAPTER 7 AI SYSTEM IMPLEMENTATION .................................................. 80 \n7.1 \nOur Model’s Methodology .............................................................. 80 \n7.2 \nInitial Experiments: Prototype Model ..........",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 6,
    "text": "....................... 80 \n7.2 \nInitial Experiments: Prototype Model ............................................ 81 \n7.2.1 \nInitial Dataset .................................................................................. 81 \n7.2.2 \nExperiments And Results ............................................................. 82 \n7.2.2.1 \nEfficientNetB1 .......................................................................... 82 \n7.2.2.2 \nVGG19 ........................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 7,
    "text": ".... 82 \n7.2.2.2 \nVGG19 ....................................................................................... 83 \n7.2.2.3 \nResNet152 ................................................................................ 84 \n7.2.2.4 Training & Results .................................................................. 85 \n7.3 \nOptimized Final Model ...................................................................... 87 \n7.3.1 \nLarger Dataset ..........................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 10,
    "chunk_index": 8,
    "text": "7.3.1 \nLarger Dataset ................................................................................ 87 \n7.3.2 \nChosen Optimal Architecture: ResNet50V2 ...........................88",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 11,
    "chunk_index": 0,
    "text": "LIST OF CONTENTS\n \n11 \n7.3.3 \nResNet50V2 Arabic Alphabet Transfer Learning .................. 90 \n7.3.4 \nFinal Results ..................................................................................... 91 \nCHAPTER 8 CONCLUSION AND FUTURE WORK ........................................ 94 \n8.1 \nConclusion .......................................................................................... 94 \n8.2 \nFuture Work .................................................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 11,
    "chunk_index": 1,
    "text": "....................................................................................... 94 \n8.2.1 \nSearchable Handwritten Note Taking App ............................. 95 \n8.2.1.1 \nKey Features ........................................................................... 95 \n8.2.1.2 \nExisting Examples: ................................................................. 96 \n8.2.2 \nAutomatic Exam Grading Software ......................................... 96 \n8.2.2.1 \nCore Features: ...................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 11,
    "chunk_index": 2,
    "text": "................................ 96 \n8.2.2.1 \nCore Features: ........................................................................ 96 \nREFERENCES ........................................................................................................98 \n اﻟﻤﻠﺨﺺ ............................................................................................................... 108",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 0,
    "text": "LIST OF FIGURES\n \n12 \nLIST OF FIGURES \n \nFigure 1.1: Types of character recognition .................................................... 18 \nFigure 1.2: Sample handwritten and machine-printed Arabic texts. ...... 19 \nFigure 1.3: The Variations in Words & Characters in Arabic ...................... 23 \nFigure 2.1: Example of Document Warping ................................................... 32 \nFigure 3.1: Document Unwarping Using Hough Line Transform .............. 36 \nFigure 3.2: Examples fro",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 1,
    "text": "Unwarping Using Hough Line Transform .............. 36 \nFigure 3.2: Examples from DocTr .................................................................... 38 \nFigure 3.3: Example of line segmentation .................................................... 40 \nFigure 3.4: Example of word segmentation .................................................. 41 \nFigure 3.5: Diagram of histogram-based segmentation system ............ 41 \nFigure 3.6: Sample Results of CRAFT .................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 2,
    "text": "...... 41 \nFigure 3.6: Sample Results of CRAFT ............................................................. 43 \nFigure 3.7: Diagram of the final segmentation system ............................. 44 \nFigure 4.1: Segmentation Results of CRAFT................................................. 47 \nFigure 4.2: A Sample of a Full Paragraph from KHATT Dataset .............. 48 \nFigure 4.3: A draft of labeling app UI ............................................................. 49 \nFigure 4.4: PHP Datab",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 3,
    "text": "...................................................... 49 \nFigure 4.4: PHP Database Connection Diagram .......................................... 51 \nFigure 4.5: Main Page of Labeling APP ........................................................... 51 \nFigure 4.6: Main Page of Labeling APP ........................................................... 52 \nFigure 4.7: Login page of Tagger ................................................................... 54 \nFigure 4.8: Main page of Tagger .........",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 4,
    "text": ".................................. 54 \nFigure 4.8: Main page of Tagger ..................................................................... 54 \nFigure 4.9: Dashboard of Tagger ................................................................... 55 \nFigure 4.10: The General Architecture of ANNs ......................................... 55 \nFigure 4.11: Demonstration of CNN Layers and Architecture ................. 56 \nFigure 4.12: An Example of how RNNs Handle Text Sequences ............... 57 \nF",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 5,
    "text": "Figure 4.12: An Example of how RNNs Handle Text Sequences ............... 57 \nFigure 5.1: Model Development Process ....................................................... 62 \nFigure 5.2: Dataset Samples ............................................................................ 64 \nFigure 5.3: Demonstration of CER Metric .................................................... 66 \nFigure 5.4: Diagram of A Single LSTM Unit .................................................... 67 \nFigure 5.5: Bi-LST",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 6,
    "text": "Unit .................................................... 67 \nFigure 5.5: Bi-LSTM Architecture ....................................................................68 \nFigure 5.6: Diagram of the recognition system .......................................... 69 \nFigure 5.7: Diagram of entire system ............................................................ 69 \nFigure 6.1: Screenshot of Main Page ............................................................. 74 \nFigure 6.2: Screenshot of “our model",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 12,
    "chunk_index": 7,
    "text": "....................................... 74 \nFigure 6.2: Screenshot of “our model” page ................................................ 75 \nFigure 6.3: Screenshot of how results are displayed ................................. 75",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 13,
    "chunk_index": 0,
    "text": "LIST OF FIGURES\n \n13 \nFigure 7.1: Our Model’s Approach ................................................................... 81 \nFigure 7.2: Samples of Initial Dataset Showing Arabic Words .................. 82 \nFigure 7.3: EfficientNetB1 Architecture ......................................................... 83 \nFigure 7.4: VGG19 Architecture...................................................................... 84 \nFigure 7.5: ResNet152 Architecture ................................................",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 13,
    "chunk_index": 1,
    "text": "ure 7.5: ResNet152 Architecture ............................................................... 85 \nFigure 7.6: ResNet152 Performance Throughout the Epochs .................. 87 \nFigure 7.7: Example of Cosine Learning Rate Scheduler Effect ...............89 \nFigure 7.8: ResNet50V2 Performance on The Larger Dataset ................ 90 \nFigure 7.9: Demonstrating Transfer Learning Technique .......................... 91 \nFigure 7.10: ResNet50V2 Trained on Alphabet from KHATT Dataset ..... 92",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 14,
    "chunk_index": 0,
    "text": "LIST OF ABBREVIATIONS\n \n14 \nLIST OF ABBREVIATIONS \n \nAHAWP\nArabic Handwritten Alphabets, Words and Paragraphs\nAHR \nArabic Handwriting Recognition \nANN \nArtificial Neural Network \nBi-LSTM Bidirectional Long Short-Term Memory \nCER \nCharacter Error Rate \nCNN \nConvolutional Neural Network \nCPU \nCentral Processing Unit \nCRAFT \nCharacter Region Awareness for Text Detection \nCRUD \nCreate, Read, Update, Delete \nCTC \nConnectionist Temporal Classification \nDocTr \nDocument Image Transformer \nDOM \nDocument",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 14,
    "chunk_index": 1,
    "text": "onist Temporal Classification \nDocTr \nDocument Image Transformer \nDOM \nDocument Object Model \nFC \nFully connected \nGPU \nGraphics Processing Unit \nITC \nInk-to-Text Conversion \nKHATT \nKFUPM Handwritten Arabic TexT \nLLM \nLarge Language Models \nLSTM \nLong Short-Term Memory \nMCR \nMagnetic Character Recognition \nNLP \nNatural Language Processing \nOCR \nOptical Character Recognition \nOMR \nOptical Mark Recognition \nOOP \nObject-Oriented Programming \nPDO \nPHP Data Objects \nPHP \nHypertext Preprocessor \nRAM",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 14,
    "chunk_index": 2,
    "text": "-Oriented Programming \nPDO \nPHP Data Objects \nPHP \nHypertext Preprocessor \nRAM \nRandom Access Memory \nResNet \nResidual Network \nRNN \nRecurrent Neural Network \nSGD \nStochastic Gradient Descent \nVGG \nVisual Geometry Group",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 15,
    "chunk_index": 0,
    "text": "LIST OF TABLES\n \n15 \nLIST OF TABLES \n \nTable 1: Hyperparameters Used for each Experimented CNN ............... 85 \nTable 2: Comparison Between CNN Architectures Performance ..........86",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 16,
    "chunk_index": 0,
    "text": "16 \n \n \n \nCHAPTER 1\nINTRODUCTION",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 17,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n17 \n \nCHAPTER 1 INTRODUCTION \n1.1  Overview \n \n \nOptical Character Recognition (OCR) is a technology that enables \nthe conversion of different types of documents, such as scanned paper \ndocuments, PDF files, or images captured by a digital camera, into \neditable, searchable and machine-readable data [1][2][3][4][5]. OCR \nsystems work by analyzing the text in these documents and translating \nit into machine-encoded text. \nArabic Handwriting Recognition (AHR) is a special",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 17,
    "chunk_index": 1,
    "text": "it into machine-encoded text. \nArabic Handwriting Recognition (AHR) is a specialized subset of OCR \nfocused on recognizing and digitizing handwritten Arabic scripts [6][7]. \n \n1.1.1 OCR Overview \n \n \nOCR systems use various techniques to recognize and extract \ncharacters from images or scanned documents, allowing them to be \nedited, searched, and processed digitally [8]. \n \nOCR has greatly simplified the digitization process of printed \ndocuments, making it easier to convert large volumes of phy",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 17,
    "chunk_index": 2,
    "text": "process of printed \ndocuments, making it easier to convert large volumes of physical \ndocuments into electronic formats [3][9].  \n \nIt has also made it possible to extract text from images, such as \nphotographs or screenshots, enabling the recognition of text in various \ncontexts [10]. \n \nOCR is widely used as a form of data entry from printed paper data \nrecords. whether passport documents, invoices, bank statements, \ncomputerized receipts, business cards, mail, printed data, or any \nsuitable",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 17,
    "chunk_index": 3,
    "text": "s, \ncomputerized receipts, business cards, mail, printed data, or any \nsuitable documentation [3]. It is a common method of digitizing printed \ntexts so that they can be electronically edited, searched, stored more \ncompactly, displayed online, and used in machine processes such as \ncognitive computing, machine translation, (extracted) text-to-speech, \nkey data and text mining.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 18,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n18 \n \n1.1.2 Types Of OCR \n \n \nOptical character recognition (OCR) is usually referred to as an \noff-line character recognition process [11], meaning that the system \nscans and recognizes static images of the characters.  It refers to the \nmechanical or electronic translation of images of Handwritten \ncharacters or printed text into machine code without any variation. \n \nFigure 1.1 [12] shows types of OCR [13]. \n \n \nFigure 1.1: Types of character recognition \n \n1.1.2.1",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 18,
    "chunk_index": 1,
    "text": "types of OCR [13]. \n \n \nFigure 1.1: Types of character recognition \n \n1.1.2.1 \nArabic Printed and Handwritten OCR \n \nThe characters are written on paper or directly on a touch screen \nusing a pen or fingers. Printed OCR takes an image containing printed text \nas input and converts it into editable text.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 19,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n19 \nRecognizing Arabic handwritten text is more challenging than \nrecognizing printed text [14]. People have diverse ways of writing, which \nmakes it difficult even for a human to recognize it. \n \nFigure 1.2 [15] shows a sample of Arabic scripts. The handwritten script \nis usually written in cursive, even in Latin scripts. Segmenting cursive \nscripts is more complicated than segmenting printed scripts. \n \nThe handwritten script has many sizes, orientations, and resoluti",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 19,
    "chunk_index": 1,
    "text": "ed scripts. \n \nThe handwritten script has many sizes, orientations, and resolutions \ncompared with printed text, and there are no standard font sizes and \norientations. \n \n \nFigure 1.2: Sample handwritten and machine-printed Arabic texts. \n \n1.1.2.2 Online Character Recognition System \n \n \nOnline character recognition is recognizing handwriting recorded \nwith a digitizer as a time sequence of pen coordinates. The handwriting \nis captured and stored in digital form via different means in real tim",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 19,
    "chunk_index": 2,
    "text": "dwriting \nis captured and stored in digital form via different means in real time.  \nUsually, a special pen is used in conjunction with an electronic surface.  \nAs the pen moves across the surface, the two-dimensional coordinates \nof successive points are represented as a function of time and are \nstored in order. It is accepted that the online method of recognizing \nhandwritten text has achieved better results than its offline counterpart.  \nThis may be attributed to the fact that more informat",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 19,
    "chunk_index": 3,
    "text": "its offline counterpart.  \nThis may be attributed to the fact that more information may be \ncaptured in the online case such as the direction, speed, and the order \nof strokes of the handwriting.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 20,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n20 \nOnline handwriting recognition has several distinguishing features that \nmust be exploited to get more accurate results than online recognition \n[16][17][18]. \n \n It is adaptive: immediate feedback is given by the writer whose \ncorrections can be used to further train the recognizer. \n \n It is a real time process: It captures the temporal or dynamic \ninformation of the writing. This information consists of the number \nof pen strokes, the order of pen-strokes. The",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 20,
    "chunk_index": 1,
    "text": "formation consists of the number \nof pen strokes, the order of pen-strokes. The   direction of the \nwriting for each pen stroke and the speed of the writing within \neach pen stroke. \n \n Very little preprocessing is required. The operations such as \nsmoothing, and feature extraction operations such as the \ndetection of line orientations corner loops are easier and faster \nwith the pen trajectory data than on pixel images. \n \n Segmentation is easy: Segmentation operations are facilitated by \nusi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 20,
    "chunk_index": 2,
    "text": "ages. \n \n Segmentation is easy: Segmentation operations are facilitated by \nusing the pen lift information particularly for hand printed \ncharacters. \n \nOn the other hand, the disadvantages of the online character \nrecognition are as follows: \n \n The writer requires special equipment which is not as comfortable \nand natural to use as pen and paper. \n It cannot be applied to documents printed or written on papers, \npunching is much faster and easier than handwriting for small size \nalphabet su",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 20,
    "chunk_index": 3,
    "text": "punching is much faster and easier than handwriting for small size \nalphabet such as English or Arabic.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 21,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n21 \n1.1.2.3 Offline Character Recognition System \n \n \nOffline handwriting recognition refers to the process of \nrecognizing words that have been scanned from a surface (such as a \nsheet of paper) and are stored digitally in grayscale format [19]. After \nbeing stored, it is conventional to perform further processing to allow \nsuperior recognition. offline character recognition can be further \ngrouped into two types: \n Magnetic Character Recognition (MCR) \n Optical Char",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 21,
    "chunk_index": 1,
    "text": "grouped into two types: \n Magnetic Character Recognition (MCR) \n Optical Character Recognition (OCR) \n \n In MCR, the characters are printed with magnetic ink. The reading \ndevice can recognize the characters according to the unique magnetic \nfield of each character. MCR is mostly used in banks for check \nauthentication. OCR deals with the recognition of characters acquired \nby optical means, typically a scanner or a camera. The characters are in \nthe form of pixelated images, and can be eithe",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 21,
    "chunk_index": 2,
    "text": "a camera. The characters are in \nthe form of pixelated images, and can be either printed or handwritten, \nof any size, shape, or orientation. The OCR can be subdivided into \nhandwritten character recognition and printed character recognition. \nHandwritten Character Recognition is more difficult to implement than \nprinted character recognition due to diverse human handwriting styles \nand customs. In printed character recognition, the images to be \nprocessed are in the forms of standard fonts lik",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 21,
    "chunk_index": 3,
    "text": "recognition, the images to be \nprocessed are in the forms of standard fonts like Times New Roman. \n \n1.1.3 Arabic handwriting Recognition Overview \n \n \nArabic handwriting recognition focuses specifically on the \nconversion of handwritten Arabic text into machine-readable text. It \ninvolves the analysis of handwritten Arabic characters to accurately \nrecognize and interpret the handwritten content [20].",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 22,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n22 \nThe process of Arabic handwriting recognition typically involves several \nsteps: \nFirst, the handwritten document or text is scanned or captured to create \na digital image. Then, the recognition software analyzes the image and \napplies various techniques to segment the text into individual characters \nor words. These techniques may include stroke analysis, contour \ndetection, and shape recognition. \n \nOnce the text is segmented, the recognition software compares the",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 22,
    "chunk_index": 1,
    "text": "ecognition. \n \nOnce the text is segmented, the recognition software compares the \nshapes and patterns of the characters against a database of known \nArabic characters or employs machine learning algorithms to improve \nrecognition accuracy. These algorithms are trained on large datasets of \nlabeled Arabic handwriting samples to learn the variations and patterns \nin Arabic handwriting. \n \nArabic handwriting recognition has several applications across different \ndomains [7]. It can be used to digit",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 22,
    "chunk_index": 2,
    "text": "has several applications across different \ndomains [7]. It can be used to digitize handwritten Arabic documents, \nallowing for easier storage, search ability, and editing of the content. It \nalso enables the integration of Arabic handwriting input in devices such \nas tablets or smartphones, facilitating natural and convenient input \nmethods for Arabic users. Additionally, Arabic handwriting recognition \nplays a crucial role in text mining, information retrieval, and language \nprocessing tasks t",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 22,
    "chunk_index": 3,
    "text": "ial role in text mining, information retrieval, and language \nprocessing tasks that involve handwritten Arabic text. \n \n1.2  Problem Definition \n \n \nThe field of Arabic handwriting recognition has gained significant \ninterest in recent years due to its practical applications in various \ndomains, including document analysis, automatic text recognition, and \npostal services. The ability to accurately recognize handwritten Arabic \ntext poses a significant challenge due to the complexities of Arabic",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 22,
    "chunk_index": 4,
    "text": "ten Arabic \ntext poses a significant challenge due to the complexities of Arabic \ncalligraphy and the inherent variations in writing styles. Consequently, \nthe development of powerful and effective techniques for Arabic \nhandwriting recognition has become a pressing matter.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 23,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n23 \n1.2.1 Importance of OCR for Arabic language \n \n \nThe Arabic language is considered one of the most important \nlanguages, because: \n \n Arabic is the fifth most spoken language in the world [21]. \n Arabic is the Language of the Qur’an, the Holy Book of Islam. \n Rich Cultural History of Arabic in the World. \n Arabic is one of the oldest in the world with a wealth of knowledge \nthat   Archeologists to this day are still trying to uncover. \n Arabs have also made sig",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 23,
    "chunk_index": 1,
    "text": "cheologists to this day are still trying to uncover. \n Arabs have also made significant contributions in:                     \nLiterature, Mathematics, Navigation, and Architecture.  \n \n1.2.2 Arabic Handwriting Recognition Challenges \n  \n \nArabic handwriting recognition presents unique challenges shown \nin Figure 1.3 [22] compared to other scripts, including: \nArabic is a connected language, where letters are linked together, and \nthe same letter can have different forms depending on its positi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 23,
    "chunk_index": 2,
    "text": "together, and \nthe same letter can have different forms depending on its position \nwithin a word. \n \n \nFigure 1.3: The Variations in Words & Characters in Arabic",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 24,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n24 \nHandwritten Arabic exhibits significant variation among individuals due \nto factors such as writing style, spacing, and letter arrangement. These \nfactors contribute to increased difficulty in accurately recognizing \nhandwritten Arabic text. \n \nAmbiguities and irregularities in handwritten Arabic script, such as \nligatures, diacritics, and contextual variations. Despite the growing \ninterest in Arabic handwriting recognition, significant gaps and \nchallenges remain.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 24,
    "chunk_index": 1,
    "text": "rest in Arabic handwriting recognition, significant gaps and \nchallenges remain. Current recognition systems struggle to achieve \nacceptable accuracy and robustness, especially with unconstrained and \nnoisy handwritten Arabic text. Additionally, the lack of publicly available \nstandardized datasets for handwritten Arabic text hinders the \ndevelopment and evaluation of new algorithms and methodologies [23]. \n \n1.2.3 Problem Statement \n \n \nThe problem addressed in this project is the automation of",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 24,
    "chunk_index": 2,
    "text": "roblem Statement \n \n \nThe problem addressed in this project is the automation of grading \nhandwritten Arabic exams. Traditional methods of exam grading are \nlabor-intensive, time-consuming, and prone to errors. By leveraging \nadvanced handwriting recognition techniques, we aim to create a \nsystem that can accurately assess and score handwritten responses in \nArabic script, thereby eliminating the need for manual grading by \ninstructors.      \n  \n In this project, we have overcome these obstacles",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 24,
    "chunk_index": 3,
    "text": "ing by \ninstructors.      \n  \n In this project, we have overcome these obstacles, filled the gaps, and \nwe raised the level of efficiency in recognizing Arabic handwriting and \nbenefiting from its applications in several fields. \n \n1.3 Related Work \n \n \nArabic handwriting recognition (AHR) is a challenging task due to \nthe cursive nature of Arabic script and the wide variability in handwriting \nstyles. In this section, we review previous research efforts and \nmethodologies related to AHR.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 25,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n25 \n1.3.1 Traditional Approaches \n \n \nEarly AHR systems relied on classical pattern recognition and \nfeature extraction techniques [24][25][26]. These approaches involved \nsegmenting individual characters or words from handwritten text and \nthen extracting handcrafted features. While these methods achieved \nmoderate success, they struggled with handling the inherent variability \nand complexity of Arabic script [25][27][28]. \n \n1.3.2 Deep Learning Advances \n \n \nRecent ad",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 25,
    "chunk_index": 1,
    "text": "ty of Arabic script [25][27][28]. \n \n1.3.2 Deep Learning Advances \n \n \nRecent advancements in deep learning have led to significant \nimprovements in AHR performance [29]. Convolutional Neural Networks \n(CNNs)[30] and Recurrent Neural Networks (RNNs)[31] have shown \npromising results in recognizing Arabic handwritten text without the \nneed for explicit feature engineering. Models such as long short-term \nmemory (LSTM)[31][32] networks and attention mechanisms have been \napplied to capture long-ra",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 25,
    "chunk_index": 2,
    "text": "[31][32] networks and attention mechanisms have been \napplied to capture long-range dependencies and improve recognition \naccuracy, even for cursive Arabic script [33]. \n  \n1.3.3 Challenges and Limitations \n \n \nDespite the progress made in AHR, several challenges remain. \nVariations in handwriting styles, font types, and the lack of large-scale \nannotated datasets pose significant challenges to existing recognition \nsystems [34]. \n  \n1.3.4 Recent Trends and Future Directions \n \n \nEmerging trends",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 25,
    "chunk_index": 3,
    "text": "systems [34]. \n  \n1.3.4 Recent Trends and Future Directions \n \n \nEmerging trends in AHR involve integrating deep learning with \ntransfer and multi-task learning to enhance accuracy and generalization. \nReal-time online AHR systems are gaining traction, and creating public \nbenchmarks and datasets is crucial for advancing research.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 26,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n26 \n1.3.5 Comparison with Existing Work \n \n \nExisting AHR systems have shown promising results [35][36], but \nchallenges remain in unconstrained Arabic handwriting recognition. Our \napproach uses state-of-the-art deep learning techniques to address \nvariability in handwriting styles through robust feature representations \nand model architectures. Despite the significant variation in Arabic \nhandwriting, making accurate OCR recognition difficult, several \nprograms partia",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 26,
    "chunk_index": 1,
    "text": "handwriting, making accurate OCR recognition difficult, several \nprograms partially address this issue, which we will review. \n \n1.3.5.1 Google Lens \n \n \nGoogle Lens [37] is a popular OCR application designed to offer \ntext recognition capabilities. It uses image processing and deep learning \nalgorithms to accurately extract text from images captured by the \ndevice's camera. Google Lens can recognize various languages and \nfonts, making it versatile for diverse use cases. \n \nGoogle Lens leverage",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 26,
    "chunk_index": 2,
    "text": "s and \nfonts, making it versatile for diverse use cases. \n \nGoogle Lens leverages Google's vast dataset and machine learning \nmodels to continuously improve its accuracy and performance over \ntime. It is available as a standalone app and is also integrated into various \nGoogle services such as Google Photos and Google Assistant. \n \nLimitations: \n Not an open-source application. \n Limited Transparency: This lack of transparency can lead to \nuncertainty about the security and functionality of th",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 26,
    "chunk_index": 3,
    "text": "transparency can lead to \nuncertainty about the security and functionality of the software. \n Connectivity Dependency: Some features of Google Lens, such as \nreal-time translation, require an internet connection to function. \n Privacy Concerns.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 27,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n27 \n1.3.5.2 Tesseract OCR \n \n \nTesseract OCR [38] is currently an open-source OCR engine \noriginally developed by Hewlett-Packard (HP)[39] and later maintained \nby Google [40]. It is widely used for text recognition in various \napplications and has been integrated into numerous software projects \nand applications. \n \nKey Features: \n It’s Open Source; meaning anyone can use it or develop it further. \n Tesseract OCR is known for its high accuracy in recognizing \nprinted",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 27,
    "chunk_index": 1,
    "text": "further. \n Tesseract OCR is known for its high accuracy in recognizing \nprinted text in images, including complex fonts and languages. \n Language Support \n \nLimitations: \n Very limited support for handwriting recognition, especially with \nArabic. \n It has a somewhat complex setup. \n \n1.3.5.3 Sakhr \n \n \nSakhr [41] is a commercial software made by Sakhr Software, it \nsupports the Arabic language or the languages that use Arabic \ncharacters such as Farsi, Urdu Pashto. Sakhr claimed at the time",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 27,
    "chunk_index": 2,
    "text": "at use Arabic \ncharacters such as Farsi, Urdu Pashto. Sakhr claimed at the time to be \nthe best available OCR for Arabic. According to US evaluators, Sakhr had \n99.8% accuracy for the documents with high-quality images. \n \nKey Features: \n High accuracy for Arabic script recognition. \n Supports multiple languages using Arabic script. \n Recognizes both printed and handwritten text. \n Provides both online and offline recognition. \n \nLimitations: \n High cost due to being commercial software.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 28,
    "chunk_index": 0,
    "text": "CHAPTER 1 \nINTRODUCTION\n \n28 \n Limited support for non-Arabic scripts. \n Runs exclusively on Windows operating systems. \n \nGiven the limitations of each of these solutions, there is a clear need for \na different one; one that can address the unique challenges of \nunconstrained Arabic handwriting recognition. Our proposed approach \naims to fill this gap by leveraging state-of-the-art deep learning \ntechniques, robust feature representations, and innovative model \narchitectures. By focusing on t",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 28,
    "chunk_index": 1,
    "text": "t feature representations, and innovative model \narchitectures. By focusing on the variability in Arabic handwriting styles \nand employing comprehensive pre- and post-processing methods, our \nsolution seeks to achieve higher accuracy and adaptability, making it a \nmore effective tool for diverse applications.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 29,
    "chunk_index": 0,
    "text": "29 \n \n \n \nSYSTEM PLANNING\nCHAPTER 2",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 30,
    "chunk_index": 0,
    "text": "CHAPTER 2 \nSYSTEM PLANNING\n \n30 \nCHAPTER 2 SYSTEM PLANNING \n \n \nSystem planning is a critical phase in the development of any \nsoftware project. It involves defining the objectives, scope, and \nrequirements of the system to ensure that it meets the needs of its \nusers. In the context of our Arabic Handwritten OCR project, careful \nplanning is essential to handle the complexities of recognizing \nhandwritten Arabic characters and ensuring the system is robust, \naccurate, and user-friendly. \n \n2.1",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 30,
    "chunk_index": 1,
    "text": "racters and ensuring the system is robust, \naccurate, and user-friendly. \n \n2.1 Requirement Gathering \n \n \nRequirement gathering is the process of identifying the needs and \nconstraints of the stakeholders involved in the project [42]. This phase \ninvolves collecting detailed and specific information to ensure that the \nsystem we develop aligns with the expectations and requirements of its \nusers. \n \nThe primary requirements for our Arabic Handwritten OCR system are \nas follows: \n \n Operate on",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 30,
    "chunk_index": 2,
    "text": "irements for our Arabic Handwritten OCR system are \nas follows: \n \n Operate on mobile devices: The program should be optimized to \nrun efficiently on various mobile devices, including smartphones \nand tablets. \n Recognize handwritten Arabic characters from any angle: The \nsystem must accurately recognize handwritten Arabic characters \nregardless of the orientation in which they are written or scanned. \n Preserve errors in the written words without correcting them: The \nOCR system should captu",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 30,
    "chunk_index": 3,
    "text": "rrors in the written words without correcting them: The \nOCR system should capture the handwritten text as it is, including \nany mistakes, and not attempt to correct them. This is crucial for \napplications where the original written form, including errors, must \nbe retained, like the automatic exam grading application for \nexample.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 31,
    "chunk_index": 0,
    "text": "CHAPTER 2 \nSYSTEM PLANNING\n \n31 \n2.2 Problems with Performing OCR \n \n \nDeveloping Optical Character Recognition (OCR) for Arabic \npresents unique challenges due to the linguistic and script \ncharacteristics of the Arabic language [43]. Addressing these challenges \nrequires a combination of advanced image processing techniques, \nrobust language models, and extensive training data tailored specifically \nfor the Arabic language and its various degrees. Continued research and \ndevelopment in these a",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 31,
    "chunk_index": 1,
    "text": "language and its various degrees. Continued research and \ndevelopment in these areas are crucial for improving the accuracy and \nreliability of Arabic OCR systems. \n \n2.2.1 Geometric Warping \n \n \nGeometric unwarping is a critical pre-processing step in the \ndevelopment of Optical Character Recognition (OCR) systems, \nparticularly for languages such as Arabic. The process involves \ncorrecting distortions in the scanned images of documents to ensure \nthat the text is accurately recognized. \n \n2.2.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 31,
    "chunk_index": 2,
    "text": "d images of documents to ensure \nthat the text is accurately recognized. \n \n2.2.1.1 \nAngle of Taking the Picture \n \n Distortion and Perspective: The angle at which a photo is taken can \nintroduce distortion and perspective issues. Text may appear \nskewed or warped, making it harder for optical character \nrecognition (OCR) software to accurately interpret characters. \n \n Reflections and Glare: Depending on the lighting conditions and \nangle, reflections and glare can obscure parts of the text,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 31,
    "chunk_index": 3,
    "text": "ing conditions and \nangle, reflections and glare can obscure parts of the text, reducing \nOCR accuracy.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 32,
    "chunk_index": 0,
    "text": "CHAPTER 2 \nSYSTEM PLANNING\n \n32 \n2.2.1.2 Cropping of the image \n \n Incomplete Text Capture: Improper cropping can lead to parts of \nthe text being cut off or obscured. OCR systems rely on a clear \nview of the entire text to accurately recognize and extract \ncharacters. \n \n Background Noise: Unimportant details or backgrounds in the \nimage can interfere with OCR algorithms, causing them to \nmisinterpret text or ignore it altogether. \n \n2.2.1.3 Unwarping of the Image \n \n \nImage Distortion Correc",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 32,
    "chunk_index": 1,
    "text": "re it altogether. \n \n2.2.1.3 Unwarping of the Image \n \n \nImage Distortion Correction: Images taken from an angle may \nneed correction to straighten lines of text as shown in Figure 2.1 [44], \nwhich is crucial for accurate OCR. Distorted text lines can confuse OCR \nalgorithms, leading to errors in text extraction. \n \nQuality of Image Processing: The process of unwarping an image \ninvolves computational techniques to correct perspective and \ndistortion, which can vary in effectiveness depending on",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 32,
    "chunk_index": 2,
    "text": "orrect perspective and \ndistortion, which can vary in effectiveness depending on the software \nused and the complexity of the image. \n \n \nFigure 2.1: Example of Document Warping",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 33,
    "chunk_index": 0,
    "text": "CHAPTER 2 \nSYSTEM PLANNING\n \n33 \n2.2.2 Segmentation Of Paragraphs \n \n \nCreating a model to extract text from entire paragraphs in Arabic \nOCR involves handling an enormous range of variations in paragraph \nstructure, making it impracticable to create a model that can classify and \nextract text from every possible paragraph directly. The infinite \nvariations necessitate a highly generalized model, making any small \namount of data completely impractical to work with. To recognize text \nin paragrap",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 33,
    "chunk_index": 1,
    "text": "ount of data completely impractical to work with. To recognize text \nin paragraphs, the process should involve preprocessing; starting with \nline segmentation to isolate individual lines of text, word segmentation \nto break down lines into “islands”, meaning words or sub-words, using \nspace detection algorithms.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 34,
    "chunk_index": 0,
    "text": "34 \n \n \n \nCHAPTER 3\nSYSTEM DESIGN",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 35,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n35 \nCHAPTER 3 SYSTEM DESIGN \n \n \nOur system comprises two primary components: an image \npreprocessing module and a character recognition model. The \npreprocessing module is crucial because the recognition model is \noptimized to handle only the smallest possible segments of the image. \nIn our Arabic Handwritten OCR Project, the preprocessing step prepares \nand segments the input images. This allows the recognition model to be \ntrained more practically on a smaller data",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 35,
    "chunk_index": 1,
    "text": "allows the recognition model to be \ntrained more practically on a smaller data while achieving high accuracy. \n \n3.1 Geometric Unwrapping \n \n \nGeometric unwrapping is a crucial preprocessing step in Arabic \nHandwritten OCR to ensure that the text is properly aligned and \nreadable for the OCR system. The main challenge addressed here is the \ndistortion caused when images of handwritten text are taken at an \nangle, which can lead to skewed and curved text lines. To correct this, \nwe need to strai",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 35,
    "chunk_index": 2,
    "text": "ich can lead to skewed and curved text lines. To correct this, \nwe need to straighten the text lines and flatten the paper surface. \n \n3.1.1 The Naive Approach \n \n \nInitially, we employed a straightforward method using OpenCV to \ndetect and correct the skew in our images. This approach involved \ndetecting straight lines within the image and using these lines to crop \nand align the image. Specifically, we used OpenCV's Hough Line \nTransform to detect the edges of the paper [45]. This method relie",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 35,
    "chunk_index": 3,
    "text": "s Hough Line \nTransform to detect the edges of the paper [45]. This method relies on \nthe assumption that the paper's edges are straight and can be used as a \nreference to realign the text. The steps were as follows: \n \n1. Convert the image to grayscale to reduce complexity. \n2. Apply edge detection using the Canny edge detector. \n3. Use the Hough Line Transform to identify the lines in the image. \n4. Identify the lines that correspond to the paper's borders. \n5. Crop and warp the image based on",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 35,
    "chunk_index": 4,
    "text": "nes that correspond to the paper's borders. \n5. Crop and warp the image based on these lines to obtain a \nstraightened text image.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 36,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n36 \n \n \nFigure 3.1: Document Unwarping Using Hough Line Transform \n \n \nHowever, this naive approach has several limitations. First, it \nrequires the entire paper to be visible within the image. If any part of the \npaper is cut off or obscured, the line detection fails, resulting in an error. \nSecond, the background must be clearly distinguishable from the paper. \nIf the paper's background is cluttered or similar in color and texture, the \nedge detection can be unreliab",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 36,
    "chunk_index": 1,
    "text": "s cluttered or similar in color and texture, the \nedge detection can be unreliable, leading to poor alignment and \ncropping. Additionally, this method assumes the paper is perfectly flat, \nwhich is rarely the case in real-world scenarios where paper might be \ncrumpled or curved.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 37,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n37 \n3.1.2 DocTr: Document Image Transformer \n \n \nTo address the limitations of the naive approach, we adopted a \nstate-of-the-art method called DocTr: Document Image Transformer \n[46][47][48][49]. DocTr leverages deep learning techniques to perform \nboth geometric unwarping and illumination correction, ensuring high-\nquality preprocessing of handwritten document images.  \n \nDocTr utilizes a transformer-based model to understand and correct \ndistortions in document imag",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 37,
    "chunk_index": 1,
    "text": "transformer-based model to understand and correct \ndistortions in document images. The model is trained on a large dataset \nof document images, learning to recognize and rectify common issues \nsuch as geometric distortions and uneven lighting. By analyzing the \nspatial relationships within the image, the transformer can accurately \npredict the necessary transformations to straighten the text and \nnormalize the lighting, resulting in a clear and well-aligned image. \n \nThe process begins with the",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 37,
    "chunk_index": 2,
    "text": "ing, resulting in a clear and well-aligned image. \n \nThe process begins with the model taking an image of the handwritten \ndocument and generating a set of transformation parameters. These \nparameters include adjustments for geometric distortions, such as skew \nand curvature. The image is then processed using these parameters, \nproducing a flattened and uniformly lit output. This advanced \npreprocessing technique significantly enhances the quality of the input \nimage for subsequent OCR tasks, le",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 37,
    "chunk_index": 3,
    "text": "nificantly enhances the quality of the input \nimage for subsequent OCR tasks, leading to more accurate recognition \nof handwritten text, an illustrative example of this process can be found \nin Figure 3.2 [46].",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 38,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n38 \n \nFigure 3.2: Examples from DocTr \n \n \nBy employing DocTr, we overcome the challenges posed by the \nnaive approach and achieve a more robust preprocessing pipeline. This \nmethod does not require the entire paper to be visible or the \nbackground to be clearly different from the paper, making it suitable for \na wide range of real-world scenarios. Additionally, the use of a \ntransformer model allows for precise and efficient corrections, ensuring \nthat our OCR system",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 38,
    "chunk_index": 1,
    "text": "del allows for precise and efficient corrections, ensuring \nthat our OCR system receives the best possible input for accurate text \nrecognition. \n \n3.2 Paragraph Segmentation \n \n \nEffective OCR begins with a structured approach to text \nextraction and analysis. First, the text is split into paragraphs using line \nbreaks and indentation. Each paragraph is then divided into lines, and \neach line into words, separated by spaces or punctuation. Finally, a \ndetailed analysis at the sub-word level exa",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 38,
    "chunk_index": 2,
    "text": "y spaces or punctuation. Finally, a \ndetailed analysis at the sub-word level examines characters and \nfeatures. This systematic breakdown allows OCR algorithms to \naccurately extract and interpret the document's content, supporting \nvarious applications.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 39,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n39 \n3.2.1 Segmentation Based on Histogram Projections \n \n \nOur system relies on the utilization of histogram projections to \naccurately extract textual information from document images. The first \nstep in this technique is to obtain the vertical histogram of the white \npixels within the image. Vertical histogram represents the distribution of \npixel intensities or densities along the vertical axis of the document, \nproviding valuable insights into the locations of text",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 39,
    "chunk_index": 1,
    "text": "al axis of the document, \nproviding valuable insights into the locations of text lines. This helps to \nidentify the locations of individual characters or lines of text within a \ndocument image [50][51]. \n \nWith the vertical histogram in hand, the next step is to calculate a \nsuitable threshold. Our threshold was based on the weighted average of \nthe histogram values. This is done by computing the vertical histogram \nof the image and then calculating the weighted average of the \nhistogram by mult",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 39,
    "chunk_index": 2,
    "text": "of the image and then calculating the weighted average of the \nhistogram by multiplying each histogram value with its corresponding \nrow index (y-coordinate) and then dividing the sum of these products \nby the total sum of the histogram values. \n \nThis threshold helps identify the gaps or spaces between text lines, \nwhich are considered as the \"valleys\" in the histogram. By isolating these \nvalley regions, the system can effectively segment the image into \nindividual text lines for further proce",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 39,
    "chunk_index": 3,
    "text": "can effectively segment the image into \nindividual text lines for further processing as shown in Figure 3.3. \n \nOnce the text lines have been extracted, the algorithm then employs a \npathfinding technique, to traverse the identified line regions from right \nto left. During this process, the system can discard any extraneous \nelements that fall outside the designated line areas, further refining the \nsegmentation. We used the A* (A-star) algorithm as a pathfinding \ntechnique [52][53]. It is a ty",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 39,
    "chunk_index": 4,
    "text": "used the A* (A-star) algorithm as a pathfinding \ntechnique [52][53]. It is a type of informed search algorithm that uses \nheuristics to estimate the cost of reaching the goal from a given node. \nIt is used to find the shortest or optimal path between two points in a \ngraph or a grid-based environment by maintaining a set of nodes to be \nexplored, known as the \"open set,\" and a set of nodes that have already \nbeen explored, known as the \"closed set.\" The algorithm starts by adding",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 40,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n40 \nthe starting node to the open set and then repeatedly selects the node \nwith the lowest estimated total cost (the sum of the actual cost to reach \nthat node and the estimated cost to reach the goal) from the open set, \nmoves it to the closed set, and then explores its neighbors. \n \n \nFigure 3.3: Example of line segmentation \n \n \nWith the text lines now accurately segmented, we turned our \nattention to the horizontal dimension to extract words, utilizing a similar",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 40,
    "chunk_index": 1,
    "text": "r \nattention to the horizontal dimension to extract words, utilizing a similar \napproach except using the horizontal histogram instead of vertical. This \nhistogram projects the pixel distributions along the horizontal axis, \nenabling the identification of individual words within each line of text. By \napplying thresholding and pathfinding techniques, the system can \neffectively separate the words, preparing them for character-level \nrecognition. \n \nFinally, the segmentation process extends to th",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 40,
    "chunk_index": 2,
    "text": "character-level \nrecognition. \n \nFinally, the segmentation process extends to the sub-word or island \nlevel, where each word is further broken down into its constituent \ncharacters or subcomponents. This granular analysis is crucial for \naccurate optical character recognition, as it allows the system to \nidentify and interpret the individual features that make up the textual \ncontent. By systematically decomposing the text into these hierarchical \ncomponents, the OCR algorithm can achieve a high",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 40,
    "chunk_index": 3,
    "text": "e text into these hierarchical \ncomponents, the OCR algorithm can achieve a high degree of accuracy \nin extracting and interpreting the written information and Figure 3.4 \nshows how word segmentation works.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 41,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n41 \n \nFigure 3.4: Example of word segmentation \n \nFigure 3.5 Summarizes the system procedure in a diagram. \n \n \nFigure 3.5: Diagram of histogram-based segmentation system \n \n3.2.2 Issues with Histogram Projection Segmentation \n \n \nDuring the preprocessing stage, we encountered two main \nchallenges \nwith \nthe \nhistogram \nprojection-based \nsegmentation \napproach. These problems are outlined as follows: \n \nOne major limitation is the inability to effectively handle skewed",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 41,
    "chunk_index": 1,
    "text": "s follows: \n \nOne major limitation is the inability to effectively handle skewed or \nslanted text lines. The vertical histogram, which is the foundation of this \ntechnique, relies on the assumption that text lines are oriented \nhorizontally. However, in the presence of skewed text, the histogram \npeaks may not accurately represent the true line positions, leading to",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 42,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n42 \ninaccurate segmentation. This issue becomes particularly problematic \nwhen dealing with handwritten or scanned documents, where text lines \ncan exhibit significant skewness due to various factors, such as uneven \npage orientation or the user's natural handwriting inclination. \n \nThe second challenge is the difficulty in determining the appropriate \nthreshold for separating text lines and words. The method typically relies \non identifying peaks in the vertical histo",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 42,
    "chunk_index": 1,
    "text": "d words. The method typically relies \non identifying peaks in the vertical histogram to locate the text lines, but \nthe selection of the threshold value can be a complex task, especially in \nthe presence of varying text densities or irregular spacing between \nwords. If the threshold is set too high, it may fail to detect some text \nlines, while a low threshold can result in over-segmentation, where \nindividual words are incorrectly identified as separate lines. This problem \nis further exacerbat",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 42,
    "chunk_index": 2,
    "text": "are incorrectly identified as separate lines. This problem \nis further exacerbated in documents with complex layouts, such as \nthose with text interleaved with graphics or tables, where the histogram \nmay not provide a clear distinction between text and non-text regions. \n \n3.3 CRAFT: Character Region Awareness for Text Detection \n \n \nCRAFT, or Character Region Awareness for Text Detection, is a \nmodel designed to detect text in images [54]. It operates by generating \na heat map that highlights",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 42,
    "chunk_index": 3,
    "text": "tect text in images [54]. It operates by generating \na heat map that highlights areas in the image where text is likely to be \npresent (Figure 3.6). This heat map assists in identifying and localizing \ntext within the image, making it a valuable tool for text detection in \nvarious contexts, including OCR for handwritten documents.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 43,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n43 \n \nFigure 3.6: Sample Results of CRAFT \n \n \nOnce the text regions are identified using CRAFT, thresholding is \napplied to these regions to enhance the visibility of the text. \nThresholding is a process that converts grayscale images into binary \nimages by turning all pixels below a certain threshold value to black and \nall pixels above that threshold to white. This binarization simplifies \nfurther processing steps by reducing the complexity of the image and \nemphasi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 43,
    "chunk_index": 1,
    "text": "s \nfurther processing steps by reducing the complexity of the image and \nemphasizing the text areas. \n \nAfter thresholding, histogram segmentation is employed to segment \nthe image into individual words or sub-words. This involves analyzing the \nvertical and horizontal histograms of pixel intensities to detect gaps and \nboundaries between text regions. Specifically, repeated vertical \ncolumns of pixels are analyzed from right to left. Columns with a sum of \nzero indicate empty spaces, which are",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 43,
    "chunk_index": 2,
    "text": "rom right to left. Columns with a sum of \nzero indicate empty spaces, which are used as cut points to segment \nthe text. This method effectively breaks down the text into manageable \nparts for further processing. \n \nThis method addresses both the issue of image angle and paragraph size \neffectively. The initial challenge was correcting the image angle to \nensure all text lines were horizontal and properly aligned. Using CRAFT,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 44,
    "chunk_index": 0,
    "text": "CHAPTER 3 \nSYSTEM DESIGN\n \n44 \nthe model can detect text regions even if the image is taken at an angle, \nallowing for more flexible input conditions. Additionally, the \nsegmentation approach helps manage varying paragraph sizes by \nprecisely identifying text boundaries without relying on predefined \ntemplates or extensive preprocessing, Figure 3.7 shows the diagram of \nthe CRAFT based system. \n \n \nFigure 3.7: Diagram of the final segmentation system \n \n \nAlthough applying CRAFT introduces an ad",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 44,
    "chunk_index": 1,
    "text": "m of the final segmentation system \n \n \nAlthough applying CRAFT introduces an additional computational \nstep, the overall accuracy and robustness of the OCR system are \nsignificantly improved, making it a worthwhile compromise. \n \nIn summary, the integration of the CRAFT model significantly enhances \nthe preprocessing for Arabic handwritten OCR. By accurately detecting \ntext regions and handling image angles, CRAFT ensures proper text \nalignment. The histogram segmentation technique further refi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 44,
    "chunk_index": 2,
    "text": "nsures proper text \nalignment. The histogram segmentation technique further refines the \ninput by precisely cutting the smallest unit of text possible. This \npreprocessing pipeline produces clean, well-segmented text images, \nperfectly primed for the recognition system. Overall, these steps ensure \nhigher accuracy and reliability in the OCR outputs.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 45,
    "chunk_index": 0,
    "text": "45 \n \n \n \nCHAPTER 4\nSYSTEM DEVELOPMENT",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 46,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n46 \nCHAPTER 4 SYSTEM DEVELOPMENT \n \n \nWith the system design completed, the next crucial step is the \nimplementation phase. This stage involves integrating various \ntechnologies and algorithms tailored to handle the complexities of \nArabic script. It requires meticulous attention to detail, from configuring \nimage processing libraries like OpenCV for segmentation to fine-tuning \ndeep learning models for accurate character recognition. Collaborative \nefforts betwee",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 46,
    "chunk_index": 1,
    "text": "earning models for accurate character recognition. Collaborative \nefforts between experts are essential to refine the system iteratively \nand ensure optimal performance across diverse handwritten texts and \ninput conditions. Rigorous testing and validation will pave the way for a \nrobust OCR solution tailored to Arabic handwritten documents. \n \n4.1 Segmentation System \n \n \nWe utilized OpenCV for the initial segmentation system of our \nArabic Handwritten OCR project. The system was meticulously",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 46,
    "chunk_index": 2,
    "text": "ion system of our \nArabic Handwritten OCR project. The system was meticulously \nprogrammed to segment text lines and words within an image using the \naforementioned image processing techniques. \n \nAnd then, to manage the complex requirements of text detection, we \nimplemented CRAFT (Character Region Awareness for Text detection) \nvia the KerasOCR library [55][56], which includes a TensorFlow-\ncompatible implementation of CRAFT able to segment words or sub-\nwords efficiently as shown in Figure 4.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 46,
    "chunk_index": 3,
    "text": "n of CRAFT able to segment words or sub-\nwords efficiently as shown in Figure 4.1.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 47,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n47 \n \nFigure 4.1: Segmentation Results of CRAFT \n \n4.2 Recognition System \n \n \nThe recognition system in our Arabic Handwritten OCR project is \ndesigned to accurately transcribe Arabic handwritten text into machine-\nreadable format. Leveraging machine learning models and deep neural \nnetworks, the system analyzes input images of handwritten Arabic text \nto identify and convert characters into digital text. This process involves \ntraining models on labeled datasets",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 47,
    "chunk_index": 1,
    "text": "rs into digital text. This process involves \ntraining models on labeled datasets to recognize the intricate variations \nand styles inherent in Arabic handwriting, enabling the OCR system to \nachieve high accuracy in text recognition tasks. \n \n4.2.1 Dataset \n \n \nIn our search for a pre-labeled dataset of Arabic handwritten text, \nwe found that no suitable dataset was publicly available. \n \nMost of the publicly available datasets seem to only contain characters \nand their different shapes. We need",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 47,
    "chunk_index": 2,
    "text": "le datasets seem to only contain characters \nand their different shapes. We needed to include the largest amount of \nwords possible.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 48,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n48 \nConsequently, we had to create our own dataset derived from the \npublicly available KHATT dataset [57][58]. \n \nThe KHATT dataset consists of several paragraphs like the one \ndemonstrated in Figure 4.2, the whole paragraph is labeled but due to \nour segmentation system, each paragraph gets segmented in a \ndifferent way, so we had to label the new data manually. \n \n \nFigure 4.2: A Sample of a Full Paragraph from KHATT Dataset \n \n4.2.1.1 Figuring Out an Approach",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 48,
    "chunk_index": 1,
    "text": "mple of a Full Paragraph from KHATT Dataset \n \n4.2.1.1 Figuring Out an Approach to Data Labeling \n \nWe were faced with the challenge of labeling about 200,000 samples, \nwhich was a daunting task to complete manually within a month. As a \nresult, we needed to find an application that would enable us to \ncollaborate on the data labeling process from any location. \n \nAfter exploring several options, we found that the available applications \nwere either too complex, designed for broader data labelin",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 48,
    "chunk_index": 2,
    "text": "ailable applications \nwere either too complex, designed for broader data labeling purposes \nsuch as image recognition and segmentation, or were commercially \noriented with expensive licenses and long-term commitments. \nConsequently, we made the decision to create our own labeling \napplication that would support collaboration, streamline data entry, and \nfacilitate quick data review.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 49,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n49 \nWe quickly drafted a prototype UI (see Figure 4.3) that resembles a \nspreadsheet, featuring images on the left and a text input field on the \nright. \n \n \nFigure 4.3: A draft of labeling app UI \n \n4.2.1.2 Implementing The Labeling App: \n \n \nThe goal of this application is to help us in the process of data \nlabeling which is used for training the model. The idea of this application \nis to display a set of images from the KHATT dataset, then we add the \nappropria",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 49,
    "chunk_index": 1,
    "text": "is to display a set of images from the KHATT dataset, then we add the \nappropriate label for each image, and by pressing enter that label is \nsaved in our database. \n  \nWe used HTML, CSS and JavaScript for the frontend implementation \nand PHP for backend implementation for our web application. \n \n4.2.1.2.1 PHP \n \n \nPHP (Hypertext Preprocessor) is a widely used, open-source, \nserver-side \nscripting \nlanguage \nspecifically \ndesigned \nfor \nweb \ndevelopment. PHP code is executed on the server, gener",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 49,
    "chunk_index": 2,
    "text": "ally \ndesigned \nfor \nweb \ndevelopment. PHP code is executed on the server, generating HTML that",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 50,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n50 \nis then sent to the client. The client receives the results of the script but \nremains unaware of the underlying code [59]. \n \nPHP is supported by a large community of developers who continuously \ncontribute to its development and maintenance, PHP benefits from the \nadvantages of being open source. Developers have access to the source \ncode and can modify and customize it to suit their needs. Additionally, \nPHP supports object-oriented programming (OOP), allow",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 50,
    "chunk_index": 1,
    "text": "heir needs. Additionally, \nPHP supports object-oriented programming (OOP), allowing developers \nto create reusable code and streamline development processes. \n \nOne of PHP's strengths is its low learning curve. Its simple syntax, similar \nto C and other popular programming languages, allows developers to \nquickly learn and start building web applications. \n \nA key feature of PHP is its rich set of built-in functions and libraries for \nvarious tasks. Whether working with databases, handling files",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 50,
    "chunk_index": 2,
    "text": "and libraries for \nvarious tasks. Whether working with databases, handling files, or \nprocessing forms, PHP offers functions that can significantly speed up \ndevelopment. \n \nPHP provides robust features and functions for efficient data storage \nand retrieval. PHP supports a wide range of databases, including MySQL, \nPostgreSQL, SQLite, and Oracle. It offers built-in functions like mysqli \nand PDO (PHP Data Objects) that simplify connecting to databases, \nexecuting queries, and ensuring secure an",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 50,
    "chunk_index": 3,
    "text": "hat simplify connecting to databases, \nexecuting queries, and ensuring secure and reliable data access. PHP's \nseamless integration with databases through its OOP capabilities allows \ndevelopers to create classes and objects representing database tables, \nmaking it easier to interact with data and perform CRUD (Create, Read, \nUpdate, Delete) operations.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 51,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n51 \n \nFigure 4.4: PHP Database Connection Diagram \n \n4.2.1.2.2 Image-Tag App Details \n \n \nOur app efficiently loads and displays images sourced from a \nspecified directory, presenting them in a user-friendly interface tailored \nfor seamless labeling. This interface allows users to input accurate labels \ndirectly, with the entered data promptly saved into the database for \nstreamlined data management. \n \n \nFigure 4.5: Main Page of Labeling APP",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 52,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n52 \n \n \nFigure 4.6: Main Page of Labeling APP \n \n \nThe app helps in manual data Labeling as Users are empowered to \nprovide precise labels for each image via the intuitive interface. This \nmanual labeling process ensures data accuracy and reliability, which in \nturn significantly enhances the model's precision and overall \neffectiveness. \n  \nTo enhance user experience, the app supports a pagination feature that \norganizes image viewing into manageable subsets per",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 52,
    "chunk_index": 1,
    "text": "a pagination feature that \norganizes image viewing into manageable subsets per page. This \nfeature simplifies the labeling process by allowing users to smoothly \nnavigate between pages, facilitating efficient viewing and labeling of a \nlarger image set.  \n \nAn essential feature of our app is its capability to export labeled data into \na CSV file. This export functionality is instrumental for model training, \nproviding a structured and accessible dataset that contributes to the \noptimization of",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 52,
    "chunk_index": 2,
    "text": "ng a structured and accessible dataset that contributes to the \noptimization of the training process.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 53,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n53 \n4.2.1.2.3 Benefits Of Image-Tag App in Our Project: \n \n \nImproved Data Quality: Manual labeling ensures accurate and \nreliable data, leading to enhanced model precision and effectiveness. \n \nStreamlined Training Process: The program creates a ready-to-use \nlabeled dataset, reducing time and effort in data preparation and \noptimizing model training. \n \nOrganized Data Management: Storage of labels in a database and \nexport functionality to CSV maintains an organ",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 53,
    "chunk_index": 1,
    "text": "rage of labels in a database and \nexport functionality to CSV maintains an organized system for managing \nlabeled data, enhancing data management efficiency. \nIn essence, this program is a pivotal component in developing an \neffective and robust model. The meticulous labeling process and \nstructured data management it offers are foundational elements in \nachieving success in machine learning endeavors. \n \n4.2.1.3 Tagger امّوس  \n \n \nTo improve our web application, we later rewrote it from scratch",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 53,
    "chunk_index": 2,
    "text": "ger امّوس  \n \n \nTo improve our web application, we later rewrote it from scratch \nand added user login functionality to enable secure access to our app. \nThis login functionality includes creating user accounts using \nauthentication rules such as passwords. \n \nApplication concurrent work support was also a crucial thing, allowing \nmultiple users to collaborate on the same project simultaneously with \nfeatures such as real-time updates and version control. We also \nimproved the user interface for",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 53,
    "chunk_index": 3,
    "text": "real-time updates and version control. We also \nimproved the user interface for the aim of simplifying the user interface \nand improving ease of use and adoption.  We also used the dark blue \ntheme for the text readability. \n \nThese improvements make the application more powerful, secure, and \neasy to use, meeting the needs of individual and group workflows. \nThis is what the “Tagger  ﺎمّوﺳ” application aims to achieve, and through \nit we were able to create a dataset that helped in developing",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 53,
    "chunk_index": 4,
    "text": "eve, and through \nit we were able to create a dataset that helped in developing our model.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 54,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n54 \n \n \nFigure 4.7: Login page of Tagger \n \n \nFigure 4.8: Main page of Tagger",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 55,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n55 \n \nFigure 4.9: Dashboard of Tagger \n \n4.2.2 Artificial Neural Networks (ANNs) \n \n \nArtificial Neural Networks (ANNs) are a fascinating and powerful \ntype of machine-learning model inspired by the structure and function \nof the human brain [60]. ANNs mainly consist of 3 main layers which are \nthe Input Layer, Hidden Layers, and Output Layer as shown in Figure \n4.10. Let’s delve deeper into the intricacies of these networks \n \nFigure 4.10: The General Architectur",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 55,
    "chunk_index": 1,
    "text": "r into the intricacies of these networks \n \nFigure 4.10: The General Architecture of ANNs",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 56,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n56 \n4.2.2.1 Convolutional Neural Network (CNN) \n \n \nA convolutional neural network (CNN) shown in Figure 4.11 [61] is a \ncategory of machine learning model [62], namely a type of deep learning \nalgorithm well suited to analyzing visual data [63][30]. They are \ndistinguished from other neural networks by their superior performance \nwith image, speech, or audio signal inputs classifying and dealing with \nthem. The general layers of a CNN are: \n \n Convolutional laye",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 56,
    "chunk_index": 1,
    "text": "and dealing with \nthem. The general layers of a CNN are: \n \n Convolutional layers & Pooling layers which are used for feature \nextraction of the images \n Fully connected (FC) layer which provides the output or the \nclassification \n \n \nFigure 4.11: Demonstration of CNN Layers and Architecture \n \n4.2.2.2 Recurrent Neural Network (RNN) \n \n \nA recurrent neural network (RNN)[31] is a type of artificial neural \nnetwork which uses sequential data or time series data [64][65][66]. \nThese deep learning",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 56,
    "chunk_index": 2,
    "text": "hich uses sequential data or time series data [64][65][66]. \nThese deep learning algorithms are commonly used for ordinal or \ntemporal problems, such as language translation, natural language \nprocessing (NLP), and speech recognition [67][68]. They are \ndistinguished by their “memory” as they take information from prior \ninputs to influence the current input and output.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 57,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n57 \n \n4.2.2.3 Usage of CNN and RNN in OCR \n \n \nConvolutional Neural Networks (CNNs) and Recurrent Neural \nNetworks (RNNs) are two fundamental architectures in deep learning, \ndesigned to tackle different types of data and tasks [69][70]. \n \n4.2.2.3.1  CNNs for OCR \n \n \nDue to their powerful feature extraction capabilities, CNNs are \nhighly effective at recognizing individual characters within handwritten \ntext. They can discern the varied shapes and strokes that c",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 57,
    "chunk_index": 1,
    "text": "within handwritten \ntext. They can discern the varied shapes and strokes that constitute \nhandwritten characters, making them suitable for segmenting and \nrecognizing characters in isolation. \n \n4.2.2.3.2 RNNs for OCR \n \n \nRNNs, with their ability to process sequences, excel in \nunderstanding the flow and context of handwriting. This is particularly \nbeneficial for cursive writing or scripts where characters are connected, \nand the context is necessary for accurate recognition. RNNs, especially",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 57,
    "chunk_index": 2,
    "text": "nected, \nand the context is necessary for accurate recognition. RNNs, especially \nvariants like LSTMs or GRUs, can better handle the temporal \ndependencies inherent in handwritten text as shown in Figure 4.12 [71]. \n \nFigure 4.12: An Example of how RNNs Handle Text Sequences",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 58,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n58 \n4.2.2.4 CTC Loss Layer \n \n \nCTC is an algorithm employed for training deep neural networks in \ntasks like speech recognition and handwriting recognition [72]. CTC \nfacilitates end-to-end training of neural networks for sequence-to-\nsequence tasks without the need for explicit alignment annotations. It \ndemonstrates resilience to labeling errors or inconsistencies within the \ntraining data by implicitly learning sequence alignments [73]. The \nalgorithm is appli",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 58,
    "chunk_index": 1,
    "text": "ng data by implicitly learning sequence alignments [73]. The \nalgorithm is applicable across a diverse array of use cases. \n \nIt Calculates loss between a continuous (unsegmented) time series and \na target sequence. It does this by summing over the probability of \npossible alignments of input to target, producing a loss value which is \ndifferentiable with respect to each input node. The alignment of input to \ntarget is assumed to be “many-to-one”, which limits the length of the \ntarget sequence",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 58,
    "chunk_index": 2,
    "text": "is assumed to be “many-to-one”, which limits the length of the \ntarget sequence such that it must be the input length [74]. \n \n4.2.2.5 Requirements For Implementing the System \n \n \nImplementing the recognition system required meticulous \nplanning and execution, focusing on the hardware and software \ncomponents necessary for efficient operation. Our choice of hardware \nwas dictated by the need for substantial computational power, primarily \ndue to the large datasets and complex neural network mod",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 58,
    "chunk_index": 3,
    "text": "ional power, primarily \ndue to the large datasets and complex neural network models involved \nin Arabic handwriting recognition. \n \nInitially, we considered using Google Colab for training our models. \nHowever, we encountered several limitations that made this option \nimpractical: \n \n1. Internet Dependence: Google Colab requires a constant internet \nconnection, which isn't always available. This limitation posed \nsignificant challenges for our development process, especially in \nenvironments wit",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 58,
    "chunk_index": 4,
    "text": "nificant challenges for our development process, especially in \nenvironments with unstable connectivity.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 59,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n59 \n2. Data Upload Constraints: The datasets we used were substantial in \nsize, making it time-consuming and inefficient to upload them to \nGoogle Colab. Handling such large volumes of data locally was \nmore feasible. \n3. Free Tier Limitations: The free tier of Google Colab offers limited \nusage time, which often interrupts our training sessions. This \nrestriction necessitated a more reliable solution to ensure \ncontinuous and uninterrupted model training. \n \nGive",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 59,
    "chunk_index": 1,
    "text": "eliable solution to ensure \ncontinuous and uninterrupted model training. \n \nGiven these challenges, we opted to work locally, which required a \npowerful GPU. Our initial setup included a GPU with 6GB of VRAM, but \nthis proved insufficient for our needs. We needed a GPU with at least the \ncapabilities of the Nvidia Tesla T4, which is provided in the free tier of \nGoogle Colab. After evaluating various options, we decided on the \nNvidia RTX 3060, which provided the necessary computational power \na",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 59,
    "chunk_index": 2,
    "text": "ded on the \nNvidia RTX 3060, which provided the necessary computational power \nand memory capacity to handle our training requirements efficiently. \n \nTo implement the recognition system, we used Python and TensorFlow \n[75]. These tools were selected because of their efficiency and the \nrobust libraries available for deep learning. TensorFlow offered \nextensive support for neural network implementation and training, \nmaking it an ideal choice for our project. \n \n4.2.2.6 Nvidia’s RTX 3060 \n \n \nTh",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 59,
    "chunk_index": 3,
    "text": "making it an ideal choice for our project. \n \n4.2.2.6 Nvidia’s RTX 3060 \n \n \nThe Nvidia RTX 3060 played a crucial role in the development and \ntraining of our Arabic handwriting recognition system. With 12GB of \nVRAM [76], it offered the necessary memory capacity to handle large \ndatasets and complex neural network models without running into \nmemory limitations. The GPU's architecture allowed for efficient parallel \nprocessing, significantly reducing the training time compared to our \nprevious",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 59,
    "chunk_index": 4,
    "text": "processing, significantly reducing the training time compared to our \nprevious hardware setup. \n \nThis upgrade was essential for achieving the performance and accuracy \nrequired for our system. The additional VRAM enabled us to experiment",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 60,
    "chunk_index": 0,
    "text": "CHAPTER 4 \nSYSTEM DEVELOPMENT\n \n60 \nwith larger batch sizes and more complex models, which improved the \nrecognition accuracy of our system. The RTX 3060's compatibility with \nTensorFlow and other machine learning frameworks ensured a smooth \nintegration into our development pipeline, allowing us to leverage its full \npotential for our project. \n \nIn summary, the integration of the CRAFT model significantly enhances \nthe preprocessing for Arabic handwritten OCR. By accurately detecting \ntext reg",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 60,
    "chunk_index": 1,
    "text": "the preprocessing for Arabic handwritten OCR. By accurately detecting \ntext regions and handling image angles, CRAFT ensures proper text \nalignment. The histogram segmentation technique further refines the \ninput by precisely cutting the smallest unit of text possible. This \npreprocessing pipeline produces clean, well-segmented text images, \nperfectly primed for the recognition system. Overall, these steps ensure \nhigher accuracy and reliability in the OCR output",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 61,
    "chunk_index": 0,
    "text": "61 \n \n \n \nAI SYSTEM\nCHAPTER 5",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 62,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n62 \n \nCHAPTER 5 AI SYSTEM \n \n \nDeveloping machine learning models for handwriting recognition \ninvolves several key stages. Initially, a suitable dataset is collected and \nprepared, including data cleaning, labeling, and augmentation to \nenhance generalization. Next, various model architectures are tested to \nidentify the best fit for the task, optimizing configurations and \nhyperparameters. Training the model is an iterative process requiring \ncareful tuning and validatio",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 62,
    "chunk_index": 1,
    "text": "aining the model is an iterative process requiring \ncareful tuning and validation. Techniques like transfer learning can \nfurther improve performance by leveraging pre-trained models. \nThroughout development, performance metrics are continually \nevaluated to ensure the model meets accuracy and reliability standards \nand Figure 5.1 [77] shows the relation between the training data and the \nmodel. \n \n \nFigure 5.1: Model Development Process \n \n5.1 Data Collection & Preparation \n \n \nDeveloping a rob",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 62,
    "chunk_index": 2,
    "text": "l Development Process \n \n5.1 Data Collection & Preparation \n \n \nDeveloping a robust handwriting recognition model begins with \nthe meticulous collection and preparation of a comprehensive dataset. \nThis process involves curating a diverse array of samples that",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 63,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n63 \nencompass various handwriting styles and nuances essential for training \nour models effectively. For our endeavor, we focused on integrating \ndatasets such as KHATT for words and sub-words, and AHAWP \nalongside Arabic Handwritten Characters as Alphabetical Arabic \nCharacters datasets. Each dataset was meticulously cleaned, accurately \nlabeled, and split into training and validation sets to optimize training and \nevaluate model performance. To enhance the dataset's vari",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 63,
    "chunk_index": 1,
    "text": "optimize training and \nevaluate model performance. To enhance the dataset's variability and \nprepare our models for real-world scenarios, we employed advanced \ntechniques like data augmentation. These efforts were instrumental in \nensuring our models could generalize well and achieve high accuracy in \nrecognizing Arabic handwriting, as evidenced by the results depicted in \nFigure 5.2. \n \n5.1.1 Dataset Collection and Preparation \n \n \nWe gathered datasets including KHATT [57], AHAWP [78], and \nAra",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 63,
    "chunk_index": 2,
    "text": "Preparation \n \n \nWe gathered datasets including KHATT [57], AHAWP [78], and \nArabic Handwritten Characters to cover diverse handwriting styles. \nData was split into training (80%) and validation (20%) sets to optimize \nmodel training and evaluate performance [79]. \n \nEach dataset underwent rigorous cleaning and accurate labeling to \nensure high-quality input for training. \n \n5.1.2 Data Augmentation Techniques \n \n \nWe Implemented image rotation by +10 and -10 degrees to \nenhance dataset diversity",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 63,
    "chunk_index": 3,
    "text": "Implemented image rotation by +10 and -10 degrees to \nenhance dataset diversity. This step was crucial in preventing overfitting \nby exposing the model to Increased variability of training samples [80].",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 64,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n64 \n \nFigure 5.2: Dataset Samples \n \n5.2 Defining The Training Process \n \n \nThe training process utilizes the prepared data to teach the model \nto make accurate predictions by iteratively adjusting its parameters to \nminimize error [81]. During this phase, the model undergoes multiple \ncycles of learning, where it continuously refines its internal parameters \nto better capture the patterns and relationships within the data. This \nprocess involves feeding the data into the",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 64,
    "chunk_index": 1,
    "text": "relationships within the data. This \nprocess involves feeding the data into the model, calculating the loss or \nerror between the predicted and actual values, and then updating the \nmodel’s parameters to reduce this loss. Through repeated iterations, the \nmodel gradually improves its performance, learning to generalize from \nthe training data to make accurate predictions on unseen data. Several \ncrucial key hyperparameters are adjusted during this phase mentioned \nbelow. \n \n5.2.1 Batch Size",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 64,
    "chunk_index": 2,
    "text": "meters are adjusted during this phase mentioned \nbelow. \n \n5.2.1 Batch Size \n \n \nThe batch size in machine learning determines the number of \ntraining samples used in one iteration. Smaller batch sizes provide more \nfrequent updates, which can introduce noise into the learning process \nbut can help the model to learn more detailed patterns [82]. On the other",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 65,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n65 \nhand, larger batch sizes offer more stable updates and require more \nmemory, often leading to more efficient training. \n \n5.2.2 Epochs \n \n \nEpochs indicate how many times the entire dataset is passed \nthrough the model during training. Increasing the number of epochs \nallows the model to learn more thoroughly, but too many epochs can \nlead to overfitting, where the model performs well on the training data \nbut poorly on new, unseen data [83]. \n \n5.2.3 Optimizer \n \n \nTh",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 65,
    "chunk_index": 1,
    "text": "e training data \nbut poorly on new, unseen data [83]. \n \n5.2.3 Optimizer \n \n \nThe optimizer is the algorithm that governs how the model's \nparameters are updated during training. Common optimizers include \nAdam and Stochastic Gradient Descent (SGD). The choice of optimizer \nsignificantly affects the speed and quality of convergence to the \nminimum of the loss function, impacting overall model performance \n[84]. \n \n5.2.4 Learning Rate \n \n \nThe learning rate influences the step size at each iterat",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 65,
    "chunk_index": 2,
    "text": "2.4 Learning Rate \n \n \nThe learning rate influences the step size at each iteration while \nmoving toward the minimum of the loss function. A higher learning rate \ncan lead to faster convergence but might overshoot the minimum, \ncausing instability. Conversely, a lower learning rate ensures more \nprecise convergence but may take longer and could get stuck in local \nminima [83]. Proper tuning of these hyperparameters is essential for \nachieving a well-performing and generalized model. \n \n5.3 Defin",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 65,
    "chunk_index": 3,
    "text": "s essential for \nachieving a well-performing and generalized model. \n \n5.3 Defining Model Evaluation \n \n \nModel evaluation metrics are essential for assessing the \nperformance and effectiveness of machine learning models, particularly",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 66,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n66 \nin tasks like character recognition. Key metrics used include accuracy, \nprecision, recall, and the character error rate (CER) [85]. \n \nThe CER is calculated using equation 1 \n \n𝐶𝐸𝑅= (𝑖+ 𝑠+ 𝑑) / 𝑛\n(1) \n \n \nHere, i denotes the number of insertion errors where extra \ncharacters are predicted, s represents substitution errors where \nincorrect characters are predicted, and d indicates deletion errors \nwhere characters in the ground truth are missing from the prediction. \nT",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 66,
    "chunk_index": 1,
    "text": "errors \nwhere characters in the ground truth are missing from the prediction. \nThe total number of characters evaluated is denoted by n. Additionally, \nCER is closely related to the Levenshtein distance, which measures the \nminimum number of single-character edits (insertions, deletions, or \nsubstitutions) required to transform one string into another as shown in \nFigure 5.3. By considering these factors, the CER provides a \ncomprehensive measure of how well a model performs in generating \ncorre",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 66,
    "chunk_index": 2,
    "text": "vides a \ncomprehensive measure of how well a model performs in generating \ncorrect character sequences. A lower CER indicates higher accuracy, as \nit signifies fewer errors in character prediction relative to the ground \ntruth, crucial for evaluating models in handwriting recognition, OCR, and \nother applications requiring precise character transcription. \n \n \nFigure 5.3: Demonstration of CER Metric \n \n5.4 Dealing with Characters’ Sequence: Bi-LSTM \n \n \nLong Short-Term Memory networks (LSTMs) Sh",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 66,
    "chunk_index": 3,
    "text": "th Characters’ Sequence: Bi-LSTM \n \n \nLong Short-Term Memory networks (LSTMs) Shown in Figure 5.4 \n[86] are a specialized type of recurrent neural network (RNN) designed",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 67,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n67 \nto address the vanishing gradient problem and effectively capture long-\nterm dependencies in sequential data. \n \nFigure 5.4: Diagram of A Single LSTM Unit \n \n \nLSTMs utilize a memory cell and gate mechanisms to regulate the \nflow of information through the network over time. LSTMs include \nInput (Xt): The input at time step t, Memory Cell (Ct): Maintains \ninformation over long sequences, preventing the vanishing gradient \nproblem in traditional RNNs, Hidden State (ht):",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 67,
    "chunk_index": 1,
    "text": "eventing the vanishing gradient \nproblem in traditional RNNs, Hidden State (ht): Carries information from \nthe current time step to the next. It is used along with the next input to \ncompute the gates and cell state in the following time step, Forget \nGate (ƒt): Controls what information should be discarded from the cell \nstate, Input Gate (it): Modulates the input information to update the cell \nstate, and Output Gate(ot): Produces the output based on the current \ncell state, ensuring relevant",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 67,
    "chunk_index": 2,
    "text": "te(ot): Produces the output based on the current \ncell state, ensuring relevant information is passed to the next layer or \noutput. \nBiLSTMs are a type of recurrent neural network (RNN) designed to \ncapture dependencies in sequences by processing information in both \nforward and backward directions simultaneously as shown in Figure 5.5 \n[87]. Unlike traditional LSTMs that only consider past context, BiLSTMs \nincorporate future context as well, enhancing their understanding of \nsequence data.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 68,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n68 \n \nFigure 5.5: Bi-LSTM Architecture \n \nThe BiLSTM (Bidirectional Long Short-Term Memory) architecture \nconsists of two LSTM layers: one processes sequences in a forward \ndirection, and the other processes them in a backward direction. Each \nLSTM unit computes its hidden state based on the input, previous hidden \nstate, and previous cell state. This dual-layer approach allows the \nnetwork to capture dependencies from both past and future contexts \nwithin the sequence, en",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 68,
    "chunk_index": 1,
    "text": "capture dependencies from both past and future contexts \nwithin the sequence, enhancing its understanding of the data. \nThe mathematical formulation of BiLSTMs utilizes sigmoid and tanh \nactivation functions to control the flow of information through the \nnetwork. These activation functions facilitate selective forgetting and \nupdating of information over time, enabling the network to maintain \nlong-term dependencies within sequences. This capability is crucial for \ntasks that require a nuanced",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 68,
    "chunk_index": 2,
    "text": "within sequences. This capability is crucial for \ntasks that require a nuanced understanding of sequential data, as it \nallows the network to effectively handle and learn from complex \npatterns. \n \nIn handwriting recognition, the BiLSTM architecture is ideal for modeling \ncharacter sequences in words, where the context from both preceding \nand succeeding characters influences interpretation. This makes it \nparticularly effective for recognizing Arabic handwriting, as it can \ncapture the intrica",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 68,
    "chunk_index": 3,
    "text": "rly effective for recognizing Arabic handwriting, as it can \ncapture the intricate relationships between characters within words. By \nleveraging the bidirectional processing capability, BiLSTMs enhance the \naccuracy and robustness of handwriting recognition systems.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 69,
    "chunk_index": 0,
    "text": "CHAPTER 5 \nAI SYSTEM\n \n69 \nFigure 5.6 shows the initial design of the recognition system \n \n \nFigure 5.6: Diagram of the recognition system \n \nWith this, the final system becomes as Figure 5.7. \n \n \nFigure 5.7: Diagram of entire system",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 70,
    "chunk_index": 0,
    "text": "70 \n \n \n \nCHAPTER 6\nAPPLICATION",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 71,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n71 \n \nCHAPTER 6 APPLICATION \n6.1 Overview \n \nIn the development of our application for Arabic Handwritten OCR, we \nadopted a robust architecture to ensure efficiency and scalability. The \ncore of our system is built on a front-end and back-end model, which \nallows us to separate concerns between the user interface and the \ncomputational heavy lifting. This separation is crucial for handling the \nintensive tasks involved in OCR, such as image processing and text \nrecognit",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 71,
    "chunk_index": 1,
    "text": "he \nintensive tasks involved in OCR, such as image processing and text \nrecognition, which require significant computational resources. By \noffloading these tasks to a powerful server, we ensure that the \napplication remains responsive and performs optimally even on devices \nwith limited processing power. This chapter delves into the specifics of \nour application’s architecture, the technologies used, and the rationale \nbehind our design choices. \n \n6.2 Frontend and Backend Model \n \nIn software",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 71,
    "chunk_index": 2,
    "text": "le \nbehind our design choices. \n \n6.2 Frontend and Backend Model \n \nIn software engineering, the terms frontend and backend \n(sometimes written as back end or back-end) refer to the separation of \nconcerns between the presentation layer (frontend), and the data \naccess layer (backend) of a piece of software, or the physical \ninfrastructure or hardware. In the client–server model, the client is \nusually considered the frontend and the server is usually considered the \nbackend, even when some pres",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 71,
    "chunk_index": 3,
    "text": "frontend and the server is usually considered the \nbackend, even when some presentation work is done on the server \nitself. \n \nWe employed a front-end and back-end model in developing the \napplication to offload the heavy computational tasks from the user's \ndevice to a powerful server. This approach ensures that the application \nremains responsive and efficient, even on devices with limited \nprocessing power. By handling intensive tasks like image processing and \ntext recognition on the server",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 71,
    "chunk_index": 4,
    "text": "ndling intensive tasks like image processing and \ntext recognition on the server, we can leverage more robust hardware \nand advanced algorithms, resulting in faster and more accurate OCR",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 72,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n72 \nperformance. This division of labor allows the front-end to focus on \nproviding a smooth user experience while the back end manages the \ncomplex computations, ensuring that our application is both user-\nfriendly and highly effective. \n \n6.2.1 Frontend \n \nIn the Frontend, we designed a web application which can be used \nby the user. It works by uploading an image that has the text which will \nbe converted to a digital text by our system.  \nThe website was created usin",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 72,
    "chunk_index": 1,
    "text": "ll \nbe converted to a digital text by our system.  \nThe website was created using HTML5, CSS3, JavaScript, and React JS \nfrontend framework.  \n \n6.2.1.1 Native Application \n \nA native application is a software program designed specifically \nfor use on a particular platform or device. Unlike web applications that \nrun in a browser, native applications are developed using platform-\nspecific programming languages and tools. This allows for better \nperformance, integration with device hardware, and",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 72,
    "chunk_index": 2,
    "text": "ols. This allows for better \nperformance, integration with device hardware, and a more seamless \nuser experience.  \n \nWe decided against using native applications because they are typically \nlimited to a single platform, requiring separate versions for iOS, Android, \nand other operating systems, which significantly increases development \ntime and costs. Moreover, native applications often demand powerful \nresources and frequent updates to maintain compatibility with new \noperating system version",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 72,
    "chunk_index": 3,
    "text": "nd frequent updates to maintain compatibility with new \noperating system versions and devices. This results in higher \nmaintenance efforts and resource allocation, making native applications \nless efficient for our project needs. \n \n6.2.1.2 Web Technology \n \n \nWeb technology refers to the tools, software, and programming \nlanguages used to create and maintain websites and web applications.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 73,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n73 \nIt encompasses a wide range of technologies and frameworks that work \ntogether to deliver content and functionality on the internet. Web \ntechnologies which we used are HTML, CSS, JAVASCRIPT, web browsers \nand frameworks like REACT [88].  \n \n6.2.1.2.1 HTML / CSS / JS \n \n \nHTML, CSS, and JavaScript are the three foundational \ntechnologies used to create and design websites.   \nHTML (Hyper Text Markup Language): The standard language for \ncreating web pages and structu",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 73,
    "chunk_index": 1,
    "text": "Text Markup Language): The standard language for \ncreating web pages and structuring content on the internet. HTML \ndefines the structure and layout of a webpage using tags. \n \nCSS (Cascading Style Sheets) is used to enhance the presentation and \nlayout of HTML elements. It allows developers to control the design \naspects of a webpage, such as colors, fonts, spacing, and positioning. \nCSS is important for creating visually appealing and responsive \nwebsites.  \n \nJavaScript is a programming langu",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 73,
    "chunk_index": 2,
    "text": "sually appealing and responsive \nwebsites.  \n \nJavaScript is a programming language that enables interactive and \ndynamic functionality on a webpage. It can be used to create \nanimations, form validation, interactive maps, and other user \ninteractions. JavaScript is essential for making websites more engaging \nand user-friendly.  \n \n6.2.1.2.2 React \n \nReact (also known as React.js or ReactJS) is a popular JavaScript \nlibrary for building user interfaces. Developed by Meta (formerly \nFacebook) [8",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 73,
    "chunk_index": 3,
    "text": "library for building user interfaces. Developed by Meta (formerly \nFacebook) [89], React allows developers to create dynamic, interactive, \nand fast web applications [90].  \n \nOne of the key features of React is its component-based architecture. \nComponents are reusable building blocks that represent a part of the \nuser interface. Each component encapsulates its own logic and state,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 74,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n74 \nmaking it easier to manage and update different parts of the UI [91]. \nComponents can be nested within each other to create complex \ninterfaces.  \n \nReact uses a virtual DOM (Document Object Model) to efficiently update \nand render components. When the state of a component changes, \nReact only updates the necessary parts of the DOM, resulting in faster \nperformance and improved user experience. This approach helps to \nminimize unnecessary re-renders and optimize the",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 74,
    "chunk_index": 1,
    "text": "ience. This approach helps to \nminimize unnecessary re-renders and optimize the rendering process.  \n \nReact also allows developers to create declarative and composable UI \ncomponents. With React, developers can easily compose complex UI \nelements by combining smaller components, which helps in writing \nclean, maintainable, and modular code [92]. \n \nWe decided to name our app “ضاد الرقمية” as it encapsulates how our app \nwork with the Arabic language in a digital world, Figure 6.1 shows the \nhom",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 74,
    "chunk_index": 2,
    "text": "app \nwork with the Arabic language in a digital world, Figure 6.1 shows the \nhome page design of our app. \n \n \nFigure 6.1: Screenshot of Main Page",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 75,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n75 \n \nFigure 6.2: Screenshot of “our model” page \n \n \nFigure 6.3: Screenshot of how results are displayed",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 76,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n76 \n6.2.2 Backend \n \n \nThe backend is the part of the system responsible for processing \ninput images using the segmentation system and then passing these \nimages to the model for recognition, ultimately returning the labels. It \nserves the crucial purpose of offloading the computationally intensive \ntasks from the frontend, ensuring that the user interface remains \nresponsive and efficient. \n \n6.2.2.1 Developing an API \n \n \nIn a frontend-backend model application, an AP",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 76,
    "chunk_index": 1,
    "text": "6.2.2.1 Developing an API \n \n \nIn a frontend-backend model application, an API (Application \nProgramming Interface) is essential for communication between the \ntwo parts. The API allows the frontend to send image data to the \nbackend, where it can be processed, and then receive the recognized \nlabels. This separation of concerns ensures that each part of the \napplication can be developed and maintained independently, improving \nmodularity and scalability. \n \n6.2.2.2 Fake API \n \n \nInitially, w",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 76,
    "chunk_index": 2,
    "text": "ly, improving \nmodularity and scalability. \n \n6.2.2.2 Fake API \n \n \nInitially, we developed a simple fake API that processed the image \nin an arbitrary way and returned predefined labels. This fake API was \ncrucial for testing the API's functionality and determining which package \nto use for developing the final API. By simulating the backend's behavior, \nwe were able to make informed decisions about the tools and \nframeworks we would employ. \n \n6.2.2.3 FastAPI \n \n \nFastAPI is a modern, fast (hi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 76,
    "chunk_index": 3,
    "text": "rameworks we would employ. \n \n6.2.2.3 FastAPI \n \n \nFastAPI is a modern, fast (high-performance) web framework for \nbuilding APIs with Python 3.6+ based on standard Python-type hints. It is \ndesigned to be easy to use and deploy, offering automatic interactive \nAPI documentation.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 77,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n77 \nDespite its advanced features, we found that FastAPI was more complex \nthan necessary for our application, which primarily involved sending \nimages and receiving labels. The additional features and complexity of \nFastAPI did not provide a significant advantage for our relatively \nstraightforward requirements [93]. \n \n6.2.2.4 Flask \n \n \nFlask is a lightweight WSGI web application framework in Python. \nIt is designed to make getting started quick and easy, with the abi",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 77,
    "chunk_index": 1,
    "text": "in Python. \nIt is designed to make getting started quick and easy, with the ability to \nscale up to complex applications. Flask is simple and flexible, providing \nthe essentials without requiring extensive boilerplate code. \n \nWe found Flask to be easy to work with, allowing us to define routes and \nhandle requests with minimal effort. Its simplicity made it ideal for our \nneeds, as we only needed to send images to the backend and receive \nrecognized labels in return. \n \nAfter experimenting wit",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 77,
    "chunk_index": 2,
    "text": "the backend and receive \nrecognized labels in return. \n \nAfter experimenting with different frameworks, we decided to use Flask \nand implemented the entire backend system with it. Flask provided the \nright balance of simplicity and functionality, making it the optimal choice \nfor our application. \n \n6.2.2.5 Hardware Configuration \n \n \nThe backend was implemented in a local environment (self-\nhosted) rather than on the cloud. This decision was based on our need \nto have full control over the hard",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 77,
    "chunk_index": 3,
    "text": "e cloud. This decision was based on our need \nto have full control over the hardware and software environment, \nensuring optimal performance and ease of maintenance. \nThe hardware configuration used in the backend includes: \n \n● Intel Core i3-12100F Processor: A budget-friendly central \nprocessing unit (CPU) capable of handling the necessary \ncomputations for our OCR system.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 78,
    "chunk_index": 0,
    "text": "CHAPTER 6 \nAPPLICATION\n \n78 \n● 16 Gigabytes of Random Access Memory (RAM): Sufficient \nmemory to manage the image processing and model inference \ntasks. \n● NVIDIA GeForce RTX 3060 Graphics Processing Unit (GPU): A \npowerful GPU that significantly accelerates the processing of \nimages and model computations, making the system efficient and \nresponsive. \n \nBy using this hardware configuration, we ensured that our backend \ncould handle the demands of the OCR processing effectively, providing \nquick",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 78,
    "chunk_index": 1,
    "text": "nd \ncould handle the demands of the OCR processing effectively, providing \nquick and accurate results.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 79,
    "chunk_index": 0,
    "text": "79 \n \n \n \nCHAPTER 7\nAI TRAINING & RESULTS",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 80,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n80 \nCHAPTER 7 AI SYSTEM IMPLEMENTATION \n7.1 Our Model’s Methodology \n \n \nOur model for Arabic handwriting recognition project as shown in \nFigure 7.1 begins with the Image input layer, which takes 64x64 images. \nThese inputs are then fed into a CNN that extracts core features from the \nimages. The model further processes these features through a Dense \nLayer, providing a meaningful representation of the input. Following \nthis, a Batch Normalization layer sta",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 80,
    "chunk_index": 1,
    "text": "ul representation of the input. Following \nthis, a Batch Normalization layer stabilizes the network by \nnormalizing the output of the dense layer. A Dropout Layer is used to \nreduce overfilling by ignoring a fraction of the input. The Bi-LSTM \n(RNN) layer is critical for analyzing sequences of the input image, \nmaking it ideal for recognizing the sequential nature of Arabic \nhandwriting. The output of the Bi-LSTM is stabilized by another Batch \nNormalization layer, followed by another Dropout La",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 80,
    "chunk_index": 2,
    "text": "stabilized by another Batch \nNormalization layer, followed by another Dropout Layer. The \nmodel then uses a Softmax Layer to generate the predictive output, \nensuring accurate transcription of the handwritten input. The CTC Loss \nLayer forms the final part of the network, handling the sequence and \nreducing the error percentage by accurately calculating the CTC loss \nand providing feedback for improved performance. The model’s output \nis generated after the CTC Loss Layer, providing an accurate",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 80,
    "chunk_index": 3,
    "text": "he model’s output \nis generated after the CTC Loss Layer, providing an accurate \nsequence of characters that can be used as the final output for the \nArabic handwriting recognition.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 81,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n81 \n \nFigure 7.1: Our Model’s Approach \n \n7.2 Initial Experiments: Prototype Model \n \n \nIn the initial phase of our project, we focused on developing a \nprototype model using a smaller dataset to validate our approach and \nrefine our techniques. This allowed us to quickly iterate and experiment \nwith different Convolutional Neural Network (CNN) architectures. By \ntesting various architectures, we aimed to identify the model that \noffered the best balance bet",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 81,
    "chunk_index": 1,
    "text": "architectures, we aimed to identify the model that \noffered the best balance between accuracy, computational efficiency, \nand generalization capabilities. Experimenting with these different \narchitectures enabled us to understand their performance in the \ncontext of our specific application, guiding us toward the most suitable \nmodel for further development and deployment. \n \n7.2.1 Initial Dataset \n \n \nFor the initial model prototype, we utilized a subset of the KHATT \ndataset containing 5,159 s",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 81,
    "chunk_index": 2,
    "text": "l model prototype, we utilized a subset of the KHATT \ndataset containing 5,159 samples, which we augmented to a total of \n15,477 samples shown in Figure 7.2. This dataset covered 36 Arabic \nletters and included a variety of segmented words and sub-words. As \npart of our preprocessing steps, all images were resized to a uniform \n32x64 resolution. This resizing process ensured consistency in input \ndimensions across the dataset, facilitating efficient training and \nevaluation of our models. By sta",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 81,
    "chunk_index": 3,
    "text": "e dataset, facilitating efficient training and \nevaluation of our models. By standardizing the image size, we aimed to",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 82,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n82 \noptimize computational resources while maintaining the integrity and \nquality of the dataset for accurate character recognition and model \nperformance. \n \n \nFigure 7.2: Samples of Initial Dataset Showing Arabic Words \n \n7.2.2 Experiments And Results \n \n \nIn the pursuit of refining model performance, experimentation \nwith various architectures plays a crucial role. This process involves \nexploring different configurations and designs of neural networks,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 82,
    "chunk_index": 1,
    "text": "s involves \nexploring different configurations and designs of neural networks, \nparticularly focusing on convolutional neural networks (CNNs) for tasks \nsuch as image recognition and classification. Each architecture variant \nmay include adjustments in the number of layers, types of layers (e.g., \nconvolutional, pooling, fully connected), activation functions, and \nregularization techniques. By testing multiple architectures, researchers \naim to identify the configuration that optimizes model ac",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 82,
    "chunk_index": 2,
    "text": "tectures, researchers \naim to identify the configuration that optimizes model accuracy, \nefficiency, and generalization capabilities across diverse datasets and \nreal-world scenarios. This iterative exploration allows for the discovery \nof architectures that strike a balance between complexity and \nperformance metrics, ensuring robust and reliable outcomes in machine \nlearning applications. In the initial prototype model using the initial \ndataset we experimented with different CNNs; EfficientNe",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 82,
    "chunk_index": 3,
    "text": "odel using the initial \ndataset we experimented with different CNNs; EfficientNetB1 [94], \nVGG19 [95], and lastly ResNet152 [96]. \n \n7.2.2.1 EfficientNetB1 \n \n \nEfficientNetB1's technical foundation lies in its innovative \napplication of compound scaling, a method that harmonizes the \ndimensions of depth, width (number of channels), and image resolution \nwithin the neural network architecture. By scaling these dimensions",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 83,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n83 \njointly rather than independently, EfficientNetB1 optimizes both \ncomputational efficiency and model performance for tasks such as \nArabic handwriting recognition. Increased depth allows the network to \nlearn intricate hierarchies of features essential for interpreting diverse \nArabic characters. Expanded width enhances its capacity to process \ninformation in parallel, crucial for capturing nuances in handwritten \nscript. Resolution scaling ensures effic",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 83,
    "chunk_index": 1,
    "text": "l for capturing nuances in handwritten \nscript. Resolution scaling ensures efficient utilization of computational \nresources by adapting input image sizes to maximize accuracy without \nunnecessary computational overhead. This integrated approach \nenables EfficientNetB1 to excel in recognizing Arabic handwriting by \neffectively balancing model complexity and efficiency, making it a \nrobust choice for real-world applications requiring precise character \nrecognition in Arabic script and the Figure",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 83,
    "chunk_index": 2,
    "text": "ations requiring precise character \nrecognition in Arabic script and the Figure 7.3 [97] shows the \nEfficientNetB1 architecture. \n \n \nFigure 7.3: EfficientNetB1 Architecture \n \n7.2.2.2 VGG19 \n \n \nVGG19 is a deep convolutional neural network architecture shown \nin Figure 7.4 [98], renowned for its simplicity and effectiveness in image \nrecognition tasks. Developed by the Visual Geometry Group at Oxford, \nVGG19 consists of 19 layers, including 16 convolutional layers and 3 fully \nconnected layers.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 83,
    "chunk_index": 3,
    "text": "s of 19 layers, including 16 convolutional layers and 3 fully \nconnected layers. Its design is characterized by using small 3x3 \nconvolutional filters throughout the network, which allows it to capture \nintricate features in input images. VGG19's architecture emphasizes \ndepth and homogeneous filter sizes, facilitating a straightforward \ninterpretation and implementation. Despite its simplicity compared to",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 84,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n84 \nnewer architectures, VGG19 remains highly effective due to its ability to \nextract detailed features from images, making it a strong contender for \ntasks requiring precise image classification and feature extraction, \nincluding applications in object detection and recognition. \n \n \nFigure 7.4: VGG19 Architecture \n \n7.2.2.3 ResNet152 \n \n \nResNet152, part of the Residual Network (ResNet) family, \nrepresents a significant advancement in deep learning archit",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 84,
    "chunk_index": 1,
    "text": "k (ResNet) family, \nrepresents a significant advancement in deep learning architectures and \nits architecture shown in Figure 7.5 [99], particularly for tasks like image \nrecognition and classification. Developed by Microsoft Research, \nResNet152 is distinguished by its deep structure comprising 152 layers, \nincluding residual blocks that mitigate the vanishing gradient problem in \ndeep networks. This architecture introduces skip connections, or \nshortcuts, that allow gradients to flow more dire",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 84,
    "chunk_index": 2,
    "text": "troduces skip connections, or \nshortcuts, that allow gradients to flow more directly during training, \nenabling efficient learning of complex features. ResNet152's design \nleverages residual learning principles to achieve state-of-the-art \nperformance on various benchmarks, demonstrating robustness and \nscalability in handling large-scale datasets. Its depth and skip \nconnections enhance model accuracy by facilitating the training of \ndeeper networks without sacrificing computational efficiency,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 84,
    "chunk_index": 3,
    "text": "g the training of \ndeeper networks without sacrificing computational efficiency, making \nResNet152 a pivotal architecture in advancing the capabilities of deep \nlearning models for challenging tasks in computer vision and beyond.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 85,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n85 \n \nFigure 7.5: ResNet152 Architecture \n \n7.2.2.4 Training & Results \n \n \nDuring the training phase, we trained our models for 70 epochs, \nexperimenting with different optimizers and hyperparameters to \nidentify the best configuration. We utilized a learning rate scheduler to \ndynamically adjust the learning rate during training. The learning rate \nscheduler monitors the validation loss and, if there is no improvement \nover 10 consecutive epochs, it multip",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 85,
    "chunk_index": 1,
    "text": "tion loss and, if there is no improvement \nover 10 consecutive epochs, it multiplies the learning rate by the factor \nof 0.2. This approach helps in fine-tuning the model's learning process, \nallowing it to converge more effectively and avoid getting stuck in local \nminima. By carefully managing the learning rate, the scheduler ensures \nthat the model maintains a balance between learning efficiently and not \novershooting the optimal solution, ultimately improving the model's \nperformance and sta",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 85,
    "chunk_index": 2,
    "text": "ting the optimal solution, ultimately improving the model's \nperformance and stability. Hyperparameters used for each model are \nshown in Table 1. \n \nParameters \nVGG19 EfficientNetB1 ResNet152\nOptimizer \nADAM \nNADAM \nNADAM \nLearning Rate 0.0001 0.001 \n0.001 \nβ1 \n0.9 \n0.9 \n0.9 \nβ2 \n0.999 \n0.999 \n0.999 \nε \n1×10-8 \n1×10-8 \n1×10-8 \nBatch Size \n128 \n128 \n128 \nTable 1: Hyperparameters Used for each Experimented CNN",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 86,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n86 \n \nAfter evaluating the performance of our models based on \nvalidation loss, character error rate (CER), and accuracy metrics, distinct \noutcomes were observed across different architectures: \n \n VGG19: Validation loss of 0.6, CER of 5.4%, and accuracy of 94.6%. \n EfficientNetB1: Validation loss of 0.78, CER of 7.3%, and accuracy \nof 92.7%. \n ResNet152: Superior results with a validation loss of 0.3, CER of \n2.96%, and accuracy of 97.04%. \n \nThese find",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 86,
    "chunk_index": 1,
    "text": "h a validation loss of 0.3, CER of \n2.96%, and accuracy of 97.04%. \n \nThese findings highlight that ResNet152 outperformed both VGG19 and \nEfficientNetB1 across all evaluated metrics, making it the most effective \nmodel for our specific task.  \n \nThe comparison between the models' performance is summarized in \nTable 2, emphasizing ResNet152's superiority. Additionally, Figure 7.6  \ndepicting the CER and loss of the ResNet152 model throughout the \nepochs shows a noticeable decrease in error, part",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 86,
    "chunk_index": 2,
    "text": "esNet152 model throughout the \nepochs shows a noticeable decrease in error, particularly in the final \nepochs. \n \nModel \nValidation Loss Character Error Rate Accuracy\nVGG19 \n0.6 \n5.4% \n94.6% \nEfficientNetB1 0.78 \n7.3% \n92.7% \nResNet152 \n0.3 \n2.96% \n97.04% \nTable 2: Comparison Between CNN Architectures Performance",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 87,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n87 \n \nFigure 7.6: ResNet152 Performance Throughout the Epochs \n \n7.3 Optimized Final Model \n \n \nMoving from experimental prototype models to developing the \nfinal model involved a systematic approach to refine and optimize our \ninitial findings. Initially, we explored various CNN architectures such as \nVGG19, EfficientNetB1, and ResNet152 using smaller datasets to gauge \ntheir performance and suitability. Upon identifying ResNet152 as the \nmost promising due",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 87,
    "chunk_index": 1,
    "text": "rformance and suitability. Upon identifying ResNet152 as the \nmost promising due to its superior validation loss, character error rate, \nand accuracy metrics, we transitioned to a more rigorous development \nphase.  \n \n7.3.1 Larger Dataset \n \n \nWe curated an extensive dataset incorporating various sources, \nincluding a substantial portion of the KHATT Dataset originally \ncomprising approximately 20,000 samples, which after meticulous \ncleaning and augmentation processes, expanded to 50,145 sample",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 87,
    "chunk_index": 2,
    "text": "after meticulous \ncleaning and augmentation processes, expanded to 50,145 samples. \nAdditionally, we integrated data from the AHAWP and Arabic \nHandwritten Characters datasets, also yielding a combined total of",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 88,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n88 \n58,474 samples post-cleaning and augmentation. This unified dataset \nwas meticulously structured to encompass 36 Arabic letters, with all \nimages standardized to a resolution of 64x64 pixels. This standardization \nensured uniformity in input dimensions across the dataset, essential for \ntraining robust and accurate models capable of recognizing and \ninterpreting Arabic characters across diverse handwritten styles and \nconditions. By incorporating a wide",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 88,
    "chunk_index": 1,
    "text": "ters across diverse handwritten styles and \nconditions. By incorporating a wide range of characters and maintaining \nconsistent image quality, our dataset provided a comprehensive \nfoundation for developing advanced machine learning models tailored \nfor Arabic character recognition tasks. \n \n7.3.2 Chosen Optimal Architecture: ResNet50V2 \n \n \nWe opted for ResNet50V2 as our optimal model over ResNet152, \ndespite the latter's larger size and higher resolution dataset, due to \nseveral considerations",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 88,
    "chunk_index": 2,
    "text": "tter's larger size and higher resolution dataset, due to \nseveral considerations. ResNet50V2 strikes a balance between \nperformance and computational efficiency, making it more feasible for \nscaling up to larger datasets with higher resolutions. To enhance its \ntraining efficacy, we implemented advanced techniques like the “Cosine \nLearning Rate Scheduler”. This scheduler dynamically adjusts the \nlearning rate during training according to a Cosine wave as shown in \nFigure 7.7 [100], potentially",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 88,
    "chunk_index": 3,
    "text": "training according to a Cosine wave as shown in \nFigure 7.7 [100], potentially improving model convergence and \ngeneralization.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 89,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n89 \n \nFigure 7.7: Example of Cosine Learning Rate Scheduler Effect \n \n \nWe used Nadam as the optimizer with an initial learning rate of \n0.001, a batch size of 128, and trained the model for 70 epochs. The \nNadam optimizer parameters included 𝛃1=0.9, 𝛃2=0.999, and ε=1 x e-08. \nInitially tested on a dataset with a resolution of 32x32, that test resulted \nin a model with a Character Error Rate (CER) of 4.62% corresponding to \n95.38% accuracy. Performance of th",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 89,
    "chunk_index": 1,
    "text": "r Error Rate (CER) of 4.62% corresponding to \n95.38% accuracy. Performance of the model is shown in Figure 7.8.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 90,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n90 \n \nFigure 7.8: ResNet50V2 Performance on The Larger Dataset \n \n \nRecognizing the need for further improvement, we employed \ntransfer learning techniques to refine the model's performance, \nleveraging ResNet50V2's capabilities and our refined training approach \nto achieve robust performance and scalability necessary for our specific \napplication. \n \n7.3.3 ResNet50V2 Arabic Alphabet Transfer Learning \n \n \nTransfer learning is a technique in machine learning",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 90,
    "chunk_index": 1,
    "text": "abet Transfer Learning \n \n \nTransfer learning is a technique in machine learning where a model \ntrained on one task is leveraged as a starting point for a new task as \nshown in Figure 7.9 [101]. This approach utilizes knowledge gained from \nsolving one problem and applies it to a different but related problem, \ntypically resulting in improved learning efficiency and performance. In \nour case, after initially training ResNet50V2 on an Arabic Alphabet \nCharacter dataset consisting of 58,474 sample",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 90,
    "chunk_index": 2,
    "text": "ResNet50V2 on an Arabic Alphabet \nCharacter dataset consisting of 58,474 samples, we easily achieved a",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 91,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n91 \nlow CER, high accuracy model that’s well trained to recognize Arabic \nAlphabetical Letters. This success prompted us to further enhance the \nmodel's capabilities by transferring its learned weights to the main \nKHATT Dataset, which comprises 50,145 samples. By fine-tuning the \nmodel on this larger and more diverse dataset, we aim to capitalize on \nthe features and patterns learned from the Arabic Alphabet dataset, \nthereby improving the model's accuracy",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 91,
    "chunk_index": 1,
    "text": "arned from the Arabic Alphabet dataset, \nthereby improving the model's accuracy and robustness for recognizing \nhandwritten Arabic text in the KHATT Dataset. This application of \ntransfer learning allows us to efficiently adapt the model to new data \nwhile leveraging existing knowledge to achieve superior performance in \na related domain. \n \n \nFigure 7.9: Demonstrating Transfer Learning Technique \n \n7.3.4 Final Results \n \n \nIn our final approach, we loaded the ResNet50V2 model pre-\ntrained on th",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 91,
    "chunk_index": 2,
    "text": "ts \n \n \nIn our final approach, we loaded the ResNet50V2 model pre-\ntrained on the Arabic Alphabet Character dataset and continued training \non the main KHATT Dataset using advanced techniques and optimal \nhyperparameters. The model was trained with a cosine learning rate \nscheduler, Adam optimizer with an initial learning rate of 0.001, 𝛃1 of 0.9, \n𝛃2 of 0.999, and ε set to 1 x e-7, across 70 epochs.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 92,
    "chunk_index": 0,
    "text": "CHAPTER 7 \nAI SYSTEM IMPLEMENTATION\n \n92 \n \nCombining the 58,474 samples from the alphabet dataset with the \n50,145 samples from the KHATT Dataset, the model was trained on a \ntotal of 108,619 samples. Through this comprehensive training regimen, \nour model achieved outstanding results showing performance as in \nFigure 7.10, boasting a CER of only 3% on the test set and an Accuracy \nof 97%. These metrics underscore the model's robustness and accuracy \nin recognizing handwritten Arabic text, demo",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 92,
    "chunk_index": 1,
    "text": "he model's robustness and accuracy \nin recognizing handwritten Arabic text, demonstrating its effectiveness \nin real-world applications.  \n \nThis final model represents a culmination of rigorous experimentation, \nmeticulous parameter tuning, and leveraging the power of transfer \nlearning. Its high accuracy and low error rate position it as a reliable tool \nfor applications requiring precise recognition of Arabic characters and \ntext, validating its capability as our definitive and highly effecti",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 92,
    "chunk_index": 2,
    "text": "acters and \ntext, validating its capability as our definitive and highly effective model. \n \n \nFigure 7.10: ResNet50V2 Trained on Alphabet from KHATT Dataset",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 93,
    "chunk_index": 0,
    "text": "93 \n \n \n \n \nCHAPTER 8\nCONCLUSION & \nFUTURE WORK",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 94,
    "chunk_index": 0,
    "text": "CHAPTER 8 \nCONCLUSION AND FUTURE WORK\n \n94 \nCHAPTER 8 CONCLUSION AND FUTURE WORK \n8.1 Conclusion \n \n \nDriven by our passion for developing advanced text-processing \ntechniques in Arabic, we recognized their potential to bolster various \nareas we mentioned earlier (education, government, data analysis, and \ninformation retrieval). This dedication led us to closely monitor \nadvancements in Arabic handwriting recognition. We were particularly \nimpressed by the recent research paper \"Handwritten Ara",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 94,
    "chunk_index": 1,
    "text": "n. We were particularly \nimpressed by the recent research paper \"Handwritten Arabic Bills \nReader and Recognizer.\" It motivated us to push our own research, \nculminating in noteworthy results. Our final model, the ResNet50V2, \npre-trained on the Arabic Alphabet Character dataset and further \ntrained on the main KHATT Dataset using advanced techniques and \noptimal hyperparameters, yielded significantly better performance \ncompared to the referenced research. This achievement, using a cosine \nlear",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 94,
    "chunk_index": 2,
    "text": "nce \ncompared to the referenced research. This achievement, using a cosine \nlearning rate scheduler, Adam optimizer with an initial learning rate of \n0.001, β1 of 0.9, β2 of 0.999, and ε set to 1 x e-7, across 70 epochs, \nrepresents a substantial leap in accuracy (97%) and a low Character \nError Rate (CER) of only 3% on the test set. This accomplishment fuels \nour commitment to further research in this domain, with the ultimate \ngoal of achieving even more advanced levels of Arabic handwriting",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 94,
    "chunk_index": 3,
    "text": "he ultimate \ngoal of achieving even more advanced levels of Arabic handwriting \nrecognition. \n \n8.2 Future Work \n \n \nThe Arabic language is rich in special characters and diacritics that \ncan significantly alter the meaning of a word. As a result, future research \nwill involve creating a more diverse dataset, and developing more \nsophisticated models that are specifically trained to recognize and \ninterpret these characters accurately across a more diverse dataset. \nThis could involve creating d",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 94,
    "chunk_index": 4,
    "text": "racters accurately across a more diverse dataset. \nThis could involve creating datasets from scratch; ones that focus on \nthese special characters and incorporating more complex models that \ncan capture the nuances of their usage and placement. We plan to focus \non collecting and preparing more comprehensive and diverse datasets. \nThis includes incorporating more datasets that represent different age",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 95,
    "chunk_index": 0,
    "text": "CHAPTER 8 \nCONCLUSION AND FUTURE WORK\n \n95 \ngroups, writing styles, and handwriting conditions, such as varying levels \nof pressure, speed, and ink darkness, and if possible, across multiple \nmediums. The development of synthetic datasets or the augmentation \nof existing datasets with techniques like rotation, scaling, and noise \ncould also help improve the model's robustness against real-world \nconditions and make it a powerful tool to be used in real-world \napplications. \n \n8.2.1 Searchable Ha",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 95,
    "chunk_index": 1,
    "text": "t a powerful tool to be used in real-world \napplications. \n \n8.2.1 Searchable Handwritten Note Taking App \n \n \nA Searchable Handwritten Note Taking App is a mobile or desktop \napplication that allows users to take notes by hand using a stylus or \ntouch screen, and then search and access those notes later using \nkeywords or text recognition. \n8.2.1.1 Key Features \n \n Handwritten Note Taking: This is the core functionality, allowing \nusers to write notes directly on the app's interface using a st",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 95,
    "chunk_index": 2,
    "text": "ality, allowing \nusers to write notes directly on the app's interface using a stylus \nor their finger. \n Ink-to-Text Conversion (OCR): The app uses Optical Character \nRecognition (OCR) technology to convert handwritten notes into \ndigital text. This enables searching and indexing of the content. \n Search Functionality: Users can search for specific keywords or \nphrases within their handwritten notes using the converted text. \n Organization Tools: The app might offer features for organizing \nn",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 95,
    "chunk_index": 3,
    "text": "rted text. \n Organization Tools: The app might offer features for organizing \nnotes by tags, categories, or creating folders. \n Cloud Storage and Syncing: Some apps allow storing notes in the \ncloud and syncing them across multiple devices. \n Additional Features: Advanced apps might offer features like \naudio recording alongside handwritten notes, annotation \ncapabilities on imported PDFs, or handwriting recognition in \nmultiple languages.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 96,
    "chunk_index": 0,
    "text": "CHAPTER 8 \nCONCLUSION AND FUTURE WORK\n \n96 \n8.2.1.2 Existing Examples: \n \n Notability \n GoodNotes \n Evernote (with handwriting recognition add-on) \n Microsoft OneNote \n Noteshelf \n \n8.2.2 Automatic Exam Grading Software \n \nOur future research aims to develop new techniques for recognizing \nmathematical equations within Arabic handwritten text. This includes \ntraining models to accurately parse and interpret mathematical \nsymbols, expressions, and formulas, which are integral components of",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 96,
    "chunk_index": 1,
    "text": "ematical \nsymbols, expressions, and formulas, which are integral components of \nvarious subjects including mathematics, physics, and engineering. If \nsuccessful, we envision integrating these advancements into a \ncomprehensive system that combines Language Models (LLMs) with \nOCR models. This combined system will possess the capability to \nautomatically grade handwritten Arabic exams. By leveraging the \nstrengths of LLMs in understanding context and semantics alongside \nthe precision of OCR in r",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 96,
    "chunk_index": 2,
    "text": "LLMs in understanding context and semantics alongside \nthe precision of OCR in recognizing characters and symbols, we aim to \ncreate a powerful tool for educators to efficiently assess student \nperformance in Arabic written subjects, including those involving \nmathematical components. This system could significantly streamline \nthe grading process, reduce manual effort and human error, and provide \ntimely feedback to both students and instructors. Automatic Exam \nGrading Software is a type of ed",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 96,
    "chunk_index": 3,
    "text": "both students and instructors. Automatic Exam \nGrading Software is a type of educational technology designed to \nstreamline the process of evaluating student performance in multiple-\nchoice, fill-in-the-blank, and similar standardized tests. \n \n8.2.2.1 Core Features: \n \n Scannable Answer Sheets: The software works by scanning pre-\nformatted answer sheets marked by students. These sheets",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 97,
    "chunk_index": 0,
    "text": "CHAPTER 8 \nCONCLUSION AND FUTURE WORK\n \n97 \ntypically involve filling in bubbles for multiple-choice questions or \nwriting short answers in designated areas. \n Optical Mark Recognition (OMR): The software utilizes OMR \ntechnology to recognize marks or written responses on the \nanswer sheets. OMR can translate these marks into digital data for \nfurther processing. \n Answer Key Integration: Educators can input the correct answers \nor answer patterns for each question into the software. \n Automa",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 97,
    "chunk_index": 1,
    "text": "rrect answers \nor answer patterns for each question into the software. \n Automatic Scoring: The software compares students' responses \non the scanned sheets with the answer key, automatically \ncalculating scores for each student and potentially the entire class. \n Data Analysis and Reporting: Some software may offer features \nfor generating reports that summarize student performance and \nprovide insights into class-wide understanding or areas where \nstudents might need additional support. \n \nT",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 97,
    "chunk_index": 2,
    "text": "-wide understanding or areas where \nstudents might need additional support. \n \nThe use of Automatic Exam Grading Software is becoming increasingly \ncommon as technology advances. However, it's important to use it \nstrategically, understanding its limitations and ensuring effective \nintegration with other assessment methods.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 98,
    "chunk_index": 0,
    "text": "98 \nREFERENCES \n \n[1] \nH. P. E. D. LP, “OCR Document API.” 2016. [Online]. Available: \nhttps://dev.havenondemand.com/apis/ocrdocument \n[2] \nInnovatrics, “OCR (Optical Character Recognition).” 2023. [Online]. \nAvailable: \nhttps://www.innovatrics.com/glossary/ocr-optical-\ncharacter-recognition/ \n[3] \nJ. W. T. Smith and Z. Merali, Optical Character Recognition: The \nTechnology and Its Application in Information Units and Libraries. in \nFact sheet / British Library. Research and Development Dept. \nB",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 98,
    "chunk_index": 1,
    "text": "d Libraries. in \nFact sheet / British Library. Research and Development Dept. \nBritish \nLibrary, \n1985. \n[Online]. \nAvailable: \nhttps://books.google.com.eg/books?id=okGxAAAAIAAJ \n[4] \nA. W. Services, “What is Optical Character Recognition (OCR)?” \n2024. [Online]. Available: https://aws.amazon.com/what-is/ocr/ \n[5] \nC. S. Smith, “What Is OCR (Optical Character Recognition) \nTechnology?,” \nForbes, \n2023, \n[Online]. \nAvailable: \nhttps://www.forbes.com/sites/technology/article/what-is-ocr-\ntechnolog",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 98,
    "chunk_index": 2,
    "text": "ailable: \nhttps://www.forbes.com/sites/technology/article/what-is-ocr-\ntechnology/ \n[6] \nV. Märgner and H. El Abed, Guide to OCR for Arabic Scripts. \nSpringer Science & Business Media, 2024. \n[7] \nM. Balat, Y. Mohamed, A. Heakl, and A. Zaky, “Arabic Handwritten \nText for Person Biometric Identification: A Deep Learning \nApproach,” ArXiv, vol. 2406.00409v1, 2024, [Online]. Available: \nhttps://arxiv.org/html/2406.00409v1 \n[8] \nS. V Rice, G. Nagy, and T. A. Nartker, Optical Character Recognition:",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 98,
    "chunk_index": 3,
    "text": "v1 \n[8] \nS. V Rice, G. Nagy, and T. A. Nartker, Optical Character Recognition: \nAn Illustrated Guide to the Frontier. Springer Science & Business \nMedia, 2024. \n[9] \nN. Ramesh, A. Srivastava, and K. Deeba, “Improving Optical \nCharacter Recognition Techniques,” International Journal of \nEngineering &amp; Technology, vol. 7, no. 2.24, p. 361, Jul. 2024, doi: \n10.14419/ijet.v7i2.24.12085. \n[10] B. Kunkel, “We Built one of the Top OCR Tools.” Jun. 2022. [Online]. \nAvailable: https://code.pieces.app/",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 98,
    "chunk_index": 4,
    "text": "of the Top OCR Tools.” Jun. 2022. [Online]. \nAvailable: https://code.pieces.app/blog/top-ocr-tools",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 99,
    "chunk_index": 0,
    "text": "REFERENCES\n \n99 \n[11] Medium, “Evaluating Offline Handwritten Text Recognition: Which \nMachine Learning Model is the Winner?,” Medium, vol. 8, no. 2, pp. \n63–78, 2024. \n[12] L. Hamami and D. Berkani, “Recognition system for printed multi-\nfont and multi-size Arabic characters,” Arab J Sci Eng, vol. 27, Apr. \n2002. \n[13] Conexiom, “The Advantages, Challenges, and Alternatives to OCR \nSolutions,” Conexiom, vol. 12, no. 4, pp. 45–57, 2024. \n[14] Z. Shi, S. Setlur, and V. Govindaraju, “Pre-processin",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 99,
    "chunk_index": 1,
    "text": "4, pp. 45–57, 2024. \n[14] Z. Shi, S. Setlur, and V. Govindaraju, “Pre-processing Issues in \nArabic OCR,” in Guide to OCR for Arabic Scripts, London: Springer \nLondon, 2012, pp. 79–102. doi: 10.1007/978-1-4471-4072-6_4. \n[15] I. Ahmad and G. Fink, “Handwritten Arabic text recognition using \nmulti-stage sub-core-shape HMMs,” International Journal on \nDocument Analysis and Recognition (IJDAR), vol. 22, Sep. 2019, doi: \n10.1007/s10032-019-00339-8. \n[16] V. Kulesh, K. Schaffer, I. Sethi, and M. Schw",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 99,
    "chunk_index": 2,
    "text": "10.1007/s10032-019-00339-8. \n[16] V. Kulesh, K. Schaffer, I. Sethi, and M. Schwartz, “Handwriting \nquality evaluation,” in International Conference on Advances in \nPattern Recognition, Berlin, Heidelberg, Mar. 2001, pp. 157–165. \n[17] M. Liwicki, A. Graves, H. Bunke, and J. Schmidhuber, “A novel \napproach to on-line handwriting recognition based on bidirectional \nlong short-term memory networks,” 2007. [Online]. Available: \nhttps://api.semanticscholar.org/CorpusID:5668166 \n[18] H. Akouaydi, Y.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 99,
    "chunk_index": 3,
    "text": "ilable: \nhttps://api.semanticscholar.org/CorpusID:5668166 \n[18] H. Akouaydi, Y. Hamdi, H. Boubaker, M. Zaied, F. Alaya Cheikh, and \nA. Alimi, Children’s Online Handwriting Quality Analysis. 2021. doi: \n10.36227/techrxiv.14776338.v1. \n[19] M. Suganthi and R. Arun Prakash, “An offline English optical \ncharacter recognition and NER using LSTM and adaptive neuro-\nfuzzy inference system,” Journal of Intelligent & Fuzzy Systems, vol. \n44, pp. 3877–3890, 2023, doi: 10.3233/JIFS-221486. \n[20] P. Ahmed a",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 99,
    "chunk_index": 4,
    "text": "stems, vol. \n44, pp. 3877–3890, 2023, doi: 10.3233/JIFS-221486. \n[20] P. Ahmed and Y. Al-Ohali, “Arabic Character Recognition: Progress \nand Challenges,” Journal of King Saud University - Computer and \nInformation \nSciences, \nvol. \n12, \npp. \n85–116, \n2000, \ndoi: \nhttps://doi.org/10.1016/S1319-1578(00)80004-X.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 100,
    "chunk_index": 0,
    "text": "REFERENCES\n \n100 \n[21] “Arabic Language Day.” Statistics Canada, 2024. [Online]. Available: \nhttps://www.statcan.gc.ca/o1/en/plus/2555-arabic-language-\nday \n[22] Z. Alyafeai, M. S. Al-shaibani, M. Ghaleb, and Y. A. Al-Wajih, “Calliar: \nan online handwritten dataset for Arabic calligraphy,” Neural \nComput Appl, vol. 34, no. 23, pp. 20701–20713, 2022, doi: \n10.1007/s00521-022-07537-2. \n[23] M. Gudenburg, “Navigating the Challenges of OCR-based \nResearch in the Arabic Script and Language.” 2023. [O",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 100,
    "chunk_index": 1,
    "text": "e Challenges of OCR-based \nResearch in the Arabic Script and Language.” 2023. [Online]. \nAvailable: https://href.hypotheses.org/2516 \n[24] D. Simonnet and E. Anquetil, “Handwriting Quality Analysis of Block \nLetters and Cursive Words,” Handwriting Today, Journal of the \nNational Handwriting Association, no. 15, pp. 15–21, Dec. 2016, \n[Online]. Available: https://hal.science/hal-01484924 \n[25] S. Faizullah, M. S. Ayub, S. Hussain, and M. A. Khan, “A Survey of OCR \nin Arabic Language: Applications",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 100,
    "chunk_index": 2,
    "text": ", S. Hussain, and M. A. Khan, “A Survey of OCR \nin Arabic Language: Applications, Techniques, and Challenges,” \nApplied Sciences, vol. 13, no. 7, 2023, doi: 10.3390/app13074584. \n[26] A. Zidouri, “ORAN: a basis for an Arabic OCR system,” in \nProceedings of 2004 International Symposium on Intelligent \nMultimedia, Video and Speech Processing, 2004., 2004, pp. 703–\n706. doi: 10.1109/ISIMP.2004.1434161. \n[27] N. Nayef and J. M. Ogier, “Metric-based no-reference quality \nassessment of heterogeneous d",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 100,
    "chunk_index": 3,
    "text": "d J. M. Ogier, “Metric-based no-reference quality \nassessment of heterogeneous document images,” in Document \nRecognition and Retrieval XXII, Feb. 2015, p. 94020L. \n[28] H. Osman, K. Zaghw, M. Hazem, and S. Elsehely, “An Efficient \nLanguage-Independent Multi-Font OCR for Arabic Script.” 2020. \n[29] A. Mezghani, R. Maalej, M. Elleuch, and M. Kherallah, “Recent \nadvances of ML and DL approaches for Arabic handwriting \nrecognition: A review,” Int. J. Hybrid Intell. Syst., vol. 19, no. 1,2, pp. \n61–",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 100,
    "chunk_index": 4,
    "text": "recognition: A review,” Int. J. Hybrid Intell. Syst., vol. 19, no. 1,2, pp. \n61–78, Jul. 2023, doi: 10.3233/HIS-230005. \n[30] K. O’Shea and R. Nash, “An introduction to convolutional neural \nnetworks,” arXiv preprint arXiv:1511.08458, 2015. \n[31] A. Sherstinsky, “Fundamentals of recurrent neural network (rnn) \nand long short-term memory (lstm) network,” Physica D, vol. 404, \np. 132306, 2020, doi: 10.1016/j.physd.2019.132306.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 101,
    "chunk_index": 0,
    "text": "REFERENCES\n \n101 \n[32] S. Hochreiter and J. Schmidhuber, “Long Short-term Memory,” \nNeural Comput, \nvol. \n9, \npp. \n1735–1780, \nMar. \n1997, \ndoi: \n10.1162/neco.1997.9.8.1735. \n[33] A. Mostafa et al., An End-to-End OCR Framework for Robust \nArabic-Handwriting Recognition using a Novel Transformers-\nbased Model and an Innovative 270 Million-Words Multi-Font \nCorpus of Classical Arabic with Diacritics. 2022. \n[34] N. I. Youssef and N. Abd-Alsabour, “A Review on Arabic Handwriting \nRecognition,” Jour",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 101,
    "chunk_index": 1,
    "text": "Youssef and N. Abd-Alsabour, “A Review on Arabic Handwriting \nRecognition,” Journal of Southwest Jiaotong University, vol. 57, no. \n6, pp. 746–756, Dec. 2022, doi: 10.35741/issn.0258-2724.57.6.66. \n[35] M. S. Kasem, M. Mahmoud, and H.-S. Kang, “Advancements and \nChallenges \nin \nArabic \nOptical \nCharacter \nRecognition: \nA \nComprehensive Survey,” ArXiv, vol. 2312.11812v1, 2023, [Online]. \nAvailable: https://arxiv.org/html/2312.11812v1 \n[36] A. Alaei, V. Bui, D. Doermann, and U. Pal, “Document Imag",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 101,
    "chunk_index": 2,
    "text": "tml/2312.11812v1 \n[36] A. Alaei, V. Bui, D. Doermann, and U. Pal, “Document Image Quality \nAssessment: A Survey,” ACM Comput. Surv., vol. 56, no. 2, Sep. \n2023, doi: 10.1145/3606692. \n[37] G. Cloud, “Use Cases for Optical Character Recognition (OCR) on \nGoogle \nCloud.” \nGoogle. \n[Online]. \nAvailable: \nhttps://cloud.google.com/use-cases/ocr?hl=en \n[38] R. Smith, “An Overview of the Tesseract OCR Engine,” in Ninth \nInternational Conference on Document Analysis and Recognition \n(ICDAR \n2007), \n2007",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 101,
    "chunk_index": 3,
    "text": "ernational Conference on Document Analysis and Recognition \n(ICDAR \n2007), \n2007, \npp. \n629–633. \ndoi: \n10.1109/ICDAR.2007.4376991. \n[39] S. V Rice, F. R. Jenkins, and T. A. Nartker, “The Fourth Annual Test of \nOCR Accuracy.”  \n[40] G. C. Blog, “Announcing Tesseract OCR.” Aug. 2006. \n[41] T. Kanungo, G. A. Marton, and O. Bulbul, “Performance evaluation \nof two Arabic OCR products,” in Proc.SPIE, Jan. 1999, pp. 76–83. \ndoi: 10.1117/12.339809. \n[42] A. Carrizo, J. Garbajosa, and G. Molina, “Requir",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 101,
    "chunk_index": 4,
    "text": "doi: 10.1117/12.339809. \n[42] A. Carrizo, J. Garbajosa, and G. Molina, “Requirements gathering: \nthe journey,” International Journal of Decision Systems, vol. 23, no. \n4, pp. 667–681, 2014.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 102,
    "chunk_index": 0,
    "text": "REFERENCES\n \n102 \n[43] Z. Shi, S. Setlur, and V. Govindaraju, “Pre-processing Issues in \nArabic OCR,” in Guide to OCR for Arabic Scripts, London: Springer \nLondon, 2012, pp. 79–102. doi: 10.1007/978-1-4471-4072-6_4. \n[44] K. Safjan, “15 Tools for Document Deskewing and Dewarping,” \nKrystian’s Safjan Blog, 2022. \n[45] Joel, “Document Scanner using OpenCV (HoughLines Approach).” \nApr. 2020. [Online]. Available: https://medium.com/analytics-\nvidhya/document-scanner-using-opencv-houghlines-approach-",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 102,
    "chunk_index": 1,
    "text": "/medium.com/analytics-\nvidhya/document-scanner-using-opencv-houghlines-approach-\neb276dd4a0a \n[46] H. Feng, Y. Wang, W. Zhou, J. Deng, and H. Li, “DocTr: Document \nImage Transformer for Geometric Unwarping and Illumination \nCorrection,” in Proceedings of the 29th ACM International \nConference on Multimedia, 2021, pp. 273–281. \n[47] H. Feng, W. Zhou, J. Deng, Q. Tian, and H. Li, “DocScanner: Robust \nDocument Image Rectification with Progressive Learning,” arXiv \npreprint arXiv:2110.14968, 2021.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 102,
    "chunk_index": 2,
    "text": "tification with Progressive Learning,” arXiv \npreprint arXiv:2110.14968, 2021. \n[48] H. Feng, S. Liu, J. Deng, W. Zhou, and H. Li, “Deep Unrestricted \nDocument Image Rectification,” IEEE Trans Multimedia, 2023. \n[49] S. Das, K. Ma, Z. Shu, D. Samaras, and R. Shilkrot, “DewarpNet: \nSingle-Image Document Unwarping With Stacked 3D and 2D \nRegression Networks,” in The IEEE International Conference on \nComputer Vision (ICCV), Oct. 2019. \n[50] R. Santos, G. Clemente, T. Ing Ren, and G. Cavalcanti, “Te",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 102,
    "chunk_index": 3,
    "text": "CV), Oct. 2019. \n[50] R. Santos, G. Clemente, T. Ing Ren, and G. Cavalcanti, “Text Line \nSegmentation Based on Morphology and Histogram Projection,” \nDocument Analysis and Recognition, International Conference on, \nvol. 0, pp. 651–655, Feb. 2009, doi: 10.1109/ICDAR.2009.183. \n[51] V. Alkalai Mohamed and Sorge, “A Histogram-Based Approach to \nMathematical Line Segmentation,” in Progress in Pattern \nRecognition, Image Analysis, Computer Vision, and Applications, G. \nRuiz-Shulcloper José and Sannit",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 102,
    "chunk_index": 4,
    "text": "Analysis, Computer Vision, and Applications, G. \nRuiz-Shulcloper José and Sanniti di Baja, Ed., Berlin, Heidelberg: \nSpringer Berlin Heidelberg, 2013, pp. 447–455. \n[52] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach. \nPearson Higher Education, 2024. \n[53] D. Foead, A. Ghifari, M. B. Kusuma, N. Hanafiah, and E. Gunawan, “A \nSystematic Literature Review of A* Pathfinding,” Procedia Comput",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 103,
    "chunk_index": 0,
    "text": "REFERENCES\n \n103 \nSci, \nvol. \n179, \npp. \n507–514, \n2021, \ndoi: \nhttps://doi.org/10.1016/j.procs.2021.01.034. \n[54] Y. Baek, B. Lee, D. Han, S. Yun, and H. Lee, “Character Region \nAwareness for Text Detection,” in Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition, 2019, pp. \n9365–9374. \n[55] D. Madhugiri, “Extract Text from Images Quickly Using Keras-OCR \nPipeline,” \nAnalytics \nVidhya, \n2022, \n[Online]. \nAvailable: \nhttps://www.analyticsvidhya.com/blog/2022/09/extract",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 103,
    "chunk_index": 1,
    "text": "22, \n[Online]. \nAvailable: \nhttps://www.analyticsvidhya.com/blog/2022/09/extract-text-\nfrom-images-quickly-using-keras-ocr-pipeline/ \n[56] M. PRATA, “Keras OCR Text Recognition,” Kaggle, Sep. 20, 2022. \n[Online]. \nAvailable: \nhttps://www.kaggle.com/code/mpwolke/keras-ocr-text-\nrecognition \n[57] S. A. Mahmoud et al., “KHATT: Arabic Offline Handwritten Text \nDatabase,” in 2012 International Conference on Frontiers in \nHandwriting \nRecognition, \n2012, \npp. \n449–454. \ndoi: \n10.1109/ICFHR.2012.224.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 103,
    "chunk_index": 2,
    "text": "Handwriting \nRecognition, \n2012, \npp. \n449–454. \ndoi: \n10.1109/ICFHR.2012.224. \n[58] S. A. Mahmoud et al., “KHATT: An open Arabic offline handwritten \ntext database,” Pattern Recognit, vol. 47, no. 3, pp. 1096–1112, Feb. \n2014, doi: 10.1016/j.patcog.2013.08.009. \n[59] D. Powers, PHP Solutions: Dynamic Web Design Made Easy. Apress, \n2024. \n[60] K. W. Hon, “Artificial neural networks,” in Technology and Security \nfor Lawyers and Other Professionals, Edward Elgar Publishing, \n2024, pp. 490–511. \n[6",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 103,
    "chunk_index": 3,
    "text": "awyers and Other Professionals, Edward Elgar Publishing, \n2024, pp. 490–511. \n[61] A. Hashemi, G. Orzechowski, A. Mikkola, and J. McPhee, “Multibody \ndynamics and control using machine learning,” Multibody Syst Dyn, \nvol. 58, pp. 1–35, Feb. 2023, doi: 10.1007/s11044-023-09884-x. \n[62] M. F. Ijaz and M. Woźniak, “A Survey of Deep Convolutional Neural \nNetworks Applied for Prediction of Plant Leaf Diseases,” Sensors, \nvol. 21, no. 14, p. 4749, 2021, doi: 10.3390/s21144749. \n[63] C. Xiao and J. Sun",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 103,
    "chunk_index": 4,
    "text": "vol. 21, no. 14, p. 4749, 2021, doi: 10.3390/s21144749. \n[63] C. Xiao and J. Sun, “Deep Neural Networks (DNN),” in Introduction \nto Deep Learning for Healthcare, Springer, Cham, 2021. doi: \n10.1007/978-3-030-82184-5_4.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 104,
    "chunk_index": 0,
    "text": "REFERENCES\n \n104 \n[64] IBM, “What are Recurrent Neural Networks?” 2024. [Online]. \nAvailable: \nhttps://www.ibm.com/cloud/learn/recurrent-neural-\nnetworks \n[65] Wikipedia, “Recurrent Neural Network.” 2024. [Online]. Available: \nhttps://en.wikipedia.org/wiki/Recurrent_neural_network \n[66] AWS, “What is RNN? - Recurrent Neural Networks Explained.” 2024. \n[Online]. \nAvailable: \nhttps://aws.amazon.com/machine-\nlearning/what-is-rnn/ \n[67] DataCamp, “Recurrent Neural Network Tutorial (RNN).” 2024. \n[On",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 104,
    "chunk_index": 1,
    "text": "at-is-rnn/ \n[67] DataCamp, “Recurrent Neural Network Tutorial (RNN).” 2024. \n[Online]. \nAvailable: \nhttps://www.datacamp.com/tutorial/recurrent-neural-networks-\ntutorial \n[68] T. O’Shea, S. Hitefield, and J. Corgan, “End-to-End Radio Traffic \nSequence Recognition with Deep Recurrent Neural Networks,” \nMar. 2016. \n[69] R. Najam and S. Faizullah, “Analysis of Recent Deep Learning \nTechniques for Arabic Handwritten-Text OCR and Post-OCR \nCorrection,” Applied Sciences, vol. 13, no. 13, p. 7568, 2023",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 104,
    "chunk_index": 2,
    "text": "OCR and Post-OCR \nCorrection,” Applied Sciences, vol. 13, no. 13, p. 7568, 2023, doi: \n10.3390/app13137568. \n[70] W. Albattah and S. Albahli, “Intelligent Arabic Handwriting \nRecognition Using Different Standalone and Hybrid CNN \nArchitectures,” Applied Sciences, vol. 12, no. 19, 2022, doi: \n10.3390/app121910155. \n[71] T. Mullaney, “Building My Own Google Hangouts ‘Autocomplete’ \nwith \nChar-RNN.” \n2016. \n[Online]. \nAvailable: \nhttp://tommymullaney.com/projects/char-rnn-gchat \n[72] P. \nW. \nCode,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 104,
    "chunk_index": 3,
    "text": "Available: \nhttp://tommymullaney.com/projects/char-rnn-gchat \n[72] P. \nW. \nCode, \n“CTC \nLoss \nExplained.” \n[Online]. \nAvailable: \nhttps://paperswithcode.com/method/ctc-loss \n[73] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, \n“Connectionist temporal classification: Labelling unsegmented \nsequence data with recurrent neural networks,” Proceedings of the \nInternational Conference on Machine Learning (ICML), pp. 369–\n376, \n2006, \n[Online]. \nAvailable: \nhttps://citeseerx.ist.psu.edu/viewdoc",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 104,
    "chunk_index": 4,
    "text": ". 369–\n376, \n2006, \n[Online]. \nAvailable: \nhttps://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.630\n6",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 105,
    "chunk_index": 0,
    "text": "REFERENCES\n \n105 \n[74] A. Hannun, “Sequence Modeling with CTC,” Distill, vol. 2, no. 11, 2017, \ndoi: 10.23915/distill.00008. \n[75] G. Agrawal, S. Taqvi, and R. Gulati, “Machine Learning with \nTensorFlow and PyTorch: A Comparative Analysis,” RES MILITARIS, \nvol. 10, no. 1, pp. 172–179, 2020. \n[76] NVIDIA 2020, “NVIDIA GeForce RTX 3060 Family.”  \n[77] “Data-centric AI: A complete primer,” Snorkel AI, May 17, 2022. \n[Online]. Available: https://snorkel.ai/data-centric-ai-primer/ \n[78] M. A. Khan, “",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 105,
    "chunk_index": 1,
    "text": "line]. Available: https://snorkel.ai/data-centric-ai-primer/ \n[78] M. A. Khan, “Arabic handwritten alphabets, words and paragraphs \nper user (AHAWP) dataset,” Data Brief, vol. 41, p. 107947, Jul. 2024, \ndoi: 10.1016/j.dib.2022.107947. \n[79] S. Raschka, “Training-validation-test split and cross-validation \ndone \nright.” \n2020. \n[Online]. \nAvailable: \nhttps://machinelearningmastery.com/train-test-split-and-cross-\nvalidation-for-model-evaluation/ \n[80] C. Shorten and T. M. Khoshgoftaar, “A survey o",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 105,
    "chunk_index": 2,
    "text": "ation-for-model-evaluation/ \n[80] C. Shorten and T. M. Khoshgoftaar, “A survey on Image Data \nAugmentation for Deep Learning,” J Big Data, vol. 6, no. 1, p. 60, \n2019, doi: 10.1186/s40537-019-0197-0. \n[81] Inc. Kolena, “Model Training in AI/ML: Process, Challenges, and Best \nPractices,” \nKolena, \n2023, \n[Online]. \nAvailable: \nhttps://www.kolena.com/blog/model-training-process \n[82] Dataheroes.ai, “Machine Learning Model Training,” Dataheroes, \n2023, [Online]. Available: https://www.dataheroes.ai",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 105,
    "chunk_index": 3,
    "text": "del Training,” Dataheroes, \n2023, [Online]. Available: https://www.dataheroes.ai/machine-\nlearning-model-training \n[83] ProjectPro.io, “How to Train a Machine Learning Model: The \nComplete \nGuide,” \nProjectPro, \n2023, \n[Online]. \nAvailable: \nhttps://www.projectpro.io/article/how-to-train-a-machine-\nlearning-model \n[84] Inc. Kolena, “Model Training in AI/ML: Process, Challenges, and Best \nPractices,” \nKolena, \n2023, \n[Online]. \nAvailable: \nhttps://www.kolena.com/blog/model-training-process \n[85]",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 105,
    "chunk_index": 4,
    "text": "[Online]. \nAvailable: \nhttps://www.kolena.com/blog/model-training-process \n[85] T. with Kolena, “WER, CER, and MER: Measuring textual similarity \nwith substitution, deletion, and insertion errors,” Testing with \nKolena, 2024, [Online]. Available: https://docs.kolena.com",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 106,
    "chunk_index": 0,
    "text": "REFERENCES\n \n106 \n[86] M. Xiong et al., A Rate of Penetration (ROP) Prediction Method \nBased on Improved Dung Beetle Optimization Algorithm and \nBiLSTM-SA. 2024. doi: 10.21203/rs.3.rs-4255057/v1. \n[87] C. Pavlatos, E. Makris, G. Fotis, V. Vita, and V. Mladenov, “Enhancing \nElectrical Load Prediction Using a Bidirectional LSTM Neural \nNetwork,” Electronics (Basel), vol. 12, p. 4652, Jul. 2023, doi: \n10.3390/electronics12224652. \n[88] J. H. Noor, “The Effects of Architectural Design Decisions on",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 106,
    "chunk_index": 1,
    "text": "s12224652. \n[88] J. H. Noor, “The Effects of Architectural Design Decisions on \nFramework Adoption: A Comparative Evaluation of Meta-\nFrameworks in Modern Web Development,” Aalto University, \nEspoo, 2024. \n[89] Meta, “React | Meta Open Source.” 2024. \n[90] Kinsta, “What Is React.js? A Look at the Popular JavaScript Library.” \n2023. \n[91] D. E. V Community, “ReactJS: A Comprehensive Overview of a \nPopular JavaScript Library.” 2023. \n[92] T. Rascia, “React Tutorial: An Overview and Walkthrough.” 2",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 106,
    "chunk_index": 2,
    "text": "ibrary.” 2023. \n[92] T. Rascia, “React Tutorial: An Overview and Walkthrough.” 2023. \n[93] “Comparison of FastAPI with Django and Flask,” GeeksforGeeks, \nNov. \n04, \n2023. \n[Online]. \nAvailable: \nhttps://www.geeksforgeeks.org/comparison-of-fastapi-with-\ndjango-and-flask/ \n[94] M. Tan and Q. V Le, “EfficientNet: Rethinking Model Scaling for \nConvolutional Neural Networks,” in Proceedings of the 36th \nInternational Conference on Machine Learning, 2019. \n[95] K. Simonyan and A. Zisserman, “Very Deep",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 106,
    "chunk_index": 3,
    "text": "erence on Machine Learning, 2019. \n[95] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks \nfor Large-Scale Image Recognition,” arXiv preprint arXiv:1409.1556, \n2014. \n[96] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for \nImage Recognition,” in Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition, 2016, pp. 770–778. \n[97] V. Adithya et al., “EffUnet-SpaGen: An Efficient and Spatial \nGenerative Approach to Glaucoma Detection,” J Imaging, vol",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 106,
    "chunk_index": 4,
    "text": "fficient and Spatial \nGenerative Approach to Glaucoma Detection,” J Imaging, vol. 7, p. \n92, May 2021, doi: 10.3390/jimaging7060092. \n[98] T.-H. Nguyen, T.-N. Nguyen, and B.-V. Ngo, “A VGG-19 Model with \nTransfer Learning and Image Segmentation for Classification of",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 107,
    "chunk_index": 0,
    "text": "REFERENCES\n \n107 \nTomato Leaf Disease,” AgriEngineering, vol. 4, pp. 871–887, Jul. \n2022, doi: 10.3390/agriengineering4040056. \n[99] T. Hoeser and C. Kuenzer, “Object Detection and Image \nSegmentation with Deep Learning on Earth Observation Data: A \nReview-Part I: Evolution and Recent Trends,” Remote Sens (Basel), \nvol. 12, Jul. 2020, doi: 10.3390/rs12101667. \n[100] S. V. Iyer, “Implementing Analog Filters Digitally,” in Digital Filter \nDesign using Python for Power Engineering Applications: An",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 107,
    "chunk_index": 1,
    "text": "” in Digital Filter \nDesign using Python for Power Engineering Applications: An Open \nSource Guide, S. V. Iyer, Ed., Cham: Springer International \nPublishing, 2020, pp. 71–111. doi: 10.1007/978-3-030-61860-5_5. \n[101] BotPenguin, “Transfer Learning: Techniques & Algorithms,” \nBotPenguin, Oct. 2023.",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 108,
    "chunk_index": 0,
    "text": "اﻟﻤﻠﺨﺺ  \n \n ﻗﺒﻞ ﻣﻦ \"اﻟﻌﺮﺑﻴﺔ ﻟﻠﻐﺔ اﻟﻴﺪوﻳﺔ اﻟﻜﺘﺎﺑﺔ ﻋﻠﻰ اﻟﺘﻌﺮف\" اﻟﻤﴩوع ﻫﺬا ﺗﻨﻔﻴﺬ ﺗﻢ \n ﻣﻦ ﻣﻜﻮن ﻓﺮﻳﻖ  \n11\n  \n آﻟﻲ وﺗﺤﻜﻢ ﺣﺎﺳﺒﺎت ﻗﺴﻢ اﻟﻬﻨﺪﺳﺔ ﻛﻠﻴﺔ ﻓﻲ ﻃﺎﻟﺐ\n ﻣﻦ اﻷﺳﺎﺳﻲ اﻟﻬﺪف .اﻟﺴﻼم ﻋﺒﺪ ﺧﺎﻟﺪ .د إﴍاف ﺗﺤﺖ اﻟﺴﻮﻳﺲ ﻗﻨﺎة ﺑﺠﻤﺎﻋﺔ\n اﻟﻤﴩوع ﻫﻮ ﺗﻄﻮﻳﺮ ﻧﻈﺎم\nﻳﺴﺘﻄﻴﻊ \n وﺗﺤﻮﻳﻠﻬﺎ واﻟﻌﺮﺑﻴﺔ اﻟﻨﺼﻮص ﻋﻠﻰ اﻟﺘﻌﺮف\n اﻟﻤﻘﺎﻻت ﻣﺜﻞ ،اﻟﻤﻘﺎﻻت ﻣﻦ اﻷﻧﻮاع ﻣﺨﺘﻠﻒ ﻟﺘﺤﻮﻳﻞ اﻟﻨﻈﺎم ﺗﺼﻤﻴﻢ ﺗﻢ .ﺎًرﻗﻤﻴ\n  اﻟﻤﺼﻮرة، ﻣﻠﻔﺎتPDF\n ﺑﻴﺎﻧﺎت إﻟﻰ ،اﻟﺮﻗﻤﻴﺔ ﺑﺎﻟﻜﺎﻣﲑات اﻟﺘﻘﺎﻃﻬﺎ ﺗﻢ ﺻﻮر أو ،\n .ﺑﻤﺮوﻧﺔ ﻓﻴﻬﺎ واﻟﺒﺤﺚ ﺗﻌﺪﻳﻠﻬﺎ ﺑﺈﻣﻜﺎﻧﻨﺎ  \n  \n اًﻣﺮور ،وﺗﺤﻀﲑﻫﺎ اﻟﻼزﻣﺔ اﻟﺒﻴﺎﻧﺎت ﺗﺠﻤﻴﻊ",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 108,
    "chunk_index": 1,
    "text": ".ﺑﻤﺮوﻧﺔ ﻓﻴﻬﺎ واﻟﺒﺤﺚ ﺗﻌﺪﻳﻠﻬﺎ ﺑﺈﻣﻜﺎﻧﻨﺎ  \n  \n اًﻣﺮور ،وﺗﺤﻀﲑﻫﺎ اﻟﻼزﻣﺔ اﻟﺒﻴﺎﻧﺎت ﺗﺠﻤﻴﻊ ﻣﻦ ،ﻣﺮاﺣﻞ ﻋﺪة اﻟﻤﴩوع ﺗﻀﻤﻦ\n اﻟﺬﻛﺎء ﺗﺪرﻳﺐ ﻣﺮاﺣﻞ إﻟﻰ ،اﻟﻨﻈﺎم ﻫﺬا ﺗﻄﺒﻴﻖ ﺛﻢ ،وﺗﺼﻤﻴﻤﻪ ﻟﻠﻨﻈﺎم ﺑﺎﻟﺘﺨﻄﻴﻂ\n اﻟﺘﻘﻨﻴﺎت ﻣﻦ اﻟﻌﺪﻳﺪ اﻟﻔﺮﻳﻖ اﺳﺘﺨﺪم .اﻟﻨﺘﺎﺋﺞ وﺗﺤﻠﻴﻞ اﻻﺻﻄﻨﺎﻋﻲ\nواﻟﺘﻜﻨﻮﻟﻮﺟﻴﺎ، ﻣﺜﻞ  \n اﻟﺸﺒﻜﺎت ،اﻟﻌﺮﺑﻴﺔ اﻟﺤﺮوف ﻋﻠﻰ اﻟﺘﻌﺮف ،اﻟﺼﻮر ﻣﻌﺎﻟﺠﺔ\n اﻟﻌﺼﺒﻴﺔ اﻟﻤﻠﺘﻔﺔ (CNN) \n ،\n اﻟﺸﺒﻜﺎت اﻟﻌﺼﺒﻴﺔ اﻟﻤﺘﻜﺮرة(RNN)\n ،\n واﻟﺠﻮرﻳﺰﻣﺎت\n .اﻟﻌﻤﻴﻖ اﻟﺘﻌﻠﻢ  \n  \n ﻣﻮاﺟﻬﺔ ﻣﻦ اﻟﺮﻏﻢ ﻋﻠﻰ ﻣﺜﻞ اﻟﺘﺤﺪﻳﺎت ﻣﻦ اﻟﻜﺜﲑ\nﺗﻌﻘﻴﺪﻳﺔ ﻛﺘﺎﺑﺔ اﻟﻠﻐﺔ اﻟﻌﺮﺑﻴﺔ ،\n ،واﻟﺨﻄﻮط اﻟﻜﺘﺎﺑﺔ ﻃﺮﻳﻖ ﻓﻲ",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 108,
    "chunk_index": 2,
    "text": "ﻰ ﻣﺜﻞ اﻟﺘﺤﺪﻳﺎت ﻣﻦ اﻟﻜﺜﲑ\nﺗﻌﻘﻴﺪﻳﺔ ﻛﺘﺎﺑﺔ اﻟﻠﻐﺔ اﻟﻌﺮﺑﻴﺔ ،\n ،واﻟﺨﻄﻮط اﻟﻜﺘﺎﺑﺔ ﻃﺮﻳﻖ ﻓﻲ اﻻﺧﺘﻼف ﺗﻮﻓﺮ وﻗﻠﺔ واﻟﻤﻔﺘﻮﺣﺔ اﻟﻌﺎﻣﺔ اﻟﻤﺼﺎدر\n ﻟﺒﻴﺎﻧﺎت  اﻟﻜﺘﺎﺑﺔ اﻟﻴﺪوﻳﺔ ﺑﺎﻟﻠﻐﺔ اﻟﻌﺮﺑﻴﺔ، اﺳﺘﻄﺎع اﻟﻔﺮﻳﻖ ﻋﻠﻰ ﺣﺎز ﻧﻈﺎم ﺗﻄﻮﻳﺮ\nﻣﺴﺘﻮﻳﺎت ﻋﺎﻟﻴﺔ ﻣﻦ اﻟﺪﻗﺔ واﻟ ﻟﻠﻐﺔ اﻟﻴﺪوﻳﺔ اﻟﻜﺘﺎﺑﺔ ﻋﻠﻰ اﻟﺘﻌﺮف ﻓﻲ ﻤﺘﺎﻧﺔ\n .اﻟﻌﺮﺑﻴﺔ اﻟﺒﴫي اﻟﺘﻌﺮف\" ﻣﺠﺎل ﻓﻲ ﻓﻘﻂ اﻟﻤﴩوع ﻳﺴﺎﻫﻢ ﻻ ﻟﻠﺤﺮوف(OCR)\n \"\nآﻓﺎق ﻳﻔﺘﺢ ﺎًأﻳﻀ وﻟﻜﻨﻪ  \n ﺗﺨﺼﺺ ﻓﻲ واﻟﺘﻄﻮﻳﺮ اﻟﻤﺴﺘﻘﺒﻠﻲ ﻟﻠﺒﺤﺚ ﺟﺪﻳﺪة\n اﻟﻌﺮﺑﻲ ﻟﻠﻐﺔ اﻟﻴﺪوﻳﺔ اﻟﻜﺘﺎﺑﺔ ﻋﻠﻰ اﻟﺘﻌﺮف",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_112620_13f82dd6",
    "page": 109,
    "chunk_index": 0,
    "text": "ﺟﺎﻣﻌﺔ ﻗﻨﺎة اﻟﺴﻮﻳﺲ  \n اﻟﻬﻨﺪﺳﺔ ﻛﻠﻴﺔ  \n آﻟﻲ وﺗﺤﻜﻢ ﺣﺎﺳﺒﺎت ﻗﺴﻢ \n \n اﻟﺘﻌﺮف ﻋﻠﻰ\nاﻟﺨﻂ اﻟﻴﺪوي ﻟﻠﻐﺔ اﻟﻌﺮﺑﻴﺔ \n \n ﺗﻘﺮﻳﺮ ﻣﻘﺪم ﻛﻤﺘﻄﻠﺐ ﻣ ﴩوع اﻟﺘﺨﺮج ﻟﻠﺤﺼﻮل ﻋﻠﻰ درﺟﺔ\n اﻟﺒﻜﺎﻟﻮرﻳﻮس ﻓﻲ ﻗﺴﻢ اﻟﺤﺎﺳﺒﺎت واﻟﺘﺤﻜﻢ اﻵﻟﻲ ﺑﻜﻠﻴﺔ اﻟﻬﻨﺪﺳﺔ\n ﺟﺎﻣﻌﺔ ﻗﻨﺎة اﻟﺴﻮﻳﺲ \n \n إﴍاف ﺗﺤﺖ \nد. ﺧﺎﻟﺪ ﻋﺒﺪ اﻟﺴﻼم \n \n ﻣﻦ ﻣﻘﺪم \nأﻳﻤﻦ ﺻﺎﺑﺮ ﺟﺎد ﻣﺤﻤﺪ \nأﺣﻤﺪ ﻃﻪ أﺣﻤﺪ ﻋﺒﺪ اﻟﺮﺣﻴﻢ \nأﺣﻤﺪ ﻧﺠﺎح ﻣﺤﻤﺪ أﺣﻤﺪ \nﻣﺤﻤﺪ ﻓﺘﺤﻲ ﻣﺤﻤﺪ ﻣﺤﻤﺪ ﻋﻠﻲ \nأﺑﺎﻧﻮب ﻋﺎﻳﺪ ﺧﻠﻒ \n ﻣﺤﻤﺪ ﻓؤاد رﻳﻢ \nروان ﺟﻤﺎل ﻣﺤﻤﺪ \nﻧﺪى ﻣﺤﻤﻮد ﻣﺤﻤﺪ \n ﻧﺪى ﺣﺴﲔ ﻋﴫان \nﻛﲑﻟﺲ ﺳﻤﲑ ﻓﺘﺤﻲ \nﻣﺤﻤﺪ ﻋﺒﺪ اﻟﻔﺘﺎح ﻓﺘﺤﻲ",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_113524_8851f92d",
    "page": 1,
    "chunk_index": 0,
    "text": "Arabic OCR Leaderboard (2025–2026)\nRank\nOCR Tool / Model\nKey Notes on Arabic\nAccuracy & Usability\nSize / OS\nOutput\n1\nDeepSeekOCR\n✓Excellent\ncharacter/word\nrecognition, preserves\nsemantic flow, minimal\nerrors, readable structure.\nGreat for educational\ntexts, exam passages, and\nstructured content.\n6.22 GB, Heavy\nOutputs Markdown\n2\nQARI OCR\n✓Strong for Arabic\nscript words sometimes\nmisread. Needs\npost-processing for clean\ntext.\n4.12 GB,\nVision-Language\n(Qwen2-VL-2B-\nInstruct),\nfine-tuned on\nArabic,",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_113524_8851f92d",
    "page": 1,
    "chunk_index": 1,
    "text": "an\ntext.\n4.12 GB,\nVision-Language\n(Qwen2-VL-2B-\nInstruct),\nfine-tuned on\nArabic,\nLightweight\nOutput sometimes\nfragmented with HTML\nnoise\n3\nPaddleOCR VL\n✓Very good for Arabic,\nespecially on structured\ndocuments and tables.\nSlightly behind\nDeepSeek/QARI in\ncharacter-level accuracy,\nbut excellent for layout\npreservation.\n1.79 GB,\nLightweight,\nRequires\nWSL/Linux\nOutputs Structured Text\n4\nPaddleOCR v5\n☞Good Arabic\nrecognition on printed\ntext; tested on Windows\nbut does not show top\nperformance.\nLight",
    "source": "page"
  },
  {
    "doc_id": "doc_20260206_113524_8851f92d",
    "page": 1,
    "chunk_index": 2,
    "text": "tion on printed\ntext; tested on Windows\nbut does not show top\nperformance.\nLightweight,\nWindows\ncompatible\n5\nTesseract OCR\n✗Lowest accuracy\n( 74–80% on Arabic).\nStruggles with cursive,\ndiacritics, and word\nligatures. Only suitable\nfor very simple printed\ntext.\nLightweight,\nCross-platform\nOutputs Plain Text\n1",
    "source": "page"
  }
]